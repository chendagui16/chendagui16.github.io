
 <!DOCTYPE HTML>
<html lang="English">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>NLP2 Word Vectors | My Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Dagui Chen">
    

    
    <meta name="description" content="referencecs224n
lecture 2 Word VectorsWord meaningDefinition: meaning

the idea that is represented by a word, phrase, etc.
the idea that a person wants to express by using words, signs, etc.
the idea">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP2 Word Vectors">
<meta property="og:url" content="https://chendagui16.github.io/2017/04/16/NLP2/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="referencecs224n
lecture 2 Word VectorsWord meaningDefinition: meaning

the idea that is represented by a word, phrase, etc.
the idea that a person wants to express by using words, signs, etc.
the idea">
<meta property="og:image" content="http://i4.buimg.com/567571/4ec276dc62e7c4c8.png">
<meta property="og:image" content="http://i2.muimg.com/567571/61b440b00b028d9f.png">
<meta property="og:image" content="http://i1.piimg.com/567571/297e4570b1723090.png">
<meta property="og:image" content="http://i1.piimg.com/567571/ae499869a4a9ea58.png">
<meta property="og:image" content="http://i2.muimg.com/567571/314a54b706896593.png">
<meta property="og:updated_time" content="2017-04-16T07:52:36.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP2 Word Vectors">
<meta name="twitter:description" content="referencecs224n
lecture 2 Word VectorsWord meaningDefinition: meaning

the idea that is represented by a word, phrase, etc.
the idea that a person wants to express by using words, signs, etc.
the idea">
<meta name="twitter:image" content="http://i4.buimg.com/567571/4ec276dc62e7c4c8.png">

    
    <link rel="alternative" href="/atom.xml" title="My Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="My Blog" title="My Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="My Blog">My Blog</a></h1>
				<h2 class="blog-motto">This is my blog to record some knowledge</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:chendagui16.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/16/NLP2/" title="NLP2 Word Vectors" itemprop="url">NLP2 Word Vectors</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Dagui Chen" target="_blank" itemprop="author">Dagui Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-16T02:41:40.000Z" itemprop="datePublished"> 发表于 2017-04-16</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#lecture-2-Word-Vectors"><span class="toc-number">1.</span> <span class="toc-text">lecture 2 Word Vectors</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-meaning"><span class="toc-number">1.1.</span> <span class="toc-text">Word meaning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#One-hot-vector-meaning-in-computer"><span class="toc-number">1.1.1.</span> <span class="toc-text">One-hot vector(meaning in computer)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Distributional-similarity-based-representations"><span class="toc-number">1.1.2.</span> <span class="toc-text">Distributional similarity based representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-idea-of-learning-neural-network-word-embeddings"><span class="toc-number">1.1.3.</span> <span class="toc-text">Basic idea of learning neural network word embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Directly-learning-low-dimensional-word-vectors"><span class="toc-number">1.1.4.</span> <span class="toc-text">Directly learning low-dimensional word vectors</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Main-idea-of-word2vec"><span class="toc-number">1.2.</span> <span class="toc-text">Main idea of word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-skip-gram-model"><span class="toc-number">1.2.1.</span> <span class="toc-text">The skip-gram model</span></a></li></ol></li></ol></li></ol>
		
		</div>
		
		<p>reference<br><a href="http://web.stanford.edu/class/cs224n/syllabus.html" target="_blank" rel="external">cs224n</a></p>
<h1 id="lecture-2-Word-Vectors"><a href="#lecture-2-Word-Vectors" class="headerlink" title="lecture 2 Word Vectors"></a>lecture 2 Word Vectors</h1><h2 id="Word-meaning"><a href="#Word-meaning" class="headerlink" title="Word meaning"></a>Word meaning</h2><p>Definition: <strong>meaning</strong></p>
<ul>
<li>the idea that is represented by a word, phrase, etc.</li>
<li>the idea that a person wants to express by using words, signs, etc.</li>
<li>the idea that is expressed in a word of writing<br>Commonest linguistic way of thinking of meaning</li>
<li>signifier $\iff$ signified (idea or thing) = denotation</li>
</ul>
<h3 id="One-hot-vector-meaning-in-computer"><a href="#One-hot-vector-meaning-in-computer" class="headerlink" title="One-hot vector(meaning in computer)"></a>One-hot vector(meaning in computer)</h3><p>Common answer: Use a taxonomy like WordNet that has hypernyms relationships and synonym sets<br><strong>Problems with this discrete representation</strong></p>
<ul>
<li>Great as a resource but missing nuances, e.g. <em>synonyms</em><ul>
<li>adept, expert, good, practiced, proficient, skillful</li>
</ul>
</li>
<li>Missing new words (impossible to keep up to date):<ul>
<li>wicked, badness, nifty, crack, ace, wizard, genius, ninja</li>
</ul>
</li>
<li>Subjective</li>
<li>Requires human labor to create and adapt</li>
<li>Hard to compute accurate word similarity</li>
<li>The vast majority of rule-based and statistical NLP work regards words as atomic symbols</li>
</ul>
<p>We use usually a localist representation (“one-hot”) to represent discrete word, but the different word vector $ a^T b = 0$, which means that our query and document vectors are orthogonal. There is no natural notion of similarity in a set of one-hot vectors</p>
<p>“one-hot” vector could deal with similarity separately;<br>instead we explore a direct approach where vectors encode it</p>
<h3 id="Distributional-similarity-based-representations"><a href="#Distributional-similarity-based-representations" class="headerlink" title="Distributional similarity based representations"></a>Distributional similarity based representations</h3><p>You can get a lot of value by representing a word by means of its neighbors</p>
<blockquote>
<p>You shall know a word by the company it keeps<br>We will build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context</p>
</blockquote>
<h3 id="Basic-idea-of-learning-neural-network-word-embeddings"><a href="#Basic-idea-of-learning-neural-network-word-embeddings" class="headerlink" title="Basic idea of learning neural network word embeddings"></a>Basic idea of learning neural network word embeddings</h3><p>Define a model that aims to predict between a center word $w_t$ and context words in terms of vectors<br>$$ p(context | w_t) = \dots $$<br>which has a loss function, e.g.<br>$$ J = 1 - p(w_{-t} | w_t ) $$<br>We look at many positions $t$ in a big language corpus<br>We keep adjusting the vector representations of words to minimize this loss</p>
<h3 id="Directly-learning-low-dimensional-word-vectors"><a href="#Directly-learning-low-dimensional-word-vectors" class="headerlink" title="Directly learning low-dimensional word vectors"></a>Directly learning low-dimensional word vectors</h3><ul>
<li>Learning representations by back-propagating errors (Rumelhart et al., 1986)</li>
<li><strong>A neural probabilistic language model</strong> (Bengio et al., 2003)</li>
<li>NLP (almost) from Scratch (Collobert &amp; Weston, 2008)</li>
<li>A recent, even simpler and faster model:<br>word2vec (Mikolov et al. 2013) $\rightarrow$ intro now</li>
</ul>
<h2 id="Main-idea-of-word2vec"><a href="#Main-idea-of-word2vec" class="headerlink" title="Main idea of word2vec"></a>Main idea of word2vec</h2><p><strong>Predict between every word and its context words</strong><br>Two algorithms</p>
<ol>
<li><strong>Skip-grams(SG)</strong><br> Predict context words given target (position independent)</li>
<li>Continuous Bag of Words(CBOW)<br> Predict target from bag-of-words context</li>
</ol>
<p>Two (moderately efficient) training methods</p>
<ol>
<li>Hierarchical softmax</li>
<li>Negative sampling<br><strong>Naive softmax</strong></li>
</ol>
<h3 id="The-skip-gram-model"><a href="#The-skip-gram-model" class="headerlink" title="The skip-gram model"></a>The skip-gram model</h3><p>reference: <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="external">Skip-gram tutorial</a><br>Word2vec uses a trick that we train a simple neural network with a single hidden layer to perform a certain task(<strong>Fake Task</strong>), but then we’re not actually going to use that neural network for the task we trained it on!<br>Instead, the goal is actually just to learn the weights of the hidden layer (<strong>Similar to auto-encoder</strong>)</p>
<p><strong>Fake Task</strong><br><em>Task goal</em> : Given a specific word in the middle of a sentence, look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p>
<blockquote>
<p>When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead</p>
</blockquote>
<p>A sample, window size = 5<br><img src="http://i4.buimg.com/567571/4ec276dc62e7c4c8.png" alt="sample"><br><img src="http://i2.muimg.com/567571/61b440b00b028d9f.png" alt="another explanation"></p>
<p><strong>Model detail</strong></p>
<ul>
<li>Input: one-hot vector(dimension means the scale of vocabulary)</li>
<li>Hidden layer: the word vector for picked word</li>
<li>Output layer: softmax layer, probability that a randomly selected nearby word is that vocabulary word<br><img src="http://i1.piimg.com/567571/297e4570b1723090.png" alt="skip gram"></li>
</ul>
<blockquote>
<p>For example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron)</p>
</blockquote>
<p><img src="http://i1.piimg.com/567571/ae499869a4a9ea58.png" alt="word vector"><br>So the end goal of all of this is really just to learn this hidden layer weight matrix.<br><strong>one-hot vector $\times$ hidden layer weight matrix $\iff$ lookup table</strong><br><img src="http://i2.muimg.com/567571/314a54b706896593.png" alt="lookup table"></p>
<p><strong>objective function</strong><br>For each word $t=1,\dots,T$, predict surrounding words in a window of “radius” $m$ of every word.</p>
<p>Maximize the probability of any context word given the current center word<br>$$ J’(\theta) = \prod_{t=1}^{\pi} \prod_{-m \le j \le m, j \neq 0 } p \left(w_{t+j} | w_t; \theta \right) $$<br>Negative Log likelihood<br>$$ J(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \le j \le m, j \neq 0} \log p \left( w_{t+j} | w_{t} \right) $$<br>Where $\theta$ represents all variable we will optimize</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/NLP/">NLP</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/machine-learning/">machine learning</a><a href="/tags/NLP/">NLP</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://chendagui16.github.io/2017/04/16/NLP2/" data-title="NLP2 Word Vectors | My Blog" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/04/16/NLP3/" title="NLP3">
  <strong>上一篇：</strong><br/>
  <span>
  NLP3</span>
</a>
</div>


<div class="next">
<a href="/2017/04/16/NLP1/"  title="NLP1 Introduction to NLP and DL">
 <strong>下一篇：</strong><br/> 
 <span>NLP1 Introduction to NLP and DL
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2017/04/16/NLP2/" data-title="NLP2 Word Vectors" data-url="https://chendagui16.github.io/2017/04/16/NLP2/"></div>
</section>


<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#lecture-2-Word-Vectors"><span class="toc-number">1.</span> <span class="toc-text">lecture 2 Word Vectors</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-meaning"><span class="toc-number">1.1.</span> <span class="toc-text">Word meaning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#One-hot-vector-meaning-in-computer"><span class="toc-number">1.1.1.</span> <span class="toc-text">One-hot vector(meaning in computer)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Distributional-similarity-based-representations"><span class="toc-number">1.1.2.</span> <span class="toc-text">Distributional similarity based representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-idea-of-learning-neural-network-word-embeddings"><span class="toc-number">1.1.3.</span> <span class="toc-text">Basic idea of learning neural network word embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Directly-learning-low-dimensional-word-vectors"><span class="toc-number">1.1.4.</span> <span class="toc-text">Directly learning low-dimensional word vectors</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Main-idea-of-word2vec"><span class="toc-number">1.2.</span> <span class="toc-text">Main idea of word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-skip-gram-model"><span class="toc-number">1.2.1.</span> <span class="toc-text">The skip-gram model</span></a></li></ol></li></ol></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github 名片</p>
<div class="github-card" data-github="chendagui16" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/Latex/" title="Latex">Latex<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/NLP/" title="NLP">NLP<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/algorithm/" title="algorithm">algorithm<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/code-analyze/" title="code analyze">code analyze<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/software-setting/" title="software setting">software setting<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/work-tips/" title="work tips">work tips<sup>3</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/machine-learning/" title="machine learning">machine learning<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/NLP/" title="NLP">NLP<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/algorithm/" title="algorithm">algorithm<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Q-learning/" title="Q-learning">Q-learning<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/bugs/" title="bugs">bugs<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/deepmind/" title="deepmind">deepmind<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ubuntu/" title="ubuntu">ubuntu<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/blog/" title="blog">blog<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/githup-pages/" title="githup pages">githup pages<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/backup/" title="backup">backup<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hexo/" title="hexo">hexo<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/data-structure/" title="data structure">data structure<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/software/" title="software">software<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/statistical/" title="statistical">statistical<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/work-tips/" title="work tips">work tips<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Latex/" title="Latex">Latex<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://github.com" target="_blank" title="github">github</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.tsinghua.edu.cn/publish/newthu/index.html" target="_blank" title="Tsinghua">Tsinghua</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Dagui Chen in Tsinghua University. <br/>
			This is my blog, believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/chendagui16" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		<a href="https://www.facebook.com/chendagui16" target="_blank" class="icon-facebook" title="facebook"></a>
		
		
		
		
		
		
		<a href="mailto:goblin_chen@163.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2017 
		
		<a href="/about" target="_blank" title="Dagui Chen">Dagui Chen</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"懒得跟你港"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 


<script type="text/javascript">

var disqus_shortname = 'chendagui16';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End --><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  </body>
</html>
