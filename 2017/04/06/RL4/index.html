
 <!DOCTYPE HTML>
<html lang="Chinese">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>RL4 Model-Free Prediction | My Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Dagui Chen">
    

    
    <meta name="description" content="Lecture 4 Model-Free PredictionEstimate the value function of an unknown MDP
Monte-Carlo Reinforcement learning
MC methods learn directly from episodes of experience
MC is model-free: no knowledge of">
<meta property="og:type" content="article">
<meta property="og:title" content="RL4 Model-Free Prediction">
<meta property="og:url" content="http://yoursite.com/2017/04/06/RL4/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="Lecture 4 Model-Free PredictionEstimate the value function of an unknown MDP
Monte-Carlo Reinforcement learning
MC methods learn directly from episodes of experience
MC is model-free: no knowledge of">
<meta property="og:image" content="http://i2.muimg.com/567571/5c127fc9c91b8549.png">
<meta property="og:image" content="http://i4.buimg.com/567571/7ff4a8d6a853c8e6.png">
<meta property="og:image" content="http://i1.piimg.com/567571/2a0f5cc516550dc4.png">
<meta property="og:image" content="http://i1.piimg.com/567571/6122d65e7fd835a5.png">
<meta property="og:image" content="http://i4.buimg.com/567571/718d23886e0d9e53.png">
<meta property="og:updated_time" content="2017-04-09T13:50:01.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL4 Model-Free Prediction">
<meta name="twitter:description" content="Lecture 4 Model-Free PredictionEstimate the value function of an unknown MDP
Monte-Carlo Reinforcement learning
MC methods learn directly from episodes of experience
MC is model-free: no knowledge of">
<meta name="twitter:image" content="http://i2.muimg.com/567571/5c127fc9c91b8549.png">

    
    <link rel="alternative" href="/atom.xml" title="My Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="My Blog" title="My Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="My Blog">My Blog</a></h1>
				<h2 class="blog-motto">This is my blog to record some knowledge</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:yoursite.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/06/RL4/" title="RL4 Model-Free Prediction" itemprop="url">RL4 Model-Free Prediction</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Dagui Chen" target="_blank" itemprop="author">Dagui Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-06T11:22:03.000Z" itemprop="datePublished"> Published 2017-04-06</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Lecture-4-Model-Free-Prediction"><span class="toc-number">1.</span> <span class="toc-text">Lecture 4 Model-Free Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Monte-Carlo-Reinforcement-learning"><span class="toc-number">1.1.</span> <span class="toc-text">Monte-Carlo Reinforcement learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MC-Policy-Evaluation"><span class="toc-number">1.1.1.</span> <span class="toc-text">MC Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#First-Visit-MC-Policy-Evaluation"><span class="toc-number">1.1.2.</span> <span class="toc-text">First-Visit MC Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Every-Visit-MC-Policy-Evaluation"><span class="toc-number">1.1.3.</span> <span class="toc-text">Every-Visit MC Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incremental-MC"><span class="toc-number">1.1.4.</span> <span class="toc-text">Incremental MC</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Difference-Learning"><span class="toc-number">1.2.</span> <span class="toc-text">Temporal-Difference Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-algorithm"><span class="toc-number">1.2.1.</span> <span class="toc-text">TD algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias-Variance-Trade-off"><span class="toc-number">1.2.2.</span> <span class="toc-text">Bias/Variance Trade-off</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-MC-and-TD"><span class="toc-number">1.2.3.</span> <span class="toc-text">Batch MC and TD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Certainty-Equivalence"><span class="toc-number">1.2.4.</span> <span class="toc-text">Certainty Equivalence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-and-Disadvantages-of-MC-vs-TD"><span class="toc-number">1.2.5.</span> <span class="toc-text">Advantages and Disadvantages of MC vs. TD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unified-View"><span class="toc-number">1.2.6.</span> <span class="toc-text">Unified View</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bootstrapping-and-Sampling"><span class="toc-number">1.2.7.</span> <span class="toc-text">Bootstrapping and Sampling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TD-lambda"><span class="toc-number">1.3.</span> <span class="toc-text">TD($\lambda$)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#n-Step-Return"><span class="toc-number">1.3.1.</span> <span class="toc-text">n-Step Return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Averaging-n-Step-Returns"><span class="toc-number">1.3.2.</span> <span class="toc-text">Averaging n-Step Returns</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lambda-return"><span class="toc-number">1.3.3.</span> <span class="toc-text">$\lambda$ return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Forward-view-TD-lambda"><span class="toc-number">1.3.4.</span> <span class="toc-text">Forward-view TD($\lambda$)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backward-view-TD-lambda"><span class="toc-number">1.3.5.</span> <span class="toc-text">Backward-view TD($\lambda$)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relationship-Between-Forward-and-Backward-TD"><span class="toc-number">1.3.6.</span> <span class="toc-text">Relationship Between Forward and Backward TD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Forward-and-Backward-Equivalence"><span class="toc-number">1.3.7.</span> <span class="toc-text">Forward and Backward Equivalence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-of-Forward-and-Backward-TD-lambda"><span class="toc-number">1.3.8.</span> <span class="toc-text">Summary of Forward and Backward TD($\lambda$)</span></a></li></ol></li></ol></li></ol>
		
		</div>
		
		<h1 id="Lecture-4-Model-Free-Prediction"><a href="#Lecture-4-Model-Free-Prediction" class="headerlink" title="Lecture 4 Model-Free Prediction"></a>Lecture 4 Model-Free Prediction</h1><p>Estimate the value function of an unknown MDP</p>
<h2 id="Monte-Carlo-Reinforcement-learning"><a href="#Monte-Carlo-Reinforcement-learning" class="headerlink" title="Monte-Carlo Reinforcement learning"></a>Monte-Carlo Reinforcement learning</h2><ul>
<li>MC methods learn directly from episodes of experience</li>
<li>MC is model-free: no knowledge of MDP transition/rewards</li>
<li>MC learns from complete episodes: no bootstrapping</li>
<li>MC uses the simplest possible idea: value = mean return</li>
<li>Caveat: can only apply MC to episodic MDPs<ul>
<li>All episodes must terminate</li>
</ul>
</li>
</ul>
<h3 id="MC-Policy-Evaluation"><a href="#MC-Policy-Evaluation" class="headerlink" title="MC Policy Evaluation"></a>MC Policy Evaluation</h3><ul>
<li><strong>Goal</strong>: learn $v_{\pi}$ from episodes of experience under policy $\pi$, $S_1,A_1,R_2,\dots,S_k \sim \pi $</li>
<li>Recall that the return is the total discounted reward: $G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_{T} $</li>
<li>Recall that the value function is the expected return: $v_{\pi} (s) = E_{\pi} [G_t|S_t =s]$</li>
<li>MC policy evaluation uses empirical mean return instead of expected return</li>
</ul>
<h3 id="First-Visit-MC-Policy-Evaluation"><a href="#First-Visit-MC-Policy-Evaluation" class="headerlink" title="First-Visit MC Policy Evaluation"></a>First-Visit MC Policy Evaluation</h3><ul>
<li>To evaluate state $s$</li>
<li>The <strong>first</strong> time-step $t$ that state $s$ is visited in an episode</li>
<li>Increment counter $N(s) \longleftarrow N(s) +1 $</li>
<li>Increment total return $S(s) \longleftarrow S(s) +G_t$</li>
<li>Value is estimated by mean return $V(s) = S(s)/N(s) $</li>
<li>By law of large numbers, $V(s) \rightarrow v_{\pi} (s) $ as $N(s) \rightarrow \infty$</li>
</ul>
<h3 id="Every-Visit-MC-Policy-Evaluation"><a href="#Every-Visit-MC-Policy-Evaluation" class="headerlink" title="Every-Visit MC Policy Evaluation"></a>Every-Visit MC Policy Evaluation</h3><ul>
<li>To evaluate state $s$</li>
<li><strong>Every</strong> time-step $t$ that state $s$ is visited in an episode</li>
<li>Increment counter $N(s) \longleftarrow N(s) +1 $</li>
<li>Increment total return $S(s) \longleftarrow S(s) +G_t$</li>
<li>Value is estimated by mean return $V(s) = S(s)/N(s) $</li>
<li>By law of large numbers, $V(s) \rightarrow v_{\pi} (s) $ as $N(s) \rightarrow \infty$</li>
</ul>
<h3 id="Incremental-MC"><a href="#Incremental-MC" class="headerlink" title="Incremental MC"></a>Incremental MC</h3><p><strong>Incremental Mean</strong><br>The mean $\mu_1,\mu_2,\dots$ of a sequence $x_1,x_2,\dots$ can be computed incrementally,<br>$$\mu_k = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1}) $$</p>
<p><strong>Incremental MC updates</strong></p>
<ul>
<li>Update $V(s)$ incrementally after episode $S_1,A_1,R_1,\dots,S_T$</li>
<li>For each state $S_t$ with return $G_t$.<br>$$ N(S_t) \leftarrow N(S_t) + 1 $$<br>$$ V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t)) $$ </li>
<li>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes<br>$$ V(S_t) \leftarrow V(S_t) + \alpha (G-V(S_t)) $$</li>
</ul>
<p>$\alpha$ is the update rate.</p>
<h2 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal-Difference Learning"></a>Temporal-Difference Learning</h2><ul>
<li>TD method learn directly from episodes of experience</li>
<li>TD is <em>model-free</em>: no knowledge of MDP transitions/rewards</li>
<li>TD learns from <em>incomplete</em> episodes, by bootstrapping</li>
<li>TD updates a guess towards a guess</li>
</ul>
<h3 id="TD-algorithm"><a href="#TD-algorithm" class="headerlink" title="TD algorithm"></a>TD algorithm</h3><ul>
<li><strong>Goal</strong>: learn $v_{\pi}$ online from experience under policy $\pi$</li>
<li>Incremental every-visit MC<ul>
<li>Update value $V(S_t)$ toward actual return $G_t$<br>$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t)) $$</li>
</ul>
</li>
<li>Simplest temporal-difference learning algorithm: TD(0)<ul>
<li>Update value $V(S_t)$ toward estimated return $R_{t+1} + \gamma V(S_{t+1})$<br>$$ V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) -V(S_t)) $$</li>
<li>$R_{t+1}+\gamma V(S_{t+1})$ is called the TD target</li>
<li>$\delta_t = R_{t+1} + \gamma V(S_{t+1}) -V(S_t)$ is called the TD error</li>
</ul>
</li>
</ul>
<h3 id="Bias-Variance-Trade-off"><a href="#Bias-Variance-Trade-off" class="headerlink" title="Bias/Variance Trade-off"></a>Bias/Variance Trade-off</h3><ul>
<li>Return $G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T$ is unbiased estimate of $v_{\pi} (S_t)$</li>
<li>True TD target $R_{t+1} + \gamma v_{\pi} (S_{t+1}) $ is unbiased estimate of $v_{\pi}(S_t)$</li>
<li>TD target $R_{t+1} + \gamma V(S_{t+1})$ is biased estimate of $v_{\pi} (S_t)$</li>
<li>TD target is much lower variance than the return<ul>
<li>Return depends on many random actions, transitions, rewards</li>
<li>TD target depends on one random action, transition, reward</li>
</ul>
</li>
</ul>
<h3 id="Batch-MC-and-TD"><a href="#Batch-MC-and-TD" class="headerlink" title="Batch MC and TD"></a>Batch MC and TD</h3><ul>
<li>MC and TD converge: $V(s) \rightarrow v_{\pi} (s)$ as experience $\rightarrow \infty$</li>
<li>But what about batch solution for finite experience?<ul>
<li>e.g. Repeatedly sample episode $k\in [1,K] $</li>
<li>Apply MC or TD(0) to episode $k$</li>
</ul>
</li>
</ul>
<h3 id="Certainty-Equivalence"><a href="#Certainty-Equivalence" class="headerlink" title="Certainty Equivalence"></a>Certainty Equivalence</h3><ul>
<li><p>MC converges to solution with minimum mean-squared error</p>
<ul>
<li>Best fit to the observed returns<br>$$ \sum_{k=1}^{K} \sum_{t=1}^{T_k} (G_t^k -V(s_t^k))^2 $$</li>
</ul>
</li>
<li><p>TD(0) converges to solution of max likelihood Markov model</p>
<ul>
<li>Solution to the MDP $(S,A,\hat{P},\hat{R},\gamma)$ that best fits the data<br>$$ \hat{P}_{s,s’}^a = \frac{1}{N(s,a)} \sum_{k=1}^{K} \sum_{t=1}^{T_k} 1(s_t^k,a_t^k,s_{t+1}^k = s,a,s’) $$<br>$$ \hat{R}_s^a = \frac{1}{N(s,a)} \sum_{k=1}^{K} \sum_{t=1}^{T_k} 1(s_t^k,a_t^k = s,a) r_t^k $$ <h3 id="Advantages-and-Disadvantages-of-MC-vs-TD"><a href="#Advantages-and-Disadvantages-of-MC-vs-TD" class="headerlink" title="Advantages and Disadvantages of MC vs. TD"></a>Advantages and Disadvantages of MC vs. TD</h3></li>
</ul>
</li>
<li>TD can learn before knowing the final outcome<ul>
<li>TD can learn online after every step</li>
<li>MC must wait until end of episode before return is known</li>
</ul>
</li>
<li>TD can learn without the final outcome<ul>
<li>TD can learn from incomplete sequences</li>
<li>MC can only learn from complete sequences</li>
<li>TD works in continuing (non-terminating) environments</li>
<li>MC only works for episodic (terminating) environments</li>
</ul>
</li>
<li>MC has high variance, zero bias<ul>
<li>Good convergence properties</li>
<li>(even with function approximation)</li>
<li>Not every sensitive to initial value</li>
<li>Very simple to understand and use</li>
</ul>
</li>
<li>TD has low variance, some bias<ul>
<li>Usually more efficient than MC</li>
<li>TD(0) converges to $v_{\pi} (s)$</li>
<li>(but not always with function approximation)</li>
<li>More sensitive to initial value</li>
</ul>
</li>
<li>TD exploits Markov property<ul>
<li>Usually more efficient in Markov environments</li>
</ul>
</li>
<li>MC does not exploit Markov property<ul>
<li>Usually more efficient in non-Markov environments</li>
</ul>
</li>
</ul>
<h3 id="Unified-View"><a href="#Unified-View" class="headerlink" title="Unified View"></a>Unified View</h3><p>Monte-Carlo Backup<br><img src="http://i2.muimg.com/567571/5c127fc9c91b8549.png" alt="MC Backup"><br>Temporal-Difference Backup<br><img src="http://i4.buimg.com/567571/7ff4a8d6a853c8e6.png" alt="TD Backup"><br>Dynamic Programming Backup<br><img src="http://i1.piimg.com/567571/2a0f5cc516550dc4.png" alt="DP Backup"><br>Unified View of Reinforcement Learning<br><img src="http://i1.piimg.com/567571/6122d65e7fd835a5.png" alt="Unified View of RL"></p>
<h3 id="Bootstrapping-and-Sampling"><a href="#Bootstrapping-and-Sampling" class="headerlink" title="Bootstrapping and Sampling"></a>Bootstrapping and Sampling</h3><ul>
<li><strong>Bootstrapping</strong>: update involves an estimate<ul>
<li>MC dose not bootstrap</li>
<li>DP/TD bootstraps</li>
</ul>
</li>
<li><strong>Sampling</strong>: update samples an expectation<ul>
<li>MC/TD samples</li>
<li>DP does not sample</li>
</ul>
</li>
</ul>
<h2 id="TD-lambda"><a href="#TD-lambda" class="headerlink" title="TD($\lambda$)"></a>TD($\lambda$)</h2><h3 id="n-Step-Return"><a href="#n-Step-Return" class="headerlink" title="n-Step Return"></a>n-Step Return</h3><ul>
<li>Consider the following n-step returns for $n=1,2,\dots$<ul>
<li>n=1 (TD) $G_t^{(1)}=R_{t+1} + \gamma V(S_{t+1})$</li>
<li>n=2 $G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$</li>
<li>n=$\infty$, $G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T $</li>
</ul>
</li>
<li>Define the n-step return<br>$$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^{n} V(S_{t+n})$$</li>
<li>n-step temporal-difference learning<br>$$ V(S_t) \leftarrow V(S_t) + \alpha (G_{(n)} - V(S_t)) $$</li>
</ul>
<h3 id="Averaging-n-Step-Returns"><a href="#Averaging-n-Step-Returns" class="headerlink" title="Averaging n-Step Returns"></a>Averaging n-Step Returns</h3><ul>
<li>We can average n-step returns over different n</li>
<li>e.g average the 2-step and 4-step returns $\frac{1}{2} G^{(2)} + \frac{1}{2} G^{(4)}$</li>
<li>Combines information from two different time-steps</li>
<li>Can we efficiently combine information from all time-steps</li>
</ul>
<h3 id="lambda-return"><a href="#lambda-return" class="headerlink" title="$\lambda$ return"></a>$\lambda$ return</h3><ul>
<li>The $\lambda-$return $G_t^{\lambda}$ combines all n-step return $G_t^{(n)}$</li>
<li>Using weight $(1-\lambda)\lambda^{n-1}$<br>$$ G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} $$</li>
<li>Forward-view TD($\lambda$)<br>$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t^{\lambda} -V(S_t)) $$</li>
</ul>
<h3 id="Forward-view-TD-lambda"><a href="#Forward-view-TD-lambda" class="headerlink" title="Forward-view TD($\lambda$)"></a>Forward-view TD($\lambda$)</h3><p>$$ G_{t}^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} $$</p>
<ul>
<li>Update value function towards the $\lambda$-return</li>
<li>Forward-view looks into the future to compute $G_t^{\lambda}$</li>
<li>Like MC, can only be computed from complete episodes</li>
</ul>
<h3 id="Backward-view-TD-lambda"><a href="#Backward-view-TD-lambda" class="headerlink" title="Backward-view TD($\lambda$)"></a>Backward-view TD($\lambda$)</h3><ul>
<li>Forward view provides theory</li>
<li>Backward view provides mechanism</li>
<li>Update online, every step, from incomplete sequences</li>
</ul>
<h3 id="Relationship-Between-Forward-and-Backward-TD"><a href="#Relationship-Between-Forward-and-Backward-TD" class="headerlink" title="Relationship Between Forward and Backward TD"></a>Relationship Between Forward and Backward TD</h3><p><strong>TD($\lambda$) and TD(0)</strong></p>
<ul>
<li>when $\lambda=0$, only current state is updated<br>$$ E_t (s) = 1(S_t =s) \qquad V(s) \leftarrow V(s) + \alpha \delta_t E_t(s)$$</li>
<li>This is exactly equivalent to TD(0) update<br>$$ V(S_t) \leftarrow V(S_t) + \alpha \delta_t $$</li>
</ul>
<p><strong>TD($\lambda$) and MC</strong></p>
<ul>
<li>When $\lambda=1$, credit is deferred until end of episode</li>
<li>Consider episodic environments with offline updates</li>
<li>Over the course of an episode, total update for TD(1) is the same as total update for MC<blockquote>
<p><strong>Theorem</strong><br>The sum of offline updates is identical for forward-view and backward-view TD($\lambda$)<br>$$ \sum_{t=1}^{T} \alpha \delta_t E_t(s) = \sum_{t=1}^T \alpha (G_t^{\lambda} - V(S_t) ) 1(S_t =s) $$</p>
</blockquote>
</li>
</ul>
<h3 id="Forward-and-Backward-Equivalence"><a href="#Forward-and-Backward-Equivalence" class="headerlink" title="Forward and Backward Equivalence"></a>Forward and Backward Equivalence</h3><p><strong>MC and TD(1)</strong></p>
<ul>
<li>Consider an episode where $s$ is visited once at time-step $k$.</li>
<li>TD(1) eligibility trace discounts time since visit,<br>$$ E_t(s) = \gamma E_{t-1} (s) + 1(S_t = s) = \begin{cases} 0 &amp; \text{if}~ t&lt;k \\  \gamma^{t-k} &amp; \text{if} ~ t\ge k \end{cases}$$</li>
<li>TD(1) updates accumulate error online<br>$$ \sum_{t=1}^{T-1} \alpha \delta_t E_t(s) = \alpha \sum_{t=k}^{T-1} \gamma^{t-k} \delta_t = \alpha (G_k - V(S_k)) $$</li>
<li>By end of episode it accumulates total error<br>$$ \delta_k + \gamma \delta_{k+1} + \gamma^2 \delta_{k+2} + \dots + \gamma^{T-1-k} \delta_{T-1} $$</li>
</ul>
<p><strong>TD(\lambda) and TD(1)</strong></p>
<ul>
<li>TD(1) is roughly equivalent to every-visit Monte-Carlo</li>
<li>Error is accumulated online, step-by-step</li>
<li>If value function is only updated offline at end of episode</li>
<li>Then total update is exactly the same as MC</li>
</ul>
<p><strong>Forward and Backwards TD($\lambda$)</strong></p>
<ul>
<li>Consider an episode where $s$ is visited once at time-step $k$</li>
<li>TD($\lambda$) eligibility trace discounts time since visit<br>$$ E_t(s) = \gamma\lambda E_{t-1} (s) + 1(S_t = s) = \begin{cases} 0 &amp; \text{if}~ t&lt;k \\  (\gamma\lambda)^{t-k} &amp; \text{if} ~ t\ge k \end{cases}$$</li>
<li>Backward TD($\lambda$) updates accumulate error online<br>$$ \sum_{t=1}^{T} \alpha \delta_t E_t(s) = \alpha \sum_{t=k}^T (\gamma\lambda)^{t-k} \delta_t = \alpha(G_k^\lambda - V(S_k)) $$</li>
<li>By end of episode it accumulates total error for $\lambda$-return</li>
<li>For multiple visits to $s$, $E_t(s)$ accumulates many errors</li>
</ul>
<p><strong>Offline Equivalence of Forward and Backward TD</strong><br>Offline updates</p>
<ul>
<li>Updates are accumulated within episode</li>
<li>but applied in batch at the end of episode</li>
</ul>
<p>Online updates</p>
<ul>
<li>TD($\lambda$) updates are applied online at each step within episode</li>
<li>Forward and backward-view TD($\lambda$) are slightly different</li>
<li><strong>New</strong>: Exact online TD($\lambda$) achieves perfect equivalence</li>
<li>By using a slightly differently form of eligibility trace</li>
</ul>
<h3 id="Summary-of-Forward-and-Backward-TD-lambda"><a href="#Summary-of-Forward-and-Backward-TD-lambda" class="headerlink" title="Summary of Forward and Backward TD($\lambda$)"></a>Summary of Forward and Backward TD($\lambda$)</h3><p><img src="http://i4.buimg.com/567571/718d23886e0d9e53.png" alt="Summary of Forward and Backward TD($\lambda$)"></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/reinforcement-learning/">reinforcement learning</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/reinforcement-learning/">reinforcement learning</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://yoursite.com/2017/04/06/RL4/" data-title="RL4 Model-Free Prediction | My Blog" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/04/10/RL5/" title="RL5 Model-Free Control">
  <strong>上一篇：</strong><br/>
  <span>
  RL5 Model-Free Control</span>
</a>
</div>


<div class="next">
<a href="/2017/04/05/RL3/"  title="RL3 Planning by Dynamic Programming">
 <strong>下一篇：</strong><br/> 
 <span>RL3 Planning by Dynamic Programming
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2017/04/06/RL4/" data-title="RL4 Model-Free Prediction" data-url="http://yoursite.com/2017/04/06/RL4/"></div>
</section>


<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Lecture-4-Model-Free-Prediction"><span class="toc-number">1.</span> <span class="toc-text">Lecture 4 Model-Free Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Monte-Carlo-Reinforcement-learning"><span class="toc-number">1.1.</span> <span class="toc-text">Monte-Carlo Reinforcement learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MC-Policy-Evaluation"><span class="toc-number">1.1.1.</span> <span class="toc-text">MC Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#First-Visit-MC-Policy-Evaluation"><span class="toc-number">1.1.2.</span> <span class="toc-text">First-Visit MC Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Every-Visit-MC-Policy-Evaluation"><span class="toc-number">1.1.3.</span> <span class="toc-text">Every-Visit MC Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incremental-MC"><span class="toc-number">1.1.4.</span> <span class="toc-text">Incremental MC</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Difference-Learning"><span class="toc-number">1.2.</span> <span class="toc-text">Temporal-Difference Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-algorithm"><span class="toc-number">1.2.1.</span> <span class="toc-text">TD algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias-Variance-Trade-off"><span class="toc-number">1.2.2.</span> <span class="toc-text">Bias/Variance Trade-off</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-MC-and-TD"><span class="toc-number">1.2.3.</span> <span class="toc-text">Batch MC and TD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Certainty-Equivalence"><span class="toc-number">1.2.4.</span> <span class="toc-text">Certainty Equivalence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-and-Disadvantages-of-MC-vs-TD"><span class="toc-number">1.2.5.</span> <span class="toc-text">Advantages and Disadvantages of MC vs. TD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unified-View"><span class="toc-number">1.2.6.</span> <span class="toc-text">Unified View</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bootstrapping-and-Sampling"><span class="toc-number">1.2.7.</span> <span class="toc-text">Bootstrapping and Sampling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TD-lambda"><span class="toc-number">1.3.</span> <span class="toc-text">TD($\lambda$)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#n-Step-Return"><span class="toc-number">1.3.1.</span> <span class="toc-text">n-Step Return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Averaging-n-Step-Returns"><span class="toc-number">1.3.2.</span> <span class="toc-text">Averaging n-Step Returns</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lambda-return"><span class="toc-number">1.3.3.</span> <span class="toc-text">$\lambda$ return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Forward-view-TD-lambda"><span class="toc-number">1.3.4.</span> <span class="toc-text">Forward-view TD($\lambda$)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backward-view-TD-lambda"><span class="toc-number">1.3.5.</span> <span class="toc-text">Backward-view TD($\lambda$)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relationship-Between-Forward-and-Backward-TD"><span class="toc-number">1.3.6.</span> <span class="toc-text">Relationship Between Forward and Backward TD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Forward-and-Backward-Equivalence"><span class="toc-number">1.3.7.</span> <span class="toc-text">Forward and Backward Equivalence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-of-Forward-and-Backward-TD-lambda"><span class="toc-number">1.3.8.</span> <span class="toc-text">Summary of Forward and Backward TD($\lambda$)</span></a></li></ol></li></ol></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github Card</p>
<div class="github-card" data-github="chendagui16" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/Latex/" title="Latex">Latex<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/algorithm/" title="algorithm">algorithm<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/code-analyze/" title="code analyze">code analyze<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/shortcuts/" title="shortcuts">shortcuts<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/software-setting/" title="software setting">software setting<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/work-tips/" title="work tips">work tips<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/deepmind/" title="deepmind">deepmind<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ubuntu/" title="ubuntu">ubuntu<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Q-learning/" title="Q-learning">Q-learning<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/bugs/" title="bugs">bugs<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/blog/" title="blog">blog<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/algorithm/" title="algorithm">algorithm<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hexo/" title="hexo">hexo<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/shortcuts/" title="shortcuts">shortcuts<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/software/" title="software">software<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/statistical/" title="statistical">statistical<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Latex/" title="Latex">Latex<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/githup-pages/" title="githup pages">githup pages<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/backup/" title="backup">backup<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://github.com" target="_blank" title="github">github</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.tsinghua.edu.cn/publish/newthu/index.html" target="_blank" title="Tsinghua">Tsinghua</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Dagui Chen in Tsinghua University. <br/>
			This is my blog, believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/chendagui16" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		<a href="https://www.facebook.com/chendagui16" target="_blank" class="icon-facebook" title="facebook"></a>
		
		
		
		
		
		
		<a href="mailto:goblin_chen@163.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2017 
		
		<a href="/about" target="_blank" title="Dagui Chen">Dagui Chen</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"懒得跟你港"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 


<script type="text/javascript">

var disqus_shortname = 'chendagui16';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End --><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  </body>
</html>
