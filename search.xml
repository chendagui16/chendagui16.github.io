<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[vim]]></title>
    <url>%2F2017%2F10%2F03%2Fvim%2F</url>
    <content type="text"><![CDATA[实用的vim配置 作为一个工具控，平日里就喜欢配置一些好用的工具。俗话说，工欲善其事，必先利其器，而vim可以说就是这里面的尚方宝剑了。 话说我身边使用vim的人大致分为两种：第一类是由于特殊原因（比如在服务器）无法使用图形界面，所以被迫使用了这样的可以在命令汗工作的编辑工具；第二类是主动去学习这样一款“世界上最好的编辑器”，并真心觉得它很cool的人。我的这篇博客也是面对这样的人。 至于对vim的安利，我就不说了，网上搜索下来可以搜到一大把。我用我个人感受来描述它，就是：学会它，你会自愿抛弃所有其他的编辑器，你会变得更有效率，更cool！然而vim的使用效率是和你对它的配置能力成正比的，网上有很多分享vim配置的博客，但是对于大部分萌新来说，直接使用别人的配置文件确实上手很快，但是每个人的需求不同，不学会有效的自定义，怎么可能用好这一“神器”呢？这也成为我写这一片博客的目的了，不仅仅分享vim的配置，同时也会对配置进行一定的解析。当然，由于个人接触vim时间也不太长，所以对于高手来说，可能会觉得我写得比较菜，还望指正，共同学习 这篇博客主要是分享一些有用的vim插件，分享一些好的自定义配置。它适合的读者是已经对vim的一些基本操作比较熟悉，并渴望进一步玩转vim的人。 主要配置参考aixvim. 安装我的配置要求本文中的配置环境主要需要： vim8 python支持 为什么需要vim8？因为快！vim8可以支持并发，有很多好用的并发的插件不能用岂不是很可惜。至于python支持，主要是为了一些需要python的插件，尤其是youcompleteme，有了它，可以告别IDE了！ 怎么看自己的环境是否满足要求？终端运行 1vim --version 观察是否有 1VIM - Vi IMproved 8.0 在feature是否有 1+python # 加号表示支持这种特性 如何获得vim8和python支持？对于mac，可以使用homebrew直接安装即可。 1brew install vim 对于linux来说，使用apt-get安装时，默认情况下不会安装8.0，这里给出一个从源码编译的安装方式，可以方便定制。 安装依赖 1234sudo apt-get install libncurses5-dev libgnome2-dev libgnomeui-dev \ libgtk2.0-dev libatk1.0-dev libbonoboui2-dev \ libcairo2-dev libx11-dev libxpm-dev libxt-dev python-dev \ python3-dev ruby-dev lua5.1 lua5.1-dev libperl-dev git 安装编译vim 12345678910111213cd ~git clone https://github.com/vim/vim.gitcd vim./configure --with-features=huge \ --enable-multibyte \ --enable-rubyinterp=yes \ --enable-pythoninterp=yes \ --with-python-config-dir=/usr/lib/python2.7/config \ --enable-perlinterp=yes \ --enable-luainterp=yes \ --enable-gui=gtk2 \ --enable-cscope \ --prefix=/usr/local 注意到上面开启了很多特性，包括ruby，python，lua等，如果不需要，可以去掉上面的选项。另外对于—with-python-config-dir的选项，需要将后面的路径选择成为自己电脑对应的选项，比如/usr/lib64/python2.7/config-2.7m-x86_64-linux-gnu. 另外，如果要使用python3，需要将上面的有关python的选项替换为 12--enable-python3interp=yes \--with-python3-config-dir=/usr/lib/python3.5/config \ 编译 1make &amp;&amp; make install 按照这个流程，会将vim的命令安装在/usr/local/bin的路径下，确认该路径在PATH环境变量下。 没有权限？有时，我们会碰到没有sudo权限的情况，比如在服务器上，我们只需要将上面configure命令下的选项替换成 1--prefix=$HOME 然后再编译，这时的vim命令会被安装到$HOME/bin中，将其加入PATH路径即可。（如果使用zsh，可以将.bashrc替换成.zshrc） 12echo 'export PATH="$HOME/bin:$PATH"' &gt;&gt; $HOME/.bashrcsource $HOME/.bashrc 安装 终端中输入 12curl -o install_vim.sh https://raw.githubusercontent.com/chendagui16/ConfigFile/master/install_vim.shsh install_vim.sh 进入vim，输入 1:PlugInstall 针对的环境 该配置文件使用于linux, mac操作系统，并针对这两个系统做了相对应的键位映射，为了简便，所有的配置均在$HOME/.vimrc的文件中. 通用设置以下是一些有用的通用设置，无关任何插件，自定义了一些快捷键，也打开了一些方便的功能。同时自定了一些有用的功能。 这里主要介绍一些比较特殊的功能，一些比较常见的设置见vimrc的注释。 系统剪贴板1set clipboard=unnamed 我们经常会需要将vim中的内容复制到系统剪贴板，或者将系统剪贴板中的文字粘贴到vim中，这个时候我们就需要在复制，粘贴或者剪切时使用一个特殊的前缀&quot;*或者&quot;+, 比如： 12"*yy "*p "*dd "+yy "+p "+dd 加入这个前缀就会使用系统的剪贴板。另外如果clipboard的选项中包含”unamed”的字符串，则就会默认使用系统的剪贴板，因此可以不用输入&quot;*或&quot;+。 Leader键12let mapleader="\&lt;Space&gt;"let g:mapleader="\&lt;Space&gt;" 默认的leader键为backslash, 为了方便，我们把leader键改成了Space。 命令补全对于vim的命令是可以按tab键进行补全的，但是默认的补全确实不太方便。我们通过一些设置可以进行修改 1234set wildmenuset wildmode=longest,fullset completeopt=menu,menuone,longestset completeopt+=preview 可以获得补全效果如下，可以按\和\+\选择 Undo我们经常会碰到这样的情况，我们需要编辑完文件后退出，然后发现改错了，需要回退。然而使用vim中的u键回退，不能回退到过去的历史，因此我们设置如下配置 12set undofileset undodir=~/.vim/undo/ 将所有的历史存储到了~/.vim/undo的文件夹。并可以支持无限回溯。 Tab与空格有时候，我们会对tab和空格进行区分，（比如在python里面，混合使用tab和空格作为缩进是会导致运行报错的）。因此我们这里提供了一些特殊的方法区分，并转换。 高亮Tab 为了区分tab和空格的显示，我们做了如下的设置 1set list listchars=tab:-\ ,extends:&gt;,precedes:&lt; 显示的效果如下，tab会以一个短横线显示： Space2Tab, Tab2Space 同时为了也定义了一系列的函数可以转换Tab和Space：Space2Tab, Tab2Space。这两个函数可以支持命令补全，一键运行，可以将Space转换成Tab，也可以将Tab转换成Space。这两个命令不仅仅支持在normal模式下运行，同时也支持在visual模式下指定特殊的范围运行。 Split切换vim在不同窗口下切换的默认键位是Ctrl-w+h(jkl)，这里做了一些修改 1234nnoremap &lt;leader&gt;hh &lt;C-w&gt;hnnoremap &lt;leader&gt;jj &lt;C-w&gt;jnnoremap &lt;leader&gt;kk &lt;C-w&gt;knnoremap &lt;leader&gt;ll &lt;C-w&gt;l 插入模式下的光标快速定位12345inoremap &lt;C-e&gt; &lt;End&gt;inoremap &lt;C-f&gt; &lt;Home&gt;inoremap &lt;C-d&gt; &lt;Esc&gt;VypAinoremap &lt;C-w&gt; &lt;C-o&gt;winoremap &lt;C-b&gt; &lt;C-o&gt;b 这里Ctrl+d可以快速复制当前行，并粘贴到下一行，类似于很多其他编辑器一样。 语法检查和纠正12345678" &lt;leader&gt;ss: Spell checking shortcuts" fold enable settingsnnoremap &lt;leader&gt;ss :setlocal spell!&lt;CR&gt;nnoremap &lt;leader&gt;sj ]szznnoremap &lt;leader&gt;sk [szznnoremap &lt;leader&gt;sa zg]szznnoremap &lt;leader&gt;sd 1z=nnoremap &lt;leader&gt;sf z= 在normal模式下使用leader+ss，可以开启或关闭拼写检查，leader+sj(k)，可以将光标定位到下一个（上一个）拼写错误的位置，使用leader+sd，表示自动纠正该错误，leader+sf可以提供可能的纠正列表，并可以选择纠正项。 配置和插件说明:1. 插件管理对于所有插件来说，最重要的也是最根本的插件就是管理插件了。老牌的管理插件包括vundle，和pathogen。而我这里要强烈推荐的是vim-plug. 它包括以下几大优点： 简单，易于安装，相比其他管理插件启动速度快。 支持github管理，要安装其他的插件只需要将该插件对应的github名字加入到vimrc即可。例如，你在github中发现了一个支持git的插件vim-git，然后要将这个插件加入管理只需要将其仓库名加入vimrc即可。例如： 12" vimrc中双引号后表示注释，tpope/vim-git表示其github的仓库名Plug 'tpope/vim-git' 支持外部管理，可任意指定文件夹进行管理，如： 1call plug#begin('~/.vim/bundle') "将.vim/bundle指定为插件管理文件 杀手锏，支持并发。这一点是传统的管理插件所不具备的，所有的插件均可以在并发的安装或更新，也就是说你只需要拥有一个配置文件，在网路好的情况下，可以在几分钟内在任何一台机器上安装完你的所有插件。节约下的时间就是金钱啊！ 在vim的命令行输入:PlugInstall (可用tab补全），即可出现以下画面(以下gif来自官方github）： 除了PlugInstall外，常用的命令还包括PlugUpdate和PlugClean，可以直接一键更新，clean掉vimrc中已经删除的插件条目。 2. 主题主题的插件包括两个部分，第一个部分是颜色主题，由于我在linux，mac的终端上使用的色彩主题都是solarized的主题，为了保持习惯，这里我也使用了solarized的主题（个人推荐，看着很舒服）。这个主题包含两个配色方案，分别是dark和light。使用的方法是在vimrc中加入 12345678call plug#begin('~/.vim/bundle')... " 其他插件Plug 'altercation/vim-colors-solarized' "添加插件...call plug#end()... " 其他配置colorscheme solarized "使用主题set background=dark "选择配色 另外需要注意的是，在tmux中使用vim的时候，可能会出现主题颜色失真的情况，这个时候需要设置终端为256色 123set term=screen-256color或set t_Co=256 配色问题？在终端中使用vim的时候，如果直接使用solarized的主题，有可能会碰到终端的颜色和vim的颜色叠加的情况，导致显示变得很怪。为了解决这个问题，可以先将终端改成solarized的配色。方法如下： 123456789cdgit clone git://github.com/seebi/dircolors-solarized.git cp ~/dircolors-solarized/dircolors.256dark ~/.dircolorseval ‘dircolors .dircolors’echo 'export TERM=xterm-256color' &gt;&gt; $HOME/.bashrcsource $HOME/.bashrcgit clone https://github.com/coolwanglu/gnome-terminal-colors-solarizedcd gnome-terminal-colors-solarized./set_dark.sh 主题所包含的另一个部分是airline，这个主要是提供一个彩色的状态栏，除了好看之外，也能提供不少信息，其中能包括vim模式，文件名，git分支，是否修改，编码方式，结合一些语法插件，甚至可以显示语法错误等。功能比较丰富，推荐使用。使用效果如下： 在我的配置中，使用airline如下 1234567891011121314call plug#begin('~/.vim/bundle')... " 其他插件Plug 'vim-airline/vim-airline' "添加插件Plug 'vim-airline/vim-airline-themes'...call plug#end()... " 其他配置colorscheme solarized "使用主题set background=dark "选择配色" AirLine Settinglet g:airline_powerline_fonts = 1let g:airline_theme='serene'let g:airline_left_sep=''let g:airline_right_sep='' 这里的设置主要是因为我不太喜欢airline的箭头，所以把它取消了，效果如下： 字体不正确？airline使用了powerline的字体，其中包含了很多形象的符号字体，在使用airline时，如果终端中为包含该字体则需要安装： 12345git clone https://github.com/powerline/fonts.git --depth=1cd fonts./install.shcd ..rm -rf fonts 然后需要在对应终端中选择powerline字体 3. 文件搜索文件搜索，常用的vim插件是ctrlp，ctrlp的使用方法非常简单，按下ctrl+p即可调出搜索框。支持模糊匹配，可以搜索文件，MRU（Most Recently Used), vim-buffer, tags等。 不过我们这里推荐使用的一个插件叫fzf，用Go语言实现，利用了vim8的并发机制，可以实现几万文件秒搜。效率感人。而且，它不仅仅支持vim中的使用，也可以支持shell中火tmux中的命令搜索等功能。 在我的配置文件中，对于文件搜索的配置为： 12345Plug 'junegunn/fzf', &#123; 'dir': '~/.fzf', 'do': './install --all' &#125;Plug 'ctrlpvim/ctrlp.vim'...set rtp+=~/.fzfnnoremap z, :FZF --no-mouse .&lt;CR&gt; 这里将搜索的快捷键改成了z,，不用同时按下，在vim的normal模式下，依次键入即可调出搜索框，尝试一下，你会亲身感受其速度的。 另外，在终端下，使用ctrl+r可以搜索命令历史 内容搜索一般来说，我们搜索文件时只会搜索文件名。但是经常的，我们会搜索文件内容。这时可以使用ag.vim 可以批量搜索代码，搜索文件，支持模糊匹配，正则表达式，完爆各种IDE的搜索工具。需要安装the_silver_searcher 1234567Plug 'rking/ag.vim'..." Ag.vim Settings 官方推荐配置let g:ackprg = "ag --nocolor --nogroup --column"set grepprg=ag\ --nogroup\ --nocolorcommand -nargs=+ -complete=file -bar Ag silent! grep! &lt;args&gt;|cwindow|redraw!nnoremap \ :Ag&lt;SPACE&gt; " 搜索快捷键改成了\ 如下是我在一个工程目录下搜索show_info的结果 这个窗口是可交互的，可以Enter进入相应的文件 4. 文件buffer类插件这里使用了常用的nerdtree , nerdtree-git-plugin 1234567891011121314151617Plug 'scrooloose/nerdtree'Plug 'Xuyuanp/nerdtree-git-plugin'let g:NERDTreeRespectWildIgnore=1let g:NERDTreeDirArrows=0nnoremap &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;" =============================== Ignore setting =========================set wildignore=*.o,*.obj,*~ "stuff to ignore when tab completingset wildignore+=*/tmp/*,*.so,*.swp,*.zip " MacOSX/Linuxset wildignore+=*DS_Store*set wildignore+=vendor/rails/**set wildignore+=vendor/cache/**set wildignore+=*.gemset wildignore+=log/**set wildignore+=tmp/**set wildignore+=*.png,*.jpg,*.gifset wildignore+=*.so,*.swp,*.zip,*/.Trash/**,*.pdf,*.dmg,*/Library/**,*/.rbenv/**set wildignore+=*/.nx/**,*.app,*.git,.git 这里进行的主要的设置快捷键设置，F2可以打开或关闭nerdtree，同时设置了忽略文件加入buffer的管理。同时配合nerdtree-git-plugin插件，可以实现如下gif图显示的git相关操作 另外针对文件buffer，添加了mru和rename的插件 123Plug 'yegappan/mru'Plug 'danro/rename.vim'nnoremap &lt;leader&gt;uh :MRU&lt;CR&gt; leader键在我的配置中改成了Space 这两个插件的使用很简单，输入快捷键leader+uh（used history的简称），或在vim命令行输入:MRU，可以调出最近使用的文件名buffer，相当于IDE中的打开最近文件。 rename插件的使用方式即在vim命令行中输入 1:rename &#123;newname&#125; 5. Git类相关12345Plug 'tpope/vim-git'Plug 'tpope/vim-fugitive'Plug 'gregsexton/gitv'Plug 'airblade/vim-gitgutter'Plug 'mhinz/vim-signify' 这里的vim-git 服务于vim-fugitive ，而vim-fugitive可以说是最好用的有关git的vim插件了，它不仅仅提供了有关git的语法高亮，同时也提供了各种有关git的操作接口. 这个插件所包含的功能实在是难以简单的说明，观看其github中的screenshot可以了解其强大的功能 常用的命令有： 1234567891011:Gedit :Gsplit :Gvsplit :Gtabedit:Gstatus (在弹出的窗口使用-, p, C进行交互):Gcommit:Gblame (在弹出的窗口中使用o, :Gedit进行交互):Ggrep:Glog:Gread :Gwrite:Gdiff:Gbrowse:Gmove :Gdelete... gitv 是一个基于vim-fugitive的拓展，提供了一个命令:Gitv，这个拓展可以打开一个可以交互的git文件窗口，可以查看各种提交历史，diff等 vim-gitgutter和vim-signfy这两个插件提供了几乎一样的功能，但是vim-signify不仅仅支持git，还支持svn等. 同时修改配置 12let g:gitgutter_sign_column_always = 0let g:gitgutter_max_signs = 99999 支持的效果图如下 另外，这两个插件提供了]c,[c的命令，这两个命令在使用fugitive的Gdiff时，可以快速跳转到下一个(上一个)有diff的chrunk. 6. 编程类插件语法检查传统的vim语法检查使用的是syntastic，这个插件由于不是异步的，所以运行速度很慢，会很大程度上拖慢vim的运行速度。异步的语法检查插件推荐ale， 该插件只支持vim8，由于其运行是异步的，所以基本不会卡。 1234567Plug 'w0rp/ale'" Set Ale Cheackerlet g:ale_sign_error = 'o'let g:ale_sign_warning = '*'let g:ale_linters = &#123; \'python': ['flake8'] \&#125; ale不仅仅支持语法检查，还支持自动修正。而且支持大部分的语言，如C，C++，python，R，CUDA，Latex，Lua，Markdown，MATLAB，Java，Swift等，详情见github . 代码注释vim-commentary 1Plug 'tpope/vim-commentary' 一个快速对代码进行注释的插件，可以针对不同的文件进行不同的注释，而且在已经注释的情况下，运行相同的命令，可以uncomment. 常用的命令是 12gcc " normal模式下，注释当前行(或清除注释)&#123;Visual&#125; gc "Visual模式下，注释当前选中的范围（或清除当前范围的注释） 对齐代码对齐是一个常见的需求， 一个好用的代码对齐的插件是vim-easy-align . 这个插件的使用效果如下： 1234Plug 'junegunn/vim-easy-align'" EasyAlignnmap ga &lt;Plug&gt;(EasyAlign)xmap ga &lt;Plug&gt;(EasyAlign) 按照官方的推荐设置，我们使用ga作为对齐的键位映射。 这个插件有两种使用方式，一种为交互式，一种是live交互式。同时该插件也可以通过命令的方式调用。我个人喜欢在visual模式下，键入ga来启动交互式对齐，然后在交互式对齐中输入命令。常见的命令如下（其他使用方式参见该插件的help文档）： visual模式后进入交互式 功能 等效的命令行 = 在第一个等号周围对齐 :’&lt;,’&gt;EasyAlign= 2= 在第二个等号周围对齐 :’&lt;,’&gt;EasyAlign2= -= 在最后一个等号周围对齐 :’&lt;,’&gt;EasyAlign-= *= 在所有等号周围对齐 :’&lt;,’&gt;EasyAlign*= **= 等号周围的内容自动调节 :’&lt;,’&gt;EasyAlign**= \ + 其他命令 \可以切换对齐模式，包括做左对齐（默认），右对齐，中心对齐，可重复按下切换 右对齐:’&lt;,’&gt;EasyAlign!=中心对齐：:’&lt;,’&gt;EasyAlign!!= \+其他命令 对齐的分隔符右方的文字align到分隔符上，\则为左边的文字 :’&lt;,’&gt;EasyAlign=&lt;l1 \+其他命令 对齐的分隔符与对齐的文字之间无间隔 该插件的功能远不止于此，还可以自定义对齐的方式，详情可在vim中输入:h vim-easy-align查看 不同语言的插件1234Plug 'kh3phr3n/python-syntax'Plug 'elzr/vim-json'Plug 'plasticboy/vim-markdown'Plug 'vim-latex/vim-latex' 我的配置文件里面还包含了有关不同语法的插件，这些插件针对不同的文件类型提供了不同的功能，有些只是为了语法高亮，有些则是为了提供快捷编辑，用户可以根据需求来选择添加或删除。 7. 补全emmet-vimemmet-vim 是一个用来在html格式下补全的插件，一个官方的演示如下： 配置如下 12345678910111213141516171819202122Plug 'mattn/emmet-vim'..." Emmet Config" change &lt;Tab&gt; config , if use [YouCompleteMe]let g:user_emmet_expandabbr_key ='&lt;Tab&gt;'let g:user_emmet_settings = &#123; \ 'php' : &#123; \ 'extends' : 'html', \ 'filters' : 'c', \ &#125;, \ 'xml' : &#123; \ 'extends' : 'html', \ &#125;, \ 'haml' : &#123; \ 'extends' : 'html', \ &#125;, \ 'phtml' : &#123; \ 'extends' : 'html', \ &#125; \&#125;imap &lt;expr&gt; &lt;tab&gt; emmet#expandAbbrIntelligent("\&lt;tab&gt;") 这里将补全的快捷键设置成了tab键，当输入“aa/bb/cc”后键入tab可以自动拓展为如下： 123&lt;aa&gt;&lt;/aa&gt;&lt;bb&gt;&lt;/bb&gt;&lt;cc&gt;&lt;/cc&gt; 由于这里使用了tab进行补全拓展，所以在youcompleteme中禁用了tab YouCompleteMeYouCompleteMe 是一款基于语义的补全插件，其补全功能丝毫不逊色任何IDE，不仅仅可以补全，还可以进行定义，声明的跳转。 YouCompleteMe是一款需要编译的插件，如果使用vim8，并添加了python支持后，可以使用如下命令进行编译 12cd ~/.vim/bundle/YouCompleteMepython install.py --clang-completer YouCompleteMe的配置相对比较复杂，如果按照上述命令编译出现问题，可以参考官方build文档 12345678910111213141516171819202122232425262728293031323334353637383940Plug 'Valloric/YouCompleteMe'Plug 'rdnetto/YCM-Generator', &#123; 'branch': 'stable'&#125;..."------------------ YouCompleteMe -------------------" Linux vim &amp;&amp; NeoVim Using YouCompleteMeif(has("mac")) let g:ycm_python_binary_path='/usr/local/bin/python'else let g:ycm_server_python_interpreter='/usr/bin/python'endiflet g:ycm_auto_trigger = 1let g:ycm_global_ycm_extra_conf = "~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/.ycm_extra_conf.py"let g:ycm_collect_identifiers_from_tags_files = 0let g:ycm_add_preview_to_completeopt = 5let g:ycm_use_ultisnips_completer = 1let g:ycm_cache_omnifunc = 1let g:ycm_max_diagnostics_to_display = 0let g:ycm_key_list_select_completion = ['&lt;C-n&gt;', '&lt;C-j&gt;']let g:ycm_key_list_previous_completion = ['&lt;C-p&gt;', '&lt;C-k&gt;']let g:ycm_filetype_blacklist = &#123; \ 'tagbar' : 1, \ 'qf' : 1, \ 'notes' : 1, \ 'unite' : 1, \ 'text' : 1, \ 'vimwiki' : 1, \ 'pandoc' : 1, \ 'infolog' : 1, \ 'mail' : 1, \ 'mundo': 1, \ 'fzf': 1, \ 'ctrlp' : 1 \&#125;let g:ycm_error_symbol = '&gt;&gt;'let g:ycm_warning_symbol = '&gt;*'nnoremap &lt;leader&gt;gl :YcmCompleter GoToDeclaration&lt;CR&gt;nnoremap &lt;leader&gt;gf :YcmCompleter GoToDefinition&lt;CR&gt;nnoremap &lt;leader&gt;gg :YcmCompleter GoToDefinitionElseDeclaration&lt;CR&gt; 配置说明： 对于C-family的工程，使用YCM-Generator来生成补全的tag 针对不同的配置，可能需要自己手动指定python的位置 使用Ctrl+n(p)或Ctrl+j(k)来选择补全（由于emmet中使用了tab，所以这里禁用了tab） 对于黑名单中的文件，不进行补全 开启了来自ultisnips插件的补全，可以自动拓展成代码片段 使用快捷键可以跳转到定义，声明 代码拓展1234Plug 'sirver/ultisnips'Plug 'honza/vim-snippets'...let g:UltiSnipsExpandTrigger="&lt;S-tab&gt;" 使用ultisnips 和vim-snippets 来进行代码拓展，其中vim-snippets是代码拓展的引擎，ultisnips是一些可拓展的代码。两者结合，可以产生如下的效果 在这份配置里面使用Shift+tab可以拓展。 8. 通用插件smooth-scoll12345Plug 'terryma/vim-smooth-scroll'..." Smooth Scroll the terminalnnoremap &lt;silent&gt; &lt;C-u&gt; :call smooth_scroll#up(&amp;scroll, 0, 2)&lt;CR&gt;nnoremap &lt;silent&gt; &lt;C-d&gt; :call smooth_scroll#down(&amp;scroll, 0, 2)&lt;CR&gt; 使用这个插件，可以按下Ctrl-u, Ctrl-d来平滑的翻页，类似于滚动的效果。 Multiple-cursors12345678Plug 'terryma/vim-multiple-cursors'..." Multip Cursor" Default mappinglet g:multi_cursor_next_key='&lt;C-n&gt;'let g:multi_cursor_prev_key='&lt;C-p&gt;'let g:multi_cursor_skip_key='&lt;C-x&gt;'let g:multi_cursor_quit_key='&lt;Esc&gt;' 可以同时编辑多个地方，如下 expand-region1234Plug 'terryma/vim-expand-region'...map KK &lt;Plug&gt;(expand_region_expand)map JJ &lt;Plug&gt;(expand_region_shrink) 在visualize的模式下，按KK或JJ可以拓展或收缩选中的区域 复制的区域闪烁这里使用了两个轻量的插件，实现了使得复制时，所选择内容闪烁。 12345Plug 'kana/vim-operator-user'Plug 'haya14busa/vim-operator-flashy'...map y &lt;Plug&gt;(operator-flashy)nmap Y &lt;Plug&gt;(operator-flashy)$ incsearch这个属于增强vim的默认搜索的插件，使用起来跟默认的搜索一样，但是在显示上会比较友好。 123456789101112131415Plug 'haya14busa/incsearch.vim'..." Vim incsearchlet g:vim_search_pulse_disable_auto_mappings = 1let g:incsearch#auto_nohlsearch = 1map / &lt;Plug&gt;(incsearch-forward)map ? &lt;Plug&gt;(incsearch-backward)map g/ &lt;Plug&gt;(incsearch-stay)map n &lt;Plug&gt;(incsearch-nohl-n)zzzvmap N &lt;Plug&gt;(incsearch-nohl-N)zzzvmap * &lt;Plug&gt;(incsearch-nohl-*)zzzvmap # &lt;Plug&gt;(incsearch-nohl-#)zzzvmap g* &lt;Plug&gt;(incsearch-nohl-g*)zzzvmap g# &lt;Plug&gt;(incsearch-nohl-g#)zzzv 这里的配置是将incsearch的搜索替换默认的搜索。 surround一个快速编辑surround的插件 1Plug 'tpope/vim-surround' 使用方法非常简单，参见其github。常用的命令，比如针对hello，我们想给其加双引号，只需要按下ysiw&quot;，即可编程“hello”，然后在其上按下cs“)，即可其替换成（hello）。 repeat1Plug 'tpope/vim-repeat' 我们经常会使用.来重复上次的命令，然而，使用这个操作是不能重复Plugin map的操作，使用这个插件，即可完成这样的功能。]]></content>
      <categories>
        <category>software setting</category>
      </categories>
      <tags>
        <tag>work tips</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[option-framework]]></title>
    <url>%2F2017%2F08%2F31%2Foption-framework%2F</url>
    <content type="text"><![CDATA[Option framework in reinforcement learning参考自Learning Options in Reinforcement Learningoption 是在强化学习中的特殊的结构，它被设计在一些顶层任务中加入一些小的子目标的任务。同时一个option可以看成是一个time-extended的action. 强化学习这里只定义一些符号，具体有关强化学习的细节略去强化学习基于MDP，其中包括这样一些部分，状态集合S, 动作集合A, reward集合R. 给定一个状态$s\in S$和一个动作$a \in A$, 我们会得到一个immediate reward $r_s^a$, 以及一个从状态$s$到状态$s’$的转移概率$P_{ss’}^a$. policy被定义为在每一个状态下执行各个动作的概率$\pi:S\times A \rightarrow [0, 1]$ Option framework一个option (每一个agent可能包含多个option)包括三个部分：一，初始状态集$I \subset S$, 初始状态集包含了所有能够启动option的状态，也就是说当agent进入这样的状态时，agent的就可以被一个option控制。二，停止条件$\beta \in [0,1]$, 表示agent停止被option控制的概率。 三，内部policy，$\pi’: S\times A \rightarrow [0, 1]$, 这是某个option中特定的policy。一个option可以用这样一个三元组来表示$o=&lt;I, \pi’, \beta>$ option的工作流程是，当agent运行到初始状态集$I$所包含的的状态时，agent会被option所控制，开始调用option内部的policy,$\pi’$，执行相应的动作，跳转到下一个状态，此时判断停止条件$\beta$, 若停止，则退出option，否则继续执行$\pi’$并重复上面的步骤。 原始的强化学习也能被视为option的特例，我们把每一个动作都看成一个option，所有执行该动作的状态看成该option的初始状态，$I\in { s: a\in A_s}$, 然后内部的policy就是以1的概率执行动作$a$, 停止条件为$\beta=1$, 意味着这个option每次只执行一步。 然后我们可以把原始的强化学习中的action-value (Q) function拓展为option-value function，我们定义$Q^\mu (s, o)$, 表示在状态$s\in I$时采用策略$\mu$执行option o所获得的期望收益为$$ Q^\mu (s, o) = E \left[ r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots \mid E(o\mu, s, t)\right] $$这里$o\mu$表示执行option o, 直到它停止，然后使用策略$\mu$, $E(o \mu, s, t)$表示在时间$t$和状态$s$处执行$o\mu$。 所以我们可以把option当成一种特殊的动作，这种动作具有可以用来表示一连串的基本动作，而且有着其对应的子目标。它在形式上类似于semi-MDP中的动作，所以我们能够运用semi-MDP的方法来解决带option的agent的寻优问题。当在状态$s$中执行option，直到停止时，agent切换到了$s’$, 中间经历了$k$个time step， 同时中间的累计收益用$r$表示，那么更新算法为$$ Q(s, o) \leftarrow Q(s, o) + \alpha \left[r + \gamma^k \max_{a \in O} Q(s’,a) - Q(s,o)\right] $$ Find optimal option对于一个option来说, 如果其初始状态和终止条件均已知了，那么其内部策略可以用标准的强化学习算法去优化，因此option优化的关键是求解$I$和$\beta$. 在文章learning Options in Reinforcement Learning中，认为如果某一个状态频繁地在trajectories中出现，那么代表这个状态很重要，那么就将这些状态作为option的target。在这篇文章中的算法如下：12345678910111. 随机选择一定数量的开始状态S和目标状态T, 这些状态可以用来产生针对agent的随机任务2. 对于每一对&lt;S, T&gt;, 执行 (a). 使用Q学习学习一定数量的episodes，学出一个从S到T的policy (b). 执行学好的policy数个episodes， 统计每个状态s出现的次数n(s)3. 重复以下过程，直到期望的option的数量得以满足 (a). 选择出现最多的状态，作为option的target state，T' (b). 统计经过T'的路径上的所有状态的出现的数量 (c). 统计上一步中所有状态出现数量的平均值 (d). 选择大于平均值的状态作为初始状态I的一部分 (e). 使用一些插值的方法对初始状态进行插值，获得完整的初始状态4. 对于每一个option，学习其内部policy，直到到达对应的T' option结构的自学习参考文章The Option-Critic Architecture在这篇文章中，提出了这样的观点：过去的find optimal option的方法都是寻找子目标然后学习如何相应的inter-policy，这样的方法很难将内部option的策略和统一的策略统一起来，同时子目标的解决有时也很低效，甚至和原问题一样难，因此这篇文章提出了一个能够同时学习policy, inter-policy, terminations的方法。并做了理论推导。这样的方法有以下的好处： 速度快，不用单独学习大量的子任务 end-to-end，option的设计不需要人为的设计 通用性好，更加适合transfer learning 预定义的符号MDP的状态集合$S$，动作集$A$，转移概率$P: S\times A\rightarrow [0, 1]$，reward function $r: S \times A \rightarrow [0, 1]$. 策略$\pi: S\times A \rightarrow [0,1]$，以及expected return, $V_{\pi}(s) = E_{\pi} [\sum_{t=0}^{\infty} \gamma^t r_{t+1}|s_0 = s]$，和action-value function $Q_{\pi}(s,a)= E_{\pi}[\sum_{t=0}^{\infty}\gamma^t r_{t+1} | s_0 =s ,a_0=a]$ Policy gradient methods将策略进行参数化，用$\pi_\theta$来表示，那么从$s_0$处的期望收益表示为$\rho(\theta, s_0)=E_{\pi} [\sum_{t=0}^{\infty} \gamma^t r_{t+1}|s_0 = s]$，根据policy gradient therem求出梯度 $$\frac{\partial \rho(\theta, s_0)}{\partial\theta} = \sum_s \mu_{\pi_\theta}(s | s_0) \sum_a \frac{\partial \pi_\theta (a|s)}{\partial \theta}Q_{\pi_\theta}(s,a)$$在这里$\mu_{\pi_\theta} (s|s_0) = \sum_{t=0}^{\infty} \gamma^t P(s_t = s|s_0)$表示在以$s_0$为初始状态下的trajectories上的所有状态的discouted weighting option framework这里我们用$\omega \in \Omega$表示一个option，可以表达成三元组$(I_\omega, \pi_\omega, \beta_\omega)$ 学习option这篇文章学习option的思想如下 在任何阶段，网络都会充分利用experience，并同时更新value function, policy over options, inter-policy,和termination functions. 网络采用一种叫做call-and-return的option执行模型，agent通过policy over option $\pi_\Omega$来选择option $\omega$,然后执行intra-policy $\pi_\omega$直到termination,$\beta_\omega$. 我们用$\pi_{\omega, \theta}$, 表示intra-policy，并用$\theta$进行参数化，同时用$\beta_{\omega, \vartheta}$表示停止条件，并用$\vartheta$进行参数化。 &lt;此时网络的收益函数变成了$$ \rho(\Omega, \theta, \vartheta, s_0, \omega_0) = E_{\Omega, \theta, \omega} \left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} | s_0, \omega_0\right] $$ 这里我们需要同时求得$\rho$相对$\theta, \vartheta$的梯度。 我们首先定义函数$U:\Omega \times S \rightarrow R$, 把这个函数叫做option-value function opon arrival。 这个函数表示在状态$s’$下执行option$\omega$所获获得的收益$$ U(\omega, s’) =(1-\beta_{\omega, \vartheta} (s’)) Q_{\Omega} (s’, \omega) + \beta_{\omega, \vartheta} (s’) V_{\Omega}(s’)$$这个函数由两个部分组成，分为两种情况，第一项表示如果没有跳出这个$option$的情况下，第二项表示如果已经停止执行option $\omega$之后。 我们可以把$(s,\omega)$的视为一个拓展的状态，那么在$(s, \omega)$处执行动作$a$的收益为$$ Q_U (s, \omega, a) = r(s, a) + \gamma \sum_{s’} P(s’ |s,a) U(\omega, s’)$$这里$P$表示状态转移概率，那么考虑所有的动作，option-value function如下$$ Q_{\Omega} (s,\omega) = \sum_a \pi_{\omega, \theta} (a|s) Q_U (s, \omega, a) $$ 如果将$(s,\omega)$看成一个拓展的状态, 那么从$(s_t, \omega_t)$跳转到$(s_{t+1}, \omega_{t+1}$的概率为$$P(s_{t+1}, \omega_{t+1} | s_t, \omega_t) = \sum_a \pi_{\omega_t, \theta} (a | s_t) P(s_{t+1}| s_t, a) \left( (1-\beta_{\omega_t, \vartheta}) \mathbf{1}_{\omega_t = \omega_{t+1}} + \beta_{\omega_t, \vartheta} (s_{t+1})\pi_{\Omega} (\omega_{t+1} | s_{t+1})\right) $$ 这个公式运用了概率公式的链式法则，其中包括三项，第一项是在$s_t$下执行动作$a$的概率，第二项是在$s_t$下执行$a$后能跳转到$s_{t+1}$的概率，第三项比较复杂，表示在$s_{t+1}$后选择option $\omega_{t+1}$的概率。由于转移概率与动作无关，所以对动作求和。第三项包括两个部分，第一个部分表示在跳转的过程中，并没有停止option，但是这种情况必须满足$\omega_t = \omega_{t+1}$，另外一种情况是，中间发生了option的切换，所以这一项的概率等于停止的概率乘以重新选择到option $\omega_{t+1}$的概率。 根据求导的法则，我们可以得到$$ \frac{\partial Q_{\Omega} (s,\omega)}{\partial \theta} = \left( \sum_{a} \frac{\partial \pi_{\omega, \theta} (a|s)}{\partial \theta} Q_U(s,\omega, a)\right)+\sum_a \pi_{\omega,\theta} (a|s) \sum_{s’} \gamma P(s’|s,a)\frac{\partial U(\omega, s’)}{\partial \theta}$$ 根据论文中的两个定理，具体参考论文，可以求解出相应的$\rho$相对$\theta, \vartheta$的梯度。根据定理一，求出梯度如下$$ \frac{\partial Q_{\Omega} (s,\omega)}{\partial \theta} = \sum_{s,\omega} \mu_{\Omega} (s,\omega|s_0, \omega_0) \sum_a \frac{\partial \pi_{\omega,\theta} (a|s)}{\partial \theta} Q_U (s,\omega, a)$$这里$\mu_{\Omega} (s,\omega|s_0, \omega_0)$表示从$(s_0, \omega_0)$开始的trajectories上的state-option pairs上的加权权重。$\mu_{\Omega}(s,\omega|s_0, \omega_0) = \sum_{t=0}^{\infty} \gamma^t P(s_t=s,\omega_t =\omega|s_0, \omega_0)$ 根据定理二，求出梯度如下$$ \frac{\partial Q_{\Omega} (s,\omega)}{\partial \vartheta} = -\sum_{s’,\omega} \mu_{\Omega} (s’,\omega|s_1, \omega_0) \frac{\partial \beta_{\omega,\vartheta} (s’)}{\partial \vartheta} A_\Omega (s’,\omega)$$这里$\mu_{\Omega} (s’,\omega|s_1, \omega_0)$表示从$(s_1, \omega_0)$开始的trajectories上的state-option的加权权重。$\mu_{\Omega}(s,\omega|s_1, \omega_0) = \sum_{t=0}^{\infty} \gamma^t P(s_{t+1}=s,\omega_t =\omega|s_1, \omega_0)$ 改论文使用的网络结构的如下图使用的更新算法如下]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>option</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DFP_code]]></title>
    <url>%2F2017%2F08%2F07%2FDFP-code%2F</url>
    <content type="text"><![CDATA[Code Structure common util.py tf_ops.py defaults.py Simulator doom_simulator.py multi_doom_simulator.py Agent agent.py future_predictor_agent_basic.py future_predictor_agent_advantage.py future_predictor_agent_advantage_nonorm.py target future_target_maker.py experience multi_experience_memory.py multi_experience.py Commonutil.pymake_objective_indices_and_coeffs(temporal_coeffs, meas_coeffs):网络使用两个系数作为目标的系数作为最终决断的系数，一个是temporal_coeffs, 即时间尺度上的系数，分别对应不同时间段对目标值的贡献，meas_coeffs表示不同的measure对目标值的贡献。 make_array(shape=(1,), dtype=np.float32, shared=False, fill_val=None):生成array，如果有array共享的话，从共享的区域中生成array merge_two_dicts(x, y)合并两个dict，用于defaults.py 中的参数和不同模型的参数合并 StackedBarPlot用于画不同的预测值的直方图，并可以动态显示 tf_opsmsra_stddev(x, k_h, k_w)参数初始化方差 mse_ignore_nans(preds, targs. **kwargs)忽略Nan 求MSE conv2d(input_, output_dim, k_h, k_w, d_h, d_w, msra_coeff, name)构建卷积层的api lrelu(x, leak, name)构建lrelu层的api linear(input_, output_size, name, msra_coeff)线性函数api conv_encoder(data, params, name, msra_coeff)根据conv_params的参数构建连续的卷积层 fc_net(data, param, name, last_linear, return_layers, msra_coeff)根据fc_params的参数构建连续的全连接层；根据return_layers的性质，确定最后一层的fc是否要接relu flatten(data)构建flatten层的api defaults.pydefault.py主要提供不同的默认参数dict，其中主要分为4类， target_maker, simulator, experiment(experiment, train_experiment, test_policy), agent。 给不同的文件提供不同的默认参数配置， 在具体使用的时候，该参数会被不同文件中的参数给覆盖。 target_maker_args这里的min_num_targs, 表示目标target的最小数量， 对于那些target数量不足的样本，会用nan替换 simulator_argsframe_skip表示不使用连续的帧作为状态，使用间隔的帧作为状态。 experience_args这里分为三个文件experience_args, train_experience_args, test_policy_experience_args agent_args存储所有关于网络的参数 SimulatorDoomSimulatorDoomSimulator类 init(self, args)初始化程序配置，设置doom的各项参数，具体参考doom_doc使用analyze_controls(self, config_file), 解析获得available_controls, continuous_controls, discrete_control，注意到这些都是index. 比如continuous_controls = [1,2,3], discrete_control = [4,5,6] analyze_controls(self, config_file)利用正则表达式解析available_controls, continuous_controls, discrete_control. init_game, close_game, get_random_action, is_new_episode, next_map, new_episode函数作用如名所示，注意doom的具体动作可以使用多个动作的组合，比如可控的动作是[1, 2]， 那么实际可能的动作为[True, True] or [True, False] or [False, True] or [False, False] step(self, action)执行一步动作，返回img，measure, reward, terminal. 这里action是一个bool vector, 图像均为黑白(彩色会报错), img指doomSimulator返回的图像，meas指config文件中的指定的variable，这里特指{AMMO2, HEALTH, USER2}(这里的user2指kill_count, 在wad文件里面使用). rwrd特指doomSimulator返回的reward，这个是doom内部自带的reward. MultiDoomSimulator多个doomSimulator的类，可以用来同时执行一个action的列表，同时返回imgs, meass, rwrds, terms（均为list） Agentagent.pyinit(self, sess, args):初始化各项参数同时调用prepare_controls_and_actions(), 初始化所有的control和action prepare_controls_and_actions(self)注意到这里controls表示action的index，action用bool vector表示。其中controls分为两类，一类是由网络生成的discrete_controls_to_net, 一类是外部赋予的discrete_controls_manual. 同时net_discrete_actions是一个numpy array，表示list of vector(除掉了冲突的action), 这些vector可以用来直接执行动作。另外，初始化了action_to_index的dict，用来查询不同的action的index, 初始化了onehot_dizhscrete_actions用一个one-hot向量表达不同的action，这里可以用作网络的输入。 preprocess_actiosn(self, acts)这里的acts表示batch_size个动作，每个动作用一个bool vector(包含了net_action和manunal_action)表示，该函数的作用是把这些acts转化成对应的one-hot-vector输入到网络中去. postprocess_actions(self, acts_net, act_manual)将acts_net和act_manual组合成一个正式的输出的动作，该函数的输入的acts_net是一个[batch_size]的输入，每个值均为action的index，act_manual是一个bool_vector。将act_nets, act_manual组合成一个正式的action，可以用做simulator的输入. random_actions(self, num_samples)获得随机的action make_net, make_losses，act_nets, act_manual这几个函数都没有执行，在装饰器文件如future_predictor_agent_basic等中执行 act(self, state_imgs, state_meas, objective_coeffs)调用act_net得到net_action，调用act_manual得到munual_action，使用postprocess_actions函数将其组合成一个具体的动作 build_model(self)该函数主要构建tf的模型，具体的模型参考论文 需要输入的参数包括input_images, input_measurement, input_targets, input_actions, input_objective_coeffs 如果提供了预处理的函数，则会根据预处理函数对输入进行预处理，预处理的方式参考论文 将img的值放缩在[-1, 1] 将mesurements的值放缩在[-1, 1] 将三个target分别除以7.5, 30, 1 调用make_net和make_loss构建网络 构建训练节点，学习率衰减节点，summary节点 Actor一个子类，用作实际操作action的一个接口init(self, agent, objective_coeffs, random_prob, random_objective_coeffs)初始化objective_coeffs, 如果使用随机的初始化策略，则调用reset_objective_coeffs(self, indices), 使用均匀分布来初始化，否则直接对objective_coeffs赋值reset_objective_coeffs(self, indices)使用均匀分布初始化objective_coeffsact(self, state_imgs, state_meas)$\epsilon$-greedy算法，以$\epsilon$的概率执行随机的动作，以1-$\epsilon$的概率调用agent.act()函数执行动作act_with_multi_memory(slef, multi_memory)执行和act一样的功能，但是针对mutil_memory，调用这个函数执行多个动作要比使用上面的函数高校，因为这样只会在需要的时候才会读取state。 get_actor(self, objective_coeffs, random_prob, random_objective_coeffs)返回actor的子类 train_one_batch(self, experience) 调用experience.get_random_batch函数得到state_imgs, state_meas, rwrds, terms, acts, targs, objs forward网络一次 在特定的步数打印错误，存储summary，或存储histogram 步数+1 train(self, simulator, experience, num_steps, test_police_experience) 如果可以的话，载入checkpoint 初始化writer, 和actor接口 调用experience.add_n_steps_with_actor填充训练的memory 调用train_one_batch共num_steps次，一共训练num_steps个batch 每隔固定的步数保存checkpoint或者测试test_policy，同时每隔一定的步数重新填充memory，注意到这里会衰减$\epsilon$ test_policy(self, simulator, experience, objective_coeffs, num_steps, random_prob, write_summary, write_prdiction)测试一次policy, 注意到这里会调用experience.compute_avg_meas_and_rwrd去计算得到这一次的policy的平均reward和平均measure。另外，为了保证每次测试不改变head_offset, 会暂时保存该值，并在测试完之后恢复 save, load, set_init_step功能如其，set_init_step可以接着之前的step继续训练 future_predictoragent*.py这里一共包含3个文件(future_predictor_agent_basic, future_predictor_agent_advantage, future_predictor_agent_advantage_nonorm), 都是作为agent类的子类，重写了agent类中的make_net, make_losses, act_nets的成员函数，用于对比实验 make_net(self, input_images, input_measurement, input_actions, input_objectives, reuse) 根据卷积和全联接的参数调用tf_ops中的接口来搭建网络 future_predictor_agent_basic中没有使用分支结构 future_predictor_agent_advantage中是论文中的标准结构 future_predictor_agent_advantage_nonorm是论文中的标准结构但是去除了normalization的环节 这里input_images, input_measurement, input_actions均为网络的输入，而input_actions(bool of vector)用来指出哪个所有的预测中与当前动作相关的预测。 make_losses(self, pred_relevant, targets_preprocessed, objective_indices, objective_coeffs)构建loss的计算节点，并同时去除nan，构建summry节点 act_net(self, state_imgs, state_meas, objective_coeffs) 更新prediction 按照objective_coeffs求出使收益最大的动作。 targetfuture_target_maker.pytarget_maker类，负责生成网络的学习目标 init(self, args)构造函数，注意几点 min_num_targs: target的最小数量，对于某些靠近end of episode的样本，其可能的target数量少于min_num_targs，所以这些样本无效，会用nan替代 根据不同的gamma值，对不同的future step对reward进行指数衰减。在具体的实验中，由于并没有用到simulator中的reward值，因此gamma值为空 每个时刻需要预测的target的维度包括两个部分: 1, 不同gamma衰减下的reward；2，不同的measure。 所以num_targets = len(self.meas_to_predict) + self.num_reward_targets。注意num_targets的长度必须与agent中的objective_coeffs_meas的长度相同，因为objective_coeffs_meas就是用来衡量不同的target对最终结果的加权系数 总的target的维度为self.num_targets * len(self.future_steps). 注意self.future_steps的长度必须与agent中的objective_coeffs_temporal的长度相同 根据min_num_targs，确定min_future_frames，将来用来确定该帧与之后的min_future_frames处的帧是否处于同一个episode来判断当前帧是否有效. make_targets(self, indices, meas, rwrds, n_episode, meas_mean, meas_std)生成对应indices处的targets该函数主要是在multi_experience_memory处调用，所以提供的meas，rwards都是一个大的memory, indices负责指示想要生成batch的样本所在memory处的位置. capacity指memory的大小 target分为两个部分，一个是measurement, 一个是reward 对于measurement，如果有meas_mean或者meas_std的话，会进行normalization的处理，不在一个epsisode的样本的measurement会用最近的measurement进行替换。 对于reward，target是对一个时间窗口(self.future_steps)内的rwrd进行指数衰减下的加权求和。 experiencemulti_experience_memory.pyMultiExperienceMemory类 init(self, args, multi_simulator, target_maker)初始化空的memory，这里包含两个假设：1，观测都是连续的，在每一个episode内都有一个停止的状态；2，每一个episode都比memory的长度要短。调用reset函数，初始化各项私有变量 reset(self) self._curr_indices设置成不同memory的indices self._episode_counts用来设置不同memory的eposide值，用来区分不同frame在memory中的位置 add(self, imgs, meass, rwrds, terms, acts, objs, preds) 更新images, measurements, rewards, terminals, actions，n_episode, objectives, predictions 在term处的measurements使用上一个状态measurements，同时对于term处，对measurement进行一定的衰减 为了防止出现两个term状态，在出现第二个term时，将第二个term状态的measurements置零 更新episode_counts, curr_indices和terminals add_step(self, multi_simulator, acts, objs, preds)调用multi_simulator执行一次acts, 并将其结果加入memory add_n_steps_with_actor(self, multi_simulator, num_steps, actor, verbose, write_prediction, write_logs, global_step) log中会写入具体的episode, step, time, accu_reward, prev_meas, avg_meas 如果有prediction的话，会写入prediction 调用add_steps共num_steps次，使用actor类的接口加入memory get_states(self, indices)根据indices从memory中选择state_imgs, state_meas。注意到这里的的state_imgs包含history_step个历史，且每个历史之间相差不同的history_len而state_meas仅仅包含当前indices处的measurements get_current_state(self)获得当前最近的观测值 get_last_indices(self)返回最近的indices get_target(self, indices)使用target_maker.make_targets类返回indices处的targets.这里使用了一个hack，对不同的memory处使用了一12345678 * self._n_head的数值，用来区分不同的head处的episode值 has_valid_history(self, index)判断index处是否有足够的history进行训练 has_valid_target(self, index)判断index处是否有足够的target进行训练 is_valid_target(self, index)判断index处的状态是否有效，即是否有足够的history和target get_observations(self, indices)得到indices处的所有信息包括state_imgs, state_meas, rwrds, terms, acts, targs, objs get_random_batch(self, batch_size)从memory中随机采用batch_size个有效的样本 compute_avg_meas_and_rwrd(self, start_idx, end_idx)统计从start_idx，end_idx中所有的episode中的平均measurements和rewards show(self, start_idx, end_idx, display, write_imgs, write_video, preprocess_targets, show_predictions, net_discrete_actions)show的接口，提供display, write_imgs, write_vedio, show_prediction的选项 multi_experiement.pyinit(self, target_maker_args, simulator_args, train_experience_args, test_policy_experience_args, agent_args, experiment_args)将默认的参数和给定的参数进行合并，得到具体的参数.部分参数在此处求解，比如各种shape的参数 run(self, mode) show模式，将训练的head_offset和测试的head_offset错开。测试policy，并show_memory中的结果 train模式，训练网络, 更新参数.]]></content>
      <categories>
        <category>code analyze</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OCR]]></title>
    <url>%2F2017%2F05%2F05%2FOCR%2F</url>
    <content type="text"><![CDATA[OCR 文字光学识别问题描述 文字检测 文字识别 文字检测(localization)特点 IOU不是一个好的criterion, 检测到一部分文字也行 various fonts, colors, languages and bks etc. perspective transformation layouts word/line level methods tranditional method deep learning RPN based, detection FCN based, segmentation Note: Sence text detection via holistic, multi-channel prediction 文字识别method CNN/MDLSTM + RNN + CTC Sequence to Sequence with Attention Combine CTC and Attention note: CTC用来将文字进行对齐 note: LSTM -&gt; GRU -&gt;EURNN RNN Bidirectional RNN (文字识别中经常使用) Stack RNN(百度 7个堆叠， 谷歌5个堆叠) MDLSTM/Grid LSTM challenge Chinese include too many characters Uncoutable labels Insufficient data (Synthesize) Much more computation Incaptable Too much perspective transform (STN) Vertical layout]]></content>
      <categories>
        <category>OCR</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>OCR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pickle]]></title>
    <url>%2F2017%2F05%2F01%2Fpickle%2F</url>
    <content type="text"><![CDATA[pickle 模块的简易使用序列化存储1234import pickledata = ...f = open('file.pkl','wb')pickle.dump(data, f) 序列化读取123import picklepkl_file = open('file.pkl','rb')data = pickle.load(pkl_file)]]></content>
      <categories>
        <category>work tips</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmux]]></title>
    <url>%2F2017%2F04%2F23%2Ftmux%2F</url>
    <content type="text"><![CDATA[TmuxDownload and InstallreferTmux ShortcutsSessions :new new session s list sessions $ name session Windows c create window w list window n next window p previous window f find window , name window &amp; kill window Panes % vertical split “ horizontal split o swap panes q show pane number x kill pane]]></content>
      <categories>
        <category>work tips</category>
      </categories>
      <tags>
        <tag>work tips</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL7 Policy Gradient]]></title>
    <url>%2F2017%2F04%2F18%2FRL7%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL lecture 7 Policy GradientIntroductionPolicy-Based Reinforcement Learning In the last lecture we approximated the value or action-value function using parameter $\theta$ A policy was generated directly from the value function In this lecture, we will directly parametrise the policy$$ \pi_{\theta} (s,a) = P(a|s,\theta) $$ We will focus again on model-free reinforcement learning Value-Based and Policy-Based RL Value Based Learnt Value Function Implicit policy (e.g. $\epsilon$-greedy) Policy Based No Value Function Learnt Policy Actor-Critic Learnt Value Function Learnt Policy Advantages of Policy-Based RLAdvantages: Better convergence properties Effective in high-dimensional or continuous action spaces Can learn stochastic policiesDisadvantages: Typically converge to a local rather than global optimum Evaluating a policy is typically inefficient and high variance Policy SearchPolicy Objective Functions Goal: given policy $\pi_{\theta} (s,a) $ with parameters $\theta$, find best $\theta$. But how do we measure the quality of a policy $\pi_theta$? In episodic environments we can use the start value.$$ J_1 (\theta) = V^{\pi_\theta} (s_1) = E_{\pi_\theta} (v_1) $$ In continuing environments we can use the average value$$ J_{avV} (\theta) = \sum_{s} d^{\pi_\theta} (s) V^{\pi_\theta} (s) $$ Or the average reward per time-step$$ J_{avR} (\theta) = \sum_{s} d^{\pi_\theta} (s) \sum_{a} \pi_\theta (s,a) R_s^a $$ where $d^{\pi_\theta} (s)$ is stationary distribution of Markov chain for $\pi_\theta$ Policy Optimisation Policy based reinforcement learning is an optimisation problem Find $\theta$ that maximises $J(\theta)$ Some approaches do not use gradient Hill climbing Simplex/amoeba/Nelder Mead Genetic algorithms Greater efficiency often possible using gradient Gradient descent Conjugate gradient Quasi-Newton We focus on gradient descent, many extensions possible And on methods that exploit sequential structure Finite Difference Policy GradientPolicy Gradient Let $J(\theta)$ be any policy objective function Policy gradient algorithms search for a local maximum in $J(\theta)$ by ascending the gradient of the policy, w.r.t parameters $\theta$$$ \Delta \theta = \alpha \nabla_\theta J(\theta) $$ Where $\Delta_\theta J(\theta)$ is the policy gradient, and $\alpha$ is a step-size parameter Computing Gradients By Finite Differences To evaluate policy gradient of $\pi_\theta (s,a)$ For each dimension $k \in [1,n]$ Estimate $k$th partial derivative of objective function w.r.t $\theta$ By perturbing $\theta$ by small amount $\epsilon$ in $k$th dimension Uses $n$ evaluations to compute policy gradient in $n$ dimensions Simple, noisy, inefficient - but sometimes effective Works for arbitrary policies, even if policy is not differentiable Monte-Carlo Policy GradientLikelihood RatiosScore Function We now compute the policy gradient analytically Assume policy $\pi_\theta$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta (s,a) $ Likelihood ratios exploit the following identity$$ \nabla_\theta \pi_\theta (s,a) = \pi_\theta (s,a) \frac{\nabla_\theta \pi_\theta (s,a)}{\pi_\theta (s,a)} = \pi_{\theta} (s,a) \nabla_\theta \log \pi_\theta (s,a) $$ The score function is $\nabla_\theta \log \pi_{\theta} (s,a)$ Softmax Policy We will use a softmax policy as a running example Weight actions using linear combination of features $\phi(s,a)^T \theta$ Probability of action is proportional to exponential weight$$ \pi_\theta (s,a) \propto e^{\phi(s,a)^T \theta} $$ The score function is$$ \nabla_\theta \log \pi_\theta (s,a) = \phi(s,a) - E_{\pi_\theta} (\phi(s,\cdot)) $$ Gaussian Policy In continuous action spaces, a Gaussian policy is natural Mean is a linear combination of state feature $\mu(s) = \phi(s)^T \theta$ Variance may be fixed $\sigma^2$, or can also parametrised Policy is Gaussian, $a \sim N(\mu(s),\sigma^2)$ The score function is$$ \nabla_\theta \log \pi_{\theta} (s,a) = \frac{(a-\mu(s))\phi(s)}{\sigma^2} $$ Policy Gradient TheoremOne-Step MDPs Consider a simple class of one-step MDPs Starting in state $s\sim d(s)$ Terminating after one time-step with reward $r=R_{s,a}$ Use likelihood ratios to compute the policy gradient$$ J(\theta) = E_{\pi_\theta} (r) = \sum_{s \in S} d(s) \sum_{a \in A} \pi_\theta (s,a) R_{s,a} \\ \nabla_\theta J(\theta) = \sum_{s \in S} d(s) \sum_{a \in A} \pi_\theta (s,a) \nabla_\theta \log \pi_\theta (s,a) R_{s,a} = E_{\pi_\theta} (\nabla_\theta \log \pi_\theta (s,a) r) $$ Policy Gradient Theorem The policy gradient theorem generalised the likelihood ratio approach to multi-step MDPs Replaces instantaneous reward $r$ with long-term value $Q^\pi (s,a)$ Policy gradient theorem applies to start state objective, average reward and average value objective, average reward and average value objective TheoremFor any differentiable policy $\pi_\theta(s,a)$, for any of the policy objective functions $J=J_1,J_{avR}$ or $\frac{1}{1-\gamma} J_{avV}$, the policy gradient is$$ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) Q^{\pi_\theta} (s,a)\right] $$ Monte-Carlo Policy Gradient(REINFORCE) Update parameters by stochastic gradient ascent Using policy gradient theorem Using return $v_t$ as an unbiased sample of $Q^{\pi_\theta} (s_t,a_t)$$$ \Delta \theta_t = \alpha \nabla_\theta \log \pi_\theta (s_t,a_t)v_t $$ Actor-Critic Policy GradientIntroduction to ACReducing Variance Using a Critic Monte-Carlo policy gradient still has high variance We use a critic to estimate the action-value function, $ Q_w (s,a) \approx Q^{\pi_\theta} (s,a)$ Actor-critic algorithms maintain two sets of parameters Critic Updates action-value function parameters $w$ Actor Updates policy parameters $\theta$, in direction suggested by critic Actor-critic algorithms follow an approximate policy gradient$$ \nabla_\theta J(\theta) \approx E_{\pi_\theta} (\nabla_\theta \log \pi_\theta (s,a) Q_w (s,a) ) \\ \Delta \theta = \alpha \nabla_\theta \log \pi_\theta (s,a) Q_w (s,a) $$ Estimating the Action-Value Function The critic is solving a familiar problem: policy evaluation How good is policy $\pi_theta$ for current parameters $\theta$? This problem was explored in previous two lectures, e.g. Monte-Carlo policy evaluation Temporal-Difference learning TD($\lambda$) Could also use e.g least-squares policy evaluation Action-value Actor-Critic Simple actor-critic algorithm based on action-value critic Using linear value fn approx $Q_w (s,a) = \phi(s,a)^T w $ Critic Updates $w$ by linear TD(0) Actor Updates $\theta$ by policy gradient Compatible Function ApproximationBias in Actor-Critic Algorithms Approximating the policy gradient introduces bias A biased policy gradient may not find the right solution e.g if $Q_w(s,a)$ uses aliased features, can we solve gridworld example? Luckily, if we choose value function approximation carefully Then we can avoid introducing any bias i.e We can still follow the exact policy gradient Compatible Function Approximation Theorem(Compatible Function Approximation Theorem)If the following two conditions are satisfied Value function approximator is compatible to the policy$$ \nabla_w Q_w(s,a) = \nabla_\theta \log \pi_\theta (s,a) $$ Value function parameters $w$ minimise the mean-squared error$$ \epsilon = E_{\pi_\theta} \left[ (Q^{\pi_\theta}(s,a) -Q_w(s,a))^2\right] $$Then the policy gradient is exact$$ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) Q_w (s,a) \right] $$ Proof of Compatible Function Approximation TheoremIf $w$ is chosen to minimise mean-squared error, gradient of $\epsilon$ w.r.t $w$ must be zeroSo $Q_w (s,a)$ can be substituted directly into the policy gradient$$ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) Q_w (s,a) \right] $$ Advantage Function CriticReducing Variance Using a Baseline We subtract a baseline function $B(s)$ from the policy gradient This can reduce variance, without changing expectation$$ E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) B(s) \right] = \sum_{s \in S} d^{\pi_\theta} (s) \sum_s \nabla_\theta \pi_\theta (s,a) B(s) \\ = \sum_{s \in S} d^{\pi_\theta} B(s) \nabla_\theta \sum_{a \in A} \pi_\theta (s,a) = 0 $$ A good baseline is the state value function $B(s) = V^{\pi_\theta} (s)$ So we can rewrite the policy gradient using the advantage function $A^{\pi_\theta} (s,a)$$$ A^{\pi_\theta} (s,a) = Q^{\pi_\theta} (s,a) - V^{\pi_\theta} (s) \\ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) A^{\pi_\theta} (s,a) \right] $$ Estimating the Advantage Function The advantage function can significantly reduce variance of policy gradient So the critic should really estimate the advantage function For example, by estimating both $V^{\pi_\theta} (s)$ and $Q^{\pi_\theta} (s,a)$ Using two function approximating and two parameter vectors$$ V_v (s) \approx V^{\pi_\theta} (s) \\ Q_w (s,a) \approx Q^{\pi_\theta} (s,a) \\ A(s,a) = Q_w(s,a) - V_v (s) $$ And updating both value functions by e.g TD learning For the true value function $V^{\pi_\theta} (s)$, the TD error $\delta^{\pi_\theta}$$$ \delta^{\pi_\theta} = r + \gamma V^{\pi_\theta} (s’) - V^{\pi_\theta} (s)$$ is an unbiased estimate of the advantage function$$ E_{\pi_\theta} \left[ \delta^{\pi_\theta} | s,a\right]= E_{\pi_\theta} \left[r+\gamma V^{\pi_\theta} (s’) | s,a \right] - V^{\pi_\theta} (s) \\ = Q^{\pi_\theta} (s,a) -V^{\pi_\theta} (s) = A^{\pi_\theta} (s,a) $$ So we can use the TD error to compute the policy gradient$$ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) \delta^{\pi_\theta} \right] $$ In practice we can use an approximate TD error$$ \delta_v = r + \gamma V_v (s’) - V_v (s) $$ This approach only requires one set of critic parameters $v$ Eligibility TracesCritics at Different Time-Scales Critic can estimate value function $V_\theta(s)$ from many targets at different time-scales For MC, the target is the return $v_t$$$ \Delta \theta = \alpha (v_t - V_\theta(s)) \phi(s) $$ For TD(0), the target is the TD target $r+ \gamma V(s’)$$$ \Delta \theta = \alpha (r + \gamma V(s’) - V_\theta(s)) \phi(s) $$ For forward-view TD($\lambda$), the target is the $\lambda$-return $v_t^\lambda$$$ \Delta \theta =\alpha (v_t^\lambda - V_\theta (s)) \phi(s) $$ For backward-view TD($\lambda$), we use eligibility traces$$ \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \\ e_t = \gamma \lambda e_{t-1} + \phi(s_t) \\ \Delta \theta = \alpha \delta_t e_t $$ Policy Gradient with Eligibility Traces Just like forward-view TD($\lambda$), we can mix over time-scales$$ \Delta \theta = \alpha (v_t^\lambda - V_v (s_t) ) \nabla_\theta \log \pi_\theta (s_t,a_t) $$ where $v_t^\lambda -V_v(s_t)$ is a biased estimate of advantage fn Like backward-view TD($\lambda$), we can also use eligibility traces By equivalence with TD($\lambda$), substituting $\phi(s) = \nabla_\theta \log \pi_\theta (s,a)$$$ \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \\ e_{t+1} = \lambda e_{t} + \nabla_\theta \log \pi_\theta (s,a) \\ \Delta \theta = \alpha \delta_t e_t $$ This update can be applied online, to incomplete sequences Natural Policy GradientAlternative Policy Gradient Direction Gradient ascent algorithm can follow any ascent direction A good ascent direction can significantly speed convergence Also, a policy can often be reparametrised without changing action probabilities For example, increasing score of all actions in a softmax policy The vanilla gradient is sensitive to these reparametrisations Natural Policy Gradient The natural policy gradient is parametrisation independent It finds ascent direction that is closet to vanilla gradient, when changing policy by a small, fixed amount$$ \nabla_\theta^{nat} \pi_\theta (s,a) = G_\theta^{-1} \nabla_\theta \pi_\theta (s,a) $$ where $G_\theta$ is the Fisher information matrix$$ G_\theta = E_\theta \left[ \nabla_\theta \log \pi_\theta (s,a) \nabla_\theta \log \pi_\theta (s,a)^T \right] $$ Natural Actor-Critic Using compatible function approximation$$ \nabla_w A_w (s,a) = \nabla_\theta \log \pi_\theta (s,a) $$ So the natural policy gradient simplifies,$$ \nabla_\theta J(\theta) = E_{\theta_\pi} \left[ \nabla_\theta \log \pi_\theta (s,a) A^{\pi_\theta} (s,a) \right] \\ = E_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) \nabla_\theta \log \pi_\theta (s,a)^T w \right] = G_\theta w $$so we can get $ \nabla_\theta^{nat} J(\theta) = w $ update actor parameters in direction of critic parameters Summary of Policy Gradient Algorithms The policy gradient has many equivalent forms Each leads a stochastic gradient ascent algorithm Critic uses policy evaluation (e.g MC or TD learning) to estimate $Q^\pi (s,a), A^\pi (s,a)$ or $V^\pi (s)$]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP3 More Word Vectors]]></title>
    <url>%2F2017%2F04%2F16%2FNLP3%2F</url>
    <content type="text"><![CDATA[referencecs224n lecture 3 More Word VectorsStochastic gradients with word vectors But in each window, we only have at most $2m+1$ words, so $\nabla_\theta J_t(\theta)$ is very sparse! We may as well only update the word vectors that actually appear! Solution: either you need sparse matrix update operations to only update columns of full embedding matrices $U$ and $V$, or you need to keep around a hash for word vectors If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around! Approximations The normalization factor is too computationally expensive$$ p(o|c) = \frac{\exp (u_o^T v_c)}{\sum_{w=1}^V \exp (u_w^T v_c)} $$ Implement the skip-gram model with negative sampling Main idea: train binary logistic regressions for a true pair (center word and word in its context window) versus a couple of noise pairs (the center word paired with a random word) The skip-gram model and negative sampling From paper Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al. 2013) Overall objective function: $J(\theta) = \frac{1}{T} \sum_{t=1}^{T} J_t(\theta)$$$J_t(\theta) = \log \sigma(u_o^T v_c) + \sum_{i=1}^{k} E_{j\sim P(\omega)} \left[ \log \sigma (-u_j^T v_c)\right] $$ Where $k$ is the number of negative samples and we use $\sigma$ is sigmoid function So we maximize the probability of two words co-occurring in first log Negative samplingword2vec is a huge neural network!The author of Word2Vec addressed the issue in their second paperThere are three innovations in this second paper: Treating common word pairs or phrases as a single “words” in their model Sub-sampling frequent words to decrease the number of training examples Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights Note: Sub-sampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well. Word Pair and “Phrases”Example: a word pair like “Boston Globe” has a much different meaning than the individual words “Boston” and “Globe”. So it makes sense to treat “Boston Globe” as a single word Method of phrase detection: it is covered in the “Learning Phrases” section of paper. And the code is available in word2phrase.c of their published code Sub-sampling Frequent WordsAs this example There are two problems with common words like the: When looking at word pairs, ( fox, the ) doesn’t tell use much about the meaning of fox. the appears in the context of pretty much every word. We will have many more samples of ( the, $\dots$) than we need to learn a good vector for the. Word2Vec implements a “sub-sampling” scheme to address this. For each word we encounter in training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency If we have a window size of 10, and we remove a specific instance of the from our text: As we train on the remaining words, the will not appear in any of their context windows. We’ll have 10 fewer training samples where the is the input word. Sampling rateFor any word $w_i$, $z(w_i)$ is the fraction of the total words in the corpus that are that word.$P(w_i)$ is the probability of keeping the word:$$ P(w_i) = \left( \sqrt{\frac{z(w_i)}{0.001}}+1\right) \frac{0.001}{z(w_i)}$$ Negative samplingNegative sampling addresses the problem (tremendous number of weight) by having each training sample only modify a small percentage of the weights, rather than all of them. When training the network on the word pair (fox,quick), output neuron corresponding to quick should output 1 (positive), and for all of the other output neurons should output 0 (negative). With negative sampling, we are instead going to randomly select just a small number of “negative” words to update the weights for. We will also still update the weights for our “positive” word. Recall that the output layer of our model has a weight matrix that’s $300 \times 10000$. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1800 weight value total. That’s only $0.06\%$ of the 3M weights in the output layer. In the hidden layer, only the weights for the input word are updated. Selecting Negative SamplesThe probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.$$ P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=0}^n \left( f(w_j)^{3/4}\right)}$$ Summary of word2vec Go through each word of the whole corpus Predict surrounding words of each word This captures co-occurrence of words one at a time Evaluation word vectors Related to general evaluation in NLP: Intrinsic vs extrinsic Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP2 Word Vectors]]></title>
    <url>%2F2017%2F04%2F16%2FNLP2%2F</url>
    <content type="text"><![CDATA[referencecs224n lecture 2 Word VectorsWord meaningDefinition: meaning the idea that is represented by a word, phrase, etc. the idea that a person wants to express by using words, signs, etc. the idea that is expressed in a word of writingCommonest linguistic way of thinking of meaning signifier $\iff$ signified (idea or thing) = denotation One-hot vector(meaning in computer)Common answer: Use a taxonomy like WordNet that has hypernyms relationships and synonym setsProblems with this discrete representation Great as a resource but missing nuances, e.g. synonyms adept, expert, good, practiced, proficient, skillful Missing new words (impossible to keep up to date): wicked, badness, nifty, crack, ace, wizard, genius, ninja Subjective Requires human labor to create and adapt Hard to compute accurate word similarity The vast majority of rule-based and statistical NLP work regards words as atomic symbols We use usually a localist representation (“one-hot”) to represent discrete word, but the different word vector $ a^T b = 0$, which means that our query and document vectors are orthogonal. There is no natural notion of similarity in a set of one-hot vectors “one-hot” vector could deal with similarity separately;instead we explore a direct approach where vectors encode it Distributional similarity based representationsYou can get a lot of value by representing a word by means of its neighbors You shall know a word by the company it keepsWe will build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context Basic idea of learning neural network word embeddingsDefine a model that aims to predict between a center word $w_t$ and context words in terms of vectors$$ p(context | w_t) = \dots $$which has a loss function, e.g.$$ J = 1 - p(w_{-t} | w_t ) $$We look at many positions $t$ in a big language corpusWe keep adjusting the vector representations of words to minimize this loss Directly learning low-dimensional word vectors Learning representations by back-propagating errors (Rumelhart et al., 1986) A neural probabilistic language model (Bengio et al., 2003) NLP (almost) from Scratch (Collobert &amp; Weston, 2008) A recent, even simpler and faster model:word2vec (Mikolov et al. 2013) $\rightarrow$ intro now Main idea of word2vecPredict between every word and its context wordsTwo algorithms Skip-grams(SG) Predict context words given target (position independent) Continuous Bag of Words(CBOW) Predict target from bag-of-words context Two (moderately efficient) training methods Hierarchical softmax Negative samplingNaive softmax The skip-gram modelreference: Skip-gram tutorialWord2vec uses a trick that we train a simple neural network with a single hidden layer to perform a certain task(Fake Task), but then we’re not actually going to use that neural network for the task we trained it on!Instead, the goal is actually just to learn the weights of the hidden layer (Similar to auto-encoder) Fake TaskTask goal : Given a specific word in the middle of a sentence, look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead A sample, window size = 5 Model detail Input: one-hot vector(dimension means the scale of vocabulary) Hidden layer: the word vector for picked word Output layer: softmax layer, probability that a randomly selected nearby word is that vocabulary word For example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron) So the end goal of all of this is really just to learn this hidden layer weight matrix.one-hot vector $\times$ hidden layer weight matrix $\iff$ lookup table objective functionFor each word $t=1,\dots,T$, predict surrounding words in a window of “radius” $m$ of every word. Maximize the probability of any context word given the current center word$$ J’(\theta) = \prod_{t=1}^{\pi} \prod_{-m \le j \le m, j \neq 0 } p \left(w_{t+j} | w_t; \theta \right) $$Negative Log likelihood$$ J(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \le j \le m, j \neq 0} \log p \left( w_{t+j} | w_{t} \right) $$Where $\theta$ represents all variable we will optimize]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP1 Introduction to NLP and DL]]></title>
    <url>%2F2017%2F04%2F16%2FNLP1%2F</url>
    <content type="text"><![CDATA[referencecs224n lecture 1 Introduction to NLP and DLNatural Language Processing (NLP)NLP is a field at the intersection of cs, ai and linguisticsGoal: for computers to process or “understand” natural language in order to perform tasks that are useful Performing Tasks, like making appointments, buying things Question Answering NLP levels Application Spell checking, keyword search, finding synonyms Extracting information Classifying Machine translation Spoken dialog systems Complex question answering … Human languageA human language is a system specifically constructed to convey the speaker/writer’s meaning No just an environment signal, it’s a deliberate communication Using an encoding which little kids can quickly learn(amazingly)A human language is a discrete/symbolic/categorical signaling system The categorical symbols of a language can be encoded as a signal for communication in several ways: Sound Gesture Images(writing)The symbol is invariant across different encodings! The large vocabulary, symbolic encoding of words creates a problem for machine learning-sparsity! Deep learningThe first breakthrough results of “deep learning” on large datasets happened in speech recognition Context-Dependent Pre-trained Deep Neural Network for Large Vocabulary Speech Recognition, Dahl et al.(2010) Why is NLP hard Complexity in representing, learning and using linguistic/situational/world/visual knowledge Human languages are ambiguous (unlike programming and other formal languages) Human language interpretation depends on real world, common sense, and contextual knowledge Deep NLP = Deep Learning + NLPCombine ideas and goals of NLP with using representation learning and deep learning methods to solve themSeveral big improvements in recent years in NLP with different Levels: speech, words, syntax, semantics Tools: parts-of-speech, entities, parsing Applications: machine translation, sentiment analysis, dialogue agents, question answering Representations of NLP levels: Semantics Traditional: Lambda calculus Carefully engineered functions Take as inputs specific other functions No notion of similarity or fuzziness of language DL: Every word and every phrase and every logical expression is a vector A neural network combines two vectors into one vector NLP Application: Sentiment Analysis Traditional: Curated sentiment dictionaries combined with bag-of-words representations(ignoring word order) or hand designed negation features Same deep learning models that was used for morphology, syntax and logical semantics can be used! $\rightarrow$ RecursiveNN Question Answering Traditional: A lot of feature engineering to capture world and other knowledge, e.g., regular expressions, Berant et al.(2014) DL: Again, a deep learning architecture can be used! Facts are stored in vectors Dialogue agents/Response Generation A simple, successful example is the auto-replies available in the Google Inbox app An application of the powerful, general technique of Neural Language Models, which are an instance of RNN Machine Translation Many levels of translation have been tried in the past Traditional MT systems are large complex systems]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data structure]]></title>
    <url>%2F2017%2F04%2F14%2Fdata-structure%2F</url>
    <content type="text"><![CDATA[data structure二叉搜索树二叉搜索树是一个满足以下性质的二叉树 对于任何结点$x$, 其左子树中的关键字最大不超过$x.key$, 其右子树中的关键字最小不低于$x.key$. 不同的二叉搜索树可以代表同一组值的集合, 大部分搜索树的最坏运行时间与树的高度成正比 遍历先序遍历(preorder tree walk): 根-左-右中序遍历(inorder tree walk): 左-根-右后序遍历(postorder tree walk): 左-右-根 根据二叉搜索树的性质, 通过中序遍历可以按序输出所有关键字12345INORDER-TREE-WALK(x):if x!= NIL INORDER-TREE-WALK(x.left) print x.key INORDER-TREE-WALK(x.right) 中序遍历需要$\Theta(n)$的时间 查询1234567TREE-SEARCH(x, k):if x==NIL or k==x.key return xif k&lt;x.key return TREE-SEARCH(x.left, k)else return TREE-SEARCH(x.right, k) 使用循环代替递归来重写这个程序1234567ITERATIVE-TREE-SEARCH(x, k):while x != NIL and k != key if k &lt; x.key x = x.left else x = x.rightreturn 最大关键字元素和最小关键字元素1234TREE-MINIMUM(x):while x.left != NIL x = x.leftreturn x 1234TREE-MAXIMUM(x):while x.right != NIL x = x.rightreturn x 后继和前驱12345678TREE-SUCCESSOR(x):if x.right != NIL return TREE-MINIMUM(x.right)y = x.pwhile y != NIL and x == y.right x = y y = y.preturn y 在一颗高度为$h$的二叉搜索树上, 动态集合上的操作SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, PREDECESSOR可以在$O(h)$时间内完成 插入和删除Insert把一个新值$v$插入到一颗完全二叉搜索树中, 需要调用过程TREE-INSERT. 该过程以节点$z$作为输入, 其中$z.key=v, z.left=NIL, z.right=NIL$, 这个过程需要修改$T,z$的某些属性, 来把$z$插入到树的相应位置上12345678910111213141516TREE-INSERT(T,z):y = NILx = T.rootwhile x != NIL y = x if z.key &lt; x.key x = x.left else x = x.rightz.p = yif y == NIL T.root = zelif z.key &lt; y.key y.left = zelse y.right = z Delete从一棵二叉搜索树$T$中删除一个结点$z$的整个策略分为四种情况 如果$z$没有左孩子, 那么用右孩子来替换$z$. 这里不管右孩子是NIL还是具体的结点 如果$z$仅有左孩子, 那么用左孩子来替换$z$ 如果$z$有两个孩子, 我们要查找$z$的后继$y$, 这个后继位于$z$的右子树中并且没有左孩子, 现在需要将$y$移出原来的位置进行拼接, 并替换树中的$z$ 如果$y$是$z$的右孩子, 那么用$y$替换$z$, 并仅留下$y$的右孩子 为了在二叉搜索树内移动子树, 定义一个子过程TRANSPLANT, 它是用另一棵子树替换一棵子树并成为其双亲的孩子结点. 当TRANSPLANT用一棵以$v$为根的子树来替换一棵以$u$为根的子树时, 结点$u$的双亲就变成了结点$v$的双亲, 并且最后$v$成为$u$的双亲的相应孩子. 123456789TRANSPLANT(T,u,v):if u.p == NIL T.root = velif u == u.p.left u.p.left = velse u.p.right = vif v != NIL v.p = u.p 利用TRANSPLANT过程,我们建立从二叉树$T$中删除结点$z$的算法12345678910111213TREE-DELETE(T,z)if z.left == NIL TRANSPLANT(T, z, z.right)elseif z.right == NIL TRANSPLANT(T, z, z.left)else y = TREE-MINIMUM(z.right) if y.p != z TRANSPLANT(T, y, y.right) y.right = z.right y.right.p = y TRANSPLATNT(T, z, y) y.left = z.left y.left.p = y 在一棵高度为$h$的二叉搜索树上, 实现动态集合操作INSERT和DELETE的运行时间均为$O(h)$ 图图的表示对于图$G=(V,E)$, 可以用两种标准表示方法表示: 邻接链表和邻接矩阵 邻接链表适合在稀疏图(边的条数$|E|$远远小于$|V|^2$的图), 对于图$G=(V,E)$来说, 其邻接链表表示由一个包含$|V|$条链表的数组$Adj$所构成, 每个结点有一条链表. 对于每个结点$u \in V$, 邻接链表$Adj[u]$包含所有与结点$u$之间有边连接的结点$v$, 即$Adj[u]$包含图$G$中所有与$u$邻接的结点如果$G$是一个有向图, 则所有邻接链表的长度之和等于$|E|$, 如果$G$是一个无向图, 所有邻接链表的长度之和等于$2|E|$ 邻接链表稍加修改即可以表示权重图, 我们可以直接将边的权重值存放在结点的邻接链表里. 邻接链表表示法的鲁棒性很高, 可以对其进行简单的修改来支持许多其他的图变种 邻接矩阵把图$G$中的结点编号为$1,2,\dots,|V|$, 则邻接矩阵可以有一个$|V| \times |V|$来表示, 该矩阵满足以下条件$$ a_{ij} = \begin{cases} 1 &amp; \text{若} (i,j) \in E \\ 0 &amp; \text{其他} \end{cases} $$无向图的邻接矩阵是对称的 广度优先搜索给定图$G=(V,E)$和一个可以识别的源结点$s$, 广度优先搜索对图$G$中的边进行系统性的探索来发现可以从源节点$s$到达的所有结点. 该算法始终是将已发现结点和未发现结点之间的边界, 沿其广度方向向外拓展, 算法需要在发现所有距离源结点$s$为$k$的所有结点之后, 才会发现距离源结点$s$为$k+1$的其他节点. 广度优先搜索会给结点染色(白色, 黑色, 灰色), 白色表示未发现的结点, 灰色和黑色的结点表示已被发现的结点, 灰色表示该结点周围存在着未被发现的白色结点, 黑色表示该结点周围的结点都已经被发现了. 在执行广度优先搜索的过程中将构造出一棵广度优先树. 如果我们通过结点$u$第一次搜索到了$v$, 那么称$u$是$v$的前驱或父结点. 下面给出广度优先搜索的算法, 其中我们把每个结点$u$的颜色存在属性$u.color$里, 将$u$的前驱结点存放在属性$u.pi$里, 如果$u$没有前驱结点(例如, $u=s$或者尚未被发现), 则$u.\pi = NIL$. 属性$u.d$记录的是广度优先搜索算法所计算出的从源结点$s$到结点$u$之间的距离. 该算法使用一个先进先出的队列$Q$来管理灰色结点集.12345678910111213141516171819BFS(G, s):for each vertex u in G.V - &#123;s&#125; u.color = WHITE u.d = inf u.pi = NILs.color = GRAYs.d = 0s.pi = NILQ = emptyENQUEUE(Q, s)while Q != empty u = DEQUEUE(Q) for each v in G.Adj[u] if v.color == WHITE v.color = GRAY v.d = u.d + 1 v.pi = u ENQUEUE(Q, v) u.color = BLACK 深度优先搜索深度优先搜索只要可能就在图中尽量深入. 深度优先搜索总是对最近才发现的结点$v$的出发边进行探索, 知道该结点的所有出发边都被发现为止. 一旦结点$v$的所有出发边都被发现, 搜索则回溯到$v$的前驱结点, 来搜索该前驱结点的出发边. 像广度优先搜索一样, 在对已被发现的结点$u$的邻接链表进行扫描时, 每当发现一个结点$v$时, 深度优先搜索算法将对这个事件进行记录, 将$v$的前驱属性$v.\pi$设置为$u$. 与广度优先搜索不同的是, 广度优先搜索的前驱子图形成一棵树, 而深度优先搜索的前驱子图可能由多棵树组成, 因为搜索可能从多个源结点重复进行. 深度优先搜索的前驱子图形成一个由多颗深度优先树构成的深度优先森林. 除了创建一个深度优先森林外, DFS还在每个结点上盖一个时间戳. 每个结点有两个时间戳: $v.d$记录结点$v$第一次被发现的时间(涂上灰色的时候), $v.f$记录完成对$v$的邻接链表扫描的时间(涂上黑色的时候). 显然结点$u$在时刻$u.d$之前为白色, 在时刻$u.d$和$u.f$之间为灰色, 在时刻$u.f$之后为黑色. 1234567891011121314151617181920DFS(G):for each vertex u in G.V u.color = WHITE u.pi = NILtime = 0for each vertex u in G.V if u.color == WHITE DFS-VISIT(G, u)DFS-VISIT(G, u):time = time + 1u.d = timeu.color = GRAYfor each v in G.Adj[u] if v.color == WHITE v.pi = u DFS-VISIT(G, v)u.color = BLACKtime = time + 1u.f = time]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>data structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL6 Value Function Approximation]]></title>
    <url>%2F2017%2F04%2F12%2FRL6%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL lecture 6 Value Function ApproximationIntroductionLarge-Scale Reinforcement LearningReinforcement learning can be used to solve large problems Value Function ApproximationValue Function by a lookup table Every state $s$ has an entry Q(s,a) Or every state-action pair $s,a$ has an entry $Q(s,a)$Problem with larger MDPs There are many states and/or actions to store in memory It is too slow to learn the value of each state individuallySolution for large MDPs Estimate value functions with function approximation $ \hat{v} (s,w) \approx v_{\pi}(s) $ or $ \hat{q} (s,a,w) \approx q_\pi (s,a) $ Generalise from seen states to unseen states Update parameters w using MC or TD learning Types of value function approximationWe consider differentiable function approximators Linear combinations of features Neural network Decision tree Nearest neighbour Fourier/wavelet bases $\dots$Furthermore, we require a training method that is suitable for non-stationary, non-iid data Incremental MethodsStochastic Gradient Descent Goal: find parameters vector $w$ minimising mean-squared error between approximate value fn $\hat{v}(s,w)$ and true value fn $v_{\pi} (s) $$$ J(w) = E_\pi [(v_\pi (S) -\hat{v} (S,w) )^2] $$ Gradient descent finds a local minimum$$ \Delta w = -\frac{1}{2} \alpha \nabla_w J(w)= \alpha E_\pi \left[ (v_\pi(S) -\hat{v}(S,w))\nabla_w \hat{v} (S,w) \right] $$ Stochastic gradient descent samples the gradient$$ \Delta w = \alpha (v_\pi (S) - \hat{v} (S,w)) \nabla_w \hat{v} (S,w) $$ Expected update is equal to full gradient update Linear Function ApproximationLinear Value Function Approximation Represent value function by a linear combination of features$$ \hat{S,w} = x(S)^T w = \sum_{j=1}^{n} x_j (S) w_j $$ Objective function is quadratic in parameters $w$$$ J(w) = E_\pi \left[ (v_\pi(S) - x(S)^T w)^2\right] $$ Stochastic gradient descent converges on global optimum Update rule is particularly simpleUpdata = step-size $\times$ prediction error $\times$ feature value Table Lookup Features Table lookup is special case of linear value function approximation Using table lookup features$$ x^{table} (S) = \begin{bmatrix} 1(S=s_1) \\ \vdots \\ 1(S=s_n) \end{bmatrix} $$ Parameter vector $w$ gives value of each individual state$$ \hat{v} (S,w) = \begin{bmatrix} 1(S=s_1) \\ \vdots \\ 1(S=s_n) \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ \vdots \\ w_n \end{bmatrix} $$ Incremental Prediction Algorithms Have assumed true value function $v_{\pi} (s)$ given by supervisor But in RL there is no supervisor, only rewards In practice, we substitute a target for $v_{\pi} (s)$ For MC, the target is the return $G_t$$$ \Delta w = \alpha (G_t - \hat{v} (S_t,w)) \nabla_{w} \hat{v} (S_t,w) $$ For TD(0), the target is the TD target $R_{t+1} + \gamma \hat{v} (S_{t+1},w) $$$ \Delta w = \alpha (R_{t+1} + \gamma \hat{v} (S_{t+1},w) - \hat{v} (S_t,w)) \nabla_{w} \hat{v} (S_t,w) $$ For TD($\lambda$), the target is the $\lambda$-return $G_t^\lambda$$$ \Delta w = \alpha (G_t^\lambda - \hat{v} (S_t,w)) \nabla_{w} \hat{v} (S_t,w) $$ MC with Value Function Approximation Return $G_t$ is an unbiased, noisy sample of true value $v_{pi} (S_t)$ Can therefore apply supervised learning to “training data”:$$ (S_1, G_1), (S_2, G_2), \dots, (S_T, G_T) $$ For example, using linear MC policy evaluation$$ \Delta w = \alpha (G_t - \hat{v} (S_t,w)) \Delta_w \hat{v} (S_t,w) = \alpha(G_t - \hat{v} (S_t,w))x(S_t) $$ Monte-Carlo evaluation converges to a local optimum Even when using non-linear value function approximation TD Learning with Value Function Approximation The TD-target $R_{t+1} + \gamma \hat{v} (S_{t+1},w)$ is a biased sample of true value $v_\pi (S_t)$ Can still apply supervised learning to “training data”:$$ (S_1, R_2 + \gamma \hat{v} (S_2,w) ), (S_2, R_3 + \gamma \hat{v} (S_3,w)), \dots, (R_{T-1},R_T) $$ For example, using linear TD(0)$$ \Delta w = \alpha (R+\gamma \hat{v} (S’,w) - \hat{v} (S,w)) \Delta_w \hat{v} (S,w) = \alpha \delta x(S) $$ Linear TD(0) converges (close) to global optimum Incremental Control AlgorithmControl with Value function ApproximationPolicy evaluation: Approximate policy evaluation, $\hat{q}(\cdot,\cdot,w) \approx q_\pi$Policy improvement: $\epsilon$-greedy policy improvement Action-Value Function Approximation Approximate the action-value function $\hat{q}(S,A,w) \approx q_{\pi} (S,A)$ Minimise mean-squared error between approximate action-value fn $\hat{q}(S,A,w)$ and true action-value fn $q_\pi (S,A)$$$ J(w) = E_\pi \left[(q_\pi (S,A) - \hat{q} (S,A,w))^2 \right] $$ Use stochastic gradient descent to find a local minimum$$ -\frac{1}{2} \nabla_w J(w) = ( q_{\pi} (S,A) -\hat{q} (S,A,w)) \nabla_w \hat{q} (S,A,w) \\ \Delta w = \alpha (q_{\pi} (S,A) -\hat{q} (S,A,w))\nabla_w \hat{q} (S,A,w) $$ Linear Action-Value Funtion Approximation Represent state and action by a feature vector$$ X(S,A) = \begin{pmatrix} x_1 (S,A) \\ \vdots \\ x_n (S,A) \end{pmatrix} $$ Represent action-value fn by linear combination of features$$ \hat{q} (S,A,w) = x(S,A)^T w = \sum_{j=1}^n x_j (S,A)w_j $$ Stochastic gradient descent update$$ \nabla_w \hat{q} (S,A,w) = x(S,A) \\ \Delta w =\alpha (q_\pi (S,A) - \hat{q} (S,A,w)) x(S,A) $$ Incremental Control Algorithms Like prediction, we must substitute a target for $q_\pi (S,A)$ For MC, the target is the return $G_t$$$ \Delta w = \alpha (G_t - \hat{q} (S_t,A_t,w)) \nabla_w \hat{q} (S_t,A_t,w) $$ For TD(0), the target is the TD target $R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) $$$ \Delta w = \alpha (R_{t+1}+\gamma\hat{q} (S_{t+1},A_{t+1},w) - \hat{q} (S_t,A_t,w)) \nabla_w \hat{q} (S_t,A_t,w) $$ For forward-view TD($\lambda$), target is the action-value $\lambda$-return$$ \Delta w = \alpha (q_t^\lambda - \hat{q} (S_t,A_t,w)) \nabla_w \hat{q} (S_t,A_t,w) $$ For backward-view TD($\lambda$), equivalent update is$$ \delta_t = R_{t+1}+\gamma\hat{q} (S_{t+1},A_{t+1},w) - \hat{q} (S_t,A_t,w) \\ E_t =\gamma \lambda E_{t-1} + \Delta_w \hat{q} (S_t,A_t,w) \\ \Delta w = \alpha \delta_t E_t $$ ConvergenceConvergence of Prediction AlgorithmsGradient Temporal-Difference Learning TD does not follow the gradient of any objective function This is why TD can diverge when off-policy or using non-linear function approximation Gradient TD follows true gradient of projected Bellman error Convergence of Control Algorithms Batch MethodsBatch Reinforcement Learning Gradient descent is simple and appealing But it is not sample efficient Batch methods seek to find the best fitting value function Given the agent’s experience (‘training data”) Least Squares Prediction Given value function approximation $\hat{v}(s,w) \approx v_\pi (s) $ And experience D consisting of (state,value) pairs$$ D = ((s_1,v_1^\pi),(s_2,v_2^\pi),\dots,(s_T,v_T^\pi)) $$ Which parameters $w$ given the best fitting value fn $\hat{v} (s,w)$? Least squares algorithms find parameters vector $w$ minimising sum-squared error between $\hat{v} (s_t,w)$ and target values $v_t^\pi$$$ LS(w) = \sum_{t=1}^{T} (v_t^T - \hat{v} (s_t,w))^2 = E_D \left[ (v^\pi - \hat{v} (s,w))^2\right] $$ Stochastic Gradient Descent with Experience ReplayRepeat: Sample state, value from experience $(s,v^\pi)\sim D$ Apply stochastic gradient descent update $\Delta w = \alpha (v^\pi -\hat{v} (s,w)) \nabla_w \hat{v} (s,w) $Converges to least squares solution$$ w^\pi = \arg\min_w LS(w) $$ Experience Replay in Deep Q-Network(DQN)DQN uses experience replay and fixed Q-targets Take action $a_t$ according to $\epsilon$-greedy policy Store transition $(s_t,a_t,r_{t+1},s_{t+1})$ in replay memory $D$ Sample random mini-batch of transitions $(s,a,r,s’)$ from $D$ Compute Q-learning targets w.r.t old, fixed parameters $w^-$ Optimise MSE between Q-network and Q-learning targets$$ L_i (w_i) = E_{s,a,r,s’ \sim D_i} \left[ \left( r + \gamma \max_{a’} Q(s’,a’;w_i^-) - Q(s,a;w_i) \right)^2 \right] $$ Using variant of stochastic gradient descent Linear Least Squares Prediction Experience replay finds least squares solution But it may take many iterations Using linear value function approximation $\hat{v} (s,w) = x(s)^T w $ We can solve the least squares solution directly At minimum of $LS(w)$, the expected update must be zero, $E_D (\Delta w) = 0$$$ \alpha \sum_{t=1}^{T} x(s_t) (v_t^\pi -x(s_t)^T w) = 0 \\ w =\left( \sum_{t=1}^T x(s_t)x(s_t)^T \right)^{-1} \sum_{t=1}^T x(s_t) v_t^\pi $$ For $N$ features, direct solution time is $O(n^3)$ Incremental solution time is $O(n^2)$ using Shermann-Morrison Linear Least Squares Prediction Algorithms We don’t know true values $v_t^\pi$ In practice, our “training data” must use noisy or biased sample of $v_t^\pi$ LSMC Least Squares MC uses return $v_t^\pi \approx G_t$ LSTD Least Squares TD uses TD target $v_t^\pi \approx R_{t+1} + \gamma \hat{v}(S_{t_1},w) $ LSTD($\lambda$) Least Squares TD($\lambda$) use $\lambda$-return $v_t^\pi \approx G_t^\lambda$ In each case solve directly for fixed point of MC/TD/TD($\lambda$) Convergence of Linear Least Squares Prediction Algorithms Least Squares ControlLeast Squares Policy IterationPolicy evaluation Policy evaluation by least squares Q-learningPolicy improvement Greedy policy improvement Least Squares Action-Value Function Approximation Approximate action-value function $q_\pi (s,a)$ using linear combination of features $x(s,a)$$$ \hat{q} (s,a,w) = x(s,a)^T w \approx q_\pi (s,a) $$ Minimise least squares error between $\hat{q} (s,a,w)$ and $q_\pi (s,a)$ form experience generated using policy $\pi$ consisting of $&lt;(state,action),value>$ pairs$$ D = \{ &lt; (s_1,a_1),v_1^\pi >, &lt;(s_2,a_2),v_2^\pi>,\dots,&lt;(s_T,a_T),v_T^\pi> \} $$ Least Squares Control For policy evaluation, we want to efficiently use all experience For control, we also want to improve the policy This experience is generated from many policies So to evaluate $q_pi (S,A)$ we must learn off-policy We use the same idea as Q-learning: Use experience generated by old policy, $ S_t,A_t,R_{t+1},S_{t+1} \sim \pi_{old} $ Consider alternative successor action $ A’ = \pi_{new} (S_{t+1}) $ Update $\hat{q} (S_t,A_t,w) $ towards value of alternative action $R_{t+1} + \gamma \hat{q} (S_{t+1},A’,w)$ Least Squares Q-Learning Consider the following linear Q-learning update$$ \delta = R_{t+1} + \gamma \hat{q} (S_{t+1},\pi(S_{t+1}),w) - \hat{q} (S_t,A_t,w) \\ \Delta w =\alpha \delta x(S_t,A_t) $$ LSTDQ alorithm: solve for total update = zero$$ 0 = \sum_{t=1}^T \alpha (R_{t+1} +\gamma \hat{q} (S_{t+1},\pi(S_{t+1}),w) -\hat{q} (S_t,A_t,w)) x(S_t,A_t) \\ w = \left( \sum_{t=1}^T x(S_t,A_t) (x(S_t,A_t)-\gamma x(S_{t+1},\pi(S_{t+1})))^T \right)^{-1} \sum_{t=1}^T x(S_t,A_t) R_{t+1} $$ Least Squares Policy Iteration Algorithm The following pseudocode uses LSTDQ for policy evaluation It repeatedly re-evaluates experience $D$ with difference policy Convergence of Control Algorithms$(\checkmark)$ = chatters around near-optimal value function]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sortAlgorithm]]></title>
    <url>%2F2017%2F04%2F11%2FsortAlgorithm%2F</url>
    <content type="text"><![CDATA[Several pseudo-code for sort algorithmbubble sortworst time complexity: $O(n^2)$best time complexity $O(n)$average time complexity $O(n^2)$additional space complexity $O(1)$12345bubble_sort(list, p, r):for i = p to r-1 for j = i+1 to r if list[i] &gt; list[j] swap(list[i],list[j]) selection sortaverage time complexity $O(n^2)$additional space complexity $O(1)$12345678selection_sort(list, p, r):for i = p to r-1 min = i for j = i+1 to r if list[j] &lt; min min = j if min!=i swap(list[i], list[min]) Merge sorttime complexity: $O(n\log n)$space complexity: $O(n)$12345678910111213141516171819merge_sort(list, p, r):if p &gt;= r returnmid = ceil(p+r)merge_sort(list, p, mid)merge_sort(list, mid+1, r)i = pj = mid+1k = 1while(i&lt;=mid &amp;&amp; j &lt;= r) if list[i] &gt; list[j] tmp[k++] = list[j++] else tmp[k++] = list[i++]while(i&lt;=mid) tmp[k++] = list[i++]while(j&lt;=r) tmp[k++] = list[j++]list[p..r] = tmp Quick sortworst time complexity: $O(n^2)$best time complexity: $O(n\log n)$average time complexity: $O(n \log n)$additional space complexity: $O(1)$123456789101112131415161718192021quick_sort(list, p, r):if p&gt;=r returnelse mid = partition(list, p, r) quick_sort(list, p, mid-1) quick_sort(list, mid+1, r)partition(list, p, r):tmp = list[r]i = pj = r-1while(i&lt;j) while(i&lt;j &amp;&amp; list[i] &lt;= tmp) i++ while(i&lt;j &amp;&amp; list[j] &gt;= tmp) j-- if(i&lt;j) swap(list[i], list[j])swap(list[i], list[r])return i heap sorttime complexity: $O(n\log n)$additional space complexity $O(1)$123456789101112131415161718192021222324MAX-HEAPIFY(A, i):l = LEFT(i)r = RIGHT(i)if l &lt;= A.heap-size and A[l]&gt;A[i] largest = lelse largest = iif r &lt;= A.heap-size and A[r]&gt;A[largest] largest = rif largest != i swap(A[i], A[largest]) MAX-HEAPIFY(A, i)BUILD-MAX-HEAP(A):A.heap-size = A.lengthfor i = ceil(A.length/2) downto 1 MAX-HEAPFIFY(A,i)HEAPSORT(A):BUILD-MAX-HEAP(A)for i=A.length downto 2 swap(A[1], A[i]) A.heap-size = A.heap-size - 1 MAX-HEAPIFY(A,1)]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL5 Model-Free Control]]></title>
    <url>%2F2017%2F04%2F10%2FRL5%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL lecture 5: Model-Free ControlIntroductionOptimise the value function of an unknown MDP Model-free control can solve these problems, either: MDP model is unknown, but experience can be sampled MDP model is known, but is too big to use, except by samples On and Off-Policy Learning On-policy learning Learn on the job Learn about policy $\pi$ from experience sampled from $\pi$ Off-policy learning Look over someone’s shoulder Learn about policy $\pi$ from experience sampled from $\mu$ On-Policy Monte-Carlo ControlGeneralised Policy IterationGeneralised Policy Iteration(Refresher)Policy evaluation: Estimate $v_\pi$, Iterative policy evaluationPolicy improvement: Generate $\pi’ \ge \pi$, Greedy policy improvement Generalised Policy Iteration With Monte-Carlo EvaluationPolicy evaluation: Monte-Carlo policy evaluation, $V=v_\pi$Policy improvement: Greedy policy improvement Model-Free Policy Iteration Using Action-Value Function Greedy policy improvement over $V(s)$ requires model of MDP$$ \pi’(s) = \arg\max_{a \in A} R_s^a +P_{ss’}^a V(s’)$$ Greedy policy improvement over $Q(s,a)$ is model-free$$ \pi’(s) = \arg\max_{a \in A} Q(s,a) $$ Generalised Policy Iteration with Action-Value FunctionPolicy evaluation: Monte-Carlo policy evaluation, $Q=q_\pi$Policy improvement: Greedy policy improvement $\epsilon$-Greedy Exploration Simplest idea for ensuring continual exploration All $m$ actions are tried with non-zero probability With probability $1-\epsilon$ choose the greedy action With probability $\epsilon$ choose an action at random$$ \pi(a|s) = \begin{cases} \epsilon/m + 1 -\epsilon \quad &amp;\text{if } a^* = \arg\max_{a\in A} Q(s,a) \\ \epsilon/m &amp;\text{otherwise}\end{cases}$$ TheoremFor any $\epsilon$-greedy policy $\pi$, the $\epsilon$-greedy policy $\pi’$ with respect to $q_\pi$ is an improvement, $v_{\pi’} (s) \ge v_\pi (s) $ Monte-Carlo Policy Iteration Policy evaluation Monte-Carlo policy evaluation, $Q=q_\pi$ Policy improvement $\epsilon$-greedy policy improvement Monte-Carlo ControlEvery episode Policy evaluation Monte-Carlo policy evaluation $Q\approx q_\pi$ Policy improvement $\epsilon$-greedy policy improvement GLIE DefinitionGreedy in the Limit with Infinite Exploration(GLIE) All state-action pairs are explored infinitely many times$$ \lim_{k\rightarrow \infty} N_k (s,a) =\infty $$ The policy converges on a greedy policy$$ \lim_{k \rightarrow \infty} \pi_k (a|s) = 1(a = \arg\max_{a’ \in A} Q_k (s,a’) )$$ GLIE Monte-Carlo Control Sample $k$th episode using $\pi$: $(S_1,A_1,R_2,\dots,S_T) \sim \pi$ For each state $S_t$ and action $A_t$ in the episode$$ N(S_t,A_t) \leftarrow N(S_t,A_t) +1 $$$$ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}(G_t - Q(S_t,A_t)) $$ Improve policy based on new action-value function$$ \epsilon \leftarrow 1/k\qquad \pi \leftarrow \epsilon-\text{greedy}(Q) $$ TheoremGLIE Monte-Carlo control converges to the optimal action-value function, $Q(s,a) \rightarrow q_* (s,a)$ On-policy Temporal-Difference LearningMC vs. TD Control TD learning has several advantages over Monte-Carlo(MC) Lower variance Online Incomplete sequences Natural idea: use TD instead of MC in our control loop Apply TD to $Q(S,A)$ Use $\epsilon$-greedy policy improvement Update every time-step Sarsa($\lambda$)Updating Action-Value Functions with Sarsa$$ Q(S,A) \leftarrow Q(S,A) + \alpha(R+\gamma Q(S’,A’) -Q(S,A)) $$ On-Policy Control With SarsaEvery time-step:Policy evaluation Sarsa, $Q\approx q_{\pi} $Policy improvement $\epsilon$-greedy policy improvement Sarsa Algorithm for On-Policy Control Convergence of Sarsa TheoremSarsa converges to the optimal action-value function, $Q(s,a) \rightarrow q_* (s,a)$, under the following conditions: GLIE sequence of policies $\pi_t (a|s)$ Robbins-Monro sequence of step-sizes $\alpha_t$$$ \sum_{t=1}^{\infty} \alpha_t = \infty \qquad \sum_{t=1}^{\infty} \alpha_t^2 &lt; \infty $$ n-Step Sarsa Define the n-step Q-return$$ q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}) $$ n-step Sarsa updates $Q(s,a)$ towards the n-step Q-return$$ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (q_t^{(n)} - Q(S_t,A_t)) $$ Forward View Sarsa($\lambda$) The $q^\lambda$ return combines all n-step Q-return $q_t^{(n)}$ Using weight $(1-\lambda)\lambda^{n-1}$$$ q_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)} $$ Forward-view Sarsa($\lambda$)$$ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (q_t^\lambda - Q(S_t,A_t)) $$ Backward View Sarsa($\lambda$) Just like TD($\lambda$), we use eligibility traces in an online algorithm But Sarsa($\lambda$) has one eligibility trace for each state-action pair$$ E_0 (s,a) = 0 \qquad E_t(s,a) = \gamma \lambda E_{t-1} (s,a) + 1(S_t = s,A_t =a) $$ $Q(s,a)$ is updated for every state $s$ and action $a$ In proportion to TD-error $\delta_t$ and eligibility trace $E_t(s,a)$$$ \delta_t = R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) -Q(S_t,A_t)$$$$ Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t(s,a) $$ Sarsa($\lambda$) Algorithm Off-Policy Learning Evaluate target policy $\pi(a|s)$ to compute $v_\pi (s)$ or $q_\pi (s,a)$ While following behaviour policy $\mu(a|s)$$$ (S_1,A_1,R_2,\dots,S_T) \sim \mu $$ Why is this important? Learn from observing humans or other agents Re-use experience generated from old policies $\pi_1,\pi_2,\dots,\pi_{t-1}$ Learn about optimal policy while following exploratory policy Learn about multiple policies while following one policy Importance SamplingEstimate the expectation of a different distribution$$ E_{X\sim P} [f(X)] = \sum P(X) f(X) = \sum Q(X) \frac{P(X)}{Q(X)} f(X) = E_{X\sim Q} \left[ \frac{P(X)}{Q(X)} f(X)\right] $$ Importance Sampling for Off-Policy Monte-Carlo Use returns generated from $\mu$ to evaluate $\pi$ Weight return $G_t$ according to similarity between policies Multiply importance sampling corrections along whole episode$$ G_t^{\pi/\mu} = \frac{\pi(A_t|S_t) \pi(A_{t+1}|S_{t+1}) \dots \pi(A_T|S_T)}{\mu(A_t|S_t) \mu(A_{t+1}|S_{t+1}) \dots \mu(A_t|S_t)}G_t $$ Update value towards corrected return$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t^{\pi/\mu} - V(S_t)) $$ Cannot use if $\mu$ is zero when $\pi$ is non-zero Importance sampling can dramatically increase variance On the other hand, we can get lower variance using TD as follow: Use TD target generated from $\mu$ to evaluate $\pi$ Weight TD target $R+\gamma V(S’)$ by importance sampling Only need a single importance sampling correction$$ V(S_t) \leftarrow V(S_t) + \alpha \left( \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1} + \gamma V(S_{t+1})) -V(S_t) \right)$$ Much lower variance than Monte-Carlo importance sampling Policies only need to be similar over a single step Q-learning We now consider off-policy learning of action-values $Q(s,a)$ No importance sampling is required Next action is chosen using behaviour policy $A_{t+1} \sim \mu (\cdot |S_t) $ But we consider alternative successor action $A’ \sim \pi(\cdot| S_t)$ And update $Q(S_t,A_t)$ towards value of alternative action$$ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1},A’) - Q(S_t,A_t)) $$ Off-policy Control with Q-learning We now allow both behaviour and target policies to improve The target policy $\pi$ is greedy w.r.t $Q(s,a)$$$ \pi (S_{t+1}) = \arg\max_{a’} Q(S_{t+1},a’) $$ The behaviour policy $\mu$ is e.g. $\epsilon$ w.r.t $Q(s,a)$ The Q-learning target then simplifies$$ R_{t+1} + \gamma Q(S_{t+1},A’) = R_{t+1} + \gamma Q(S_{t+1},\arg\max_{a’} Q(S_{t+1},a’) ) = R_{t+1}+ \max_{a’} \gamma Q(S_{t+1},a’) $$ TheoremQ-learning control converges to the optimal action-value function $Q(s,a) \rightarrow q_* (s,a) $ Q-learning Algorithm for Off-Policy Control SummaryRelationship between DP and TD where $x \leftarrow^{\alpha} y \iff x \leftarrow x + \alpha (y-x) $]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL4 Model-Free Prediction]]></title>
    <url>%2F2017%2F04%2F06%2FRL4%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL Lecture 4 Model-Free PredictionEstimate the value function of an unknown MDP Monte-Carlo Reinforcement learning MC methods learn directly from episodes of experience MC is model-free: no knowledge of MDP transition/rewards MC learns from complete episodes: no bootstrapping MC uses the simplest possible idea: value = mean return Caveat: can only apply MC to episodic MDPs All episodes must terminate MC Policy Evaluation Goal: learn $v_{\pi}$ from episodes of experience under policy $\pi$, $S_1,A_1,R_2,\dots,S_k \sim \pi $ Recall that the return is the total discounted reward: $G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_{T} $ Recall that the value function is the expected return: $v_{\pi} (s) = E_{\pi} [G_t|S_t =s]$ MC policy evaluation uses empirical mean return instead of expected return First-Visit MC Policy Evaluation To evaluate state $s$ The first time-step $t$ that state $s$ is visited in an episode Increment counter $N(s) \longleftarrow N(s) +1 $ Increment total return $S(s) \longleftarrow S(s) +G_t$ Value is estimated by mean return $V(s) = S(s)/N(s) $ By law of large numbers, $V(s) \rightarrow v_{\pi} (s) $ as $N(s) \rightarrow \infty$ Every-Visit MC Policy Evaluation To evaluate state $s$ Every time-step $t$ that state $s$ is visited in an episode Increment counter $N(s) \longleftarrow N(s) +1 $ Increment total return $S(s) \longleftarrow S(s) +G_t$ Value is estimated by mean return $V(s) = S(s)/N(s) $ By law of large numbers, $V(s) \rightarrow v_{\pi} (s) $ as $N(s) \rightarrow \infty$ Incremental MCIncremental MeanThe mean $\mu_1,\mu_2,\dots$ of a sequence $x_1,x_2,\dots$ can be computed incrementally,$$\mu_k = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1}) $$ Incremental MC updates Update $V(s)$ incrementally after episode $S_1,A_1,R_1,\dots,S_T$ For each state $S_t$ with return $G_t$.$$ N(S_t) \leftarrow N(S_t) + 1 $$$$ V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t)) $$ In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes$$ V(S_t) \leftarrow V(S_t) + \alpha (G-V(S_t)) $$ $\alpha$ is the update rate. Temporal-Difference Learning TD method learn directly from episodes of experience TD is model-free: no knowledge of MDP transitions/rewards TD learns from incomplete episodes, by bootstrapping TD updates a guess towards a guess TD algorithm Goal: learn $v_{\pi}$ online from experience under policy $\pi$ Incremental every-visit MC Update value $V(S_t)$ toward actual return $G_t$$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t)) $$ Simplest temporal-difference learning algorithm: TD(0) Update value $V(S_t)$ toward estimated return $R_{t+1} + \gamma V(S_{t+1})$$$ V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) -V(S_t)) $$ $R_{t+1}+\gamma V(S_{t+1})$ is called the TD target $\delta_t = R_{t+1} + \gamma V(S_{t+1}) -V(S_t)$ is called the TD error Bias/Variance Trade-off Return $G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T$ is unbiased estimate of $v_{\pi} (S_t)$ True TD target $R_{t+1} + \gamma v_{\pi} (S_{t+1}) $ is unbiased estimate of $v_{\pi}(S_t)$ TD target $R_{t+1} + \gamma V(S_{t+1})$ is biased estimate of $v_{\pi} (S_t)$ TD target is much lower variance than the return Return depends on many random actions, transitions, rewards TD target depends on one random action, transition, reward Batch MC and TD MC and TD converge: $V(s) \rightarrow v_{\pi} (s)$ as experience $\rightarrow \infty$ But what about batch solution for finite experience? e.g. Repeatedly sample episode $k\in [1,K] $ Apply MC or TD(0) to episode $k$ Certainty Equivalence MC converges to solution with minimum mean-squared error Best fit to the observed returns$$ \sum_{k=1}^{K} \sum_{t=1}^{T_k} (G_t^k -V(s_t^k))^2 $$ TD(0) converges to solution of max likelihood Markov model Solution to the MDP $(S,A,\hat{P},\hat{R},\gamma)$ that best fits the data$$ \hat{P}_{s,s’}^a = \frac{1}{N(s,a)} \sum_{k=1}^{K} \sum_{t=1}^{T_k} 1(s_t^k,a_t^k,s_{t+1}^k = s,a,s’) $$$$ \hat{R}_s^a = \frac{1}{N(s,a)} \sum_{k=1}^{K} \sum_{t=1}^{T_k} 1(s_t^k,a_t^k = s,a) r_t^k $$ Advantages and Disadvantages of MC vs. TD TD can learn before knowing the final outcome TD can learn online after every step MC must wait until end of episode before return is known TD can learn without the final outcome TD can learn from incomplete sequences MC can only learn from complete sequences TD works in continuing (non-terminating) environments MC only works for episodic (terminating) environments MC has high variance, zero bias Good convergence properties (even with function approximation) Not every sensitive to initial value Very simple to understand and use TD has low variance, some bias Usually more efficient than MC TD(0) converges to $v_{\pi} (s)$ (but not always with function approximation) More sensitive to initial value TD exploits Markov property Usually more efficient in Markov environments MC does not exploit Markov property Usually more efficient in non-Markov environments Unified ViewMonte-Carlo BackupTemporal-Difference BackupDynamic Programming BackupUnified View of Reinforcement Learning Bootstrapping and Sampling Bootstrapping: update involves an estimate MC dose not bootstrap DP/TD bootstraps Sampling: update samples an expectation MC/TD samples DP does not sample TD($\lambda$)n-Step Return Consider the following n-step returns for $n=1,2,\dots$ n=1 (TD) $G_t^{(1)}=R_{t+1} + \gamma V(S_{t+1})$ n=2 $G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$ n=$\infty$, $G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T $ Define the n-step return$$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^{n} V(S_{t+n})$$ n-step temporal-difference learning$$ V(S_t) \leftarrow V(S_t) + \alpha (G_{(n)} - V(S_t)) $$ Averaging n-Step Returns We can average n-step returns over different n e.g average the 2-step and 4-step returns $\frac{1}{2} G^{(2)} + \frac{1}{2} G^{(4)}$ Combines information from two different time-steps Can we efficiently combine information from all time-steps $\lambda$ return The $\lambda-$return $G_t^{\lambda}$ combines all n-step return $G_t^{(n)}$ Using weight $(1-\lambda)\lambda^{n-1}$$$ G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} $$ Forward-view TD($\lambda$)$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t^{\lambda} -V(S_t)) $$ Forward-view TD($\lambda$)$$ G_{t}^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} $$ Update value function towards the $\lambda$-return Forward-view looks into the future to compute $G_t^{\lambda}$ Like MC, can only be computed from complete episodes Backward-view TD($\lambda$) Forward view provides theory Backward view provides mechanism Update online, every step, from incomplete sequences Relationship Between Forward and Backward TDTD($\lambda$) and TD(0) when $\lambda=0$, only current state is updated$$ E_t (s) = 1(S_t =s) \qquad V(s) \leftarrow V(s) + \alpha \delta_t E_t(s)$$ This is exactly equivalent to TD(0) update$$ V(S_t) \leftarrow V(S_t) + \alpha \delta_t $$ TD($\lambda$) and MC When $\lambda=1$, credit is deferred until end of episode Consider episodic environments with offline updates Over the course of an episode, total update for TD(1) is the same as total update for MC TheoremThe sum of offline updates is identical for forward-view and backward-view TD($\lambda$)$$ \sum_{t=1}^{T} \alpha \delta_t E_t(s) = \sum_{t=1}^T \alpha (G_t^{\lambda} - V(S_t) ) 1(S_t =s) $$ Forward and Backward EquivalenceMC and TD(1) Consider an episode where $s$ is visited once at time-step $k$. TD(1) eligibility trace discounts time since visit,$$ E_t(s) = \gamma E_{t-1} (s) + 1(S_t = s) = \begin{cases} 0 &amp; \text{if}~ t&lt;k \\ \gamma^{t-k} &amp; \text{if} ~ t\ge k \end{cases}$$ TD(1) updates accumulate error online$$ \sum_{t=1}^{T-1} \alpha \delta_t E_t(s) = \alpha \sum_{t=k}^{T-1} \gamma^{t-k} \delta_t = \alpha (G_k - V(S_k)) $$ By end of episode it accumulates total error$$ \delta_k + \gamma \delta_{k+1} + \gamma^2 \delta_{k+2} + \dots + \gamma^{T-1-k} \delta_{T-1} $$ TD(\lambda) and TD(1) TD(1) is roughly equivalent to every-visit Monte-Carlo Error is accumulated online, step-by-step If value function is only updated offline at end of episode Then total update is exactly the same as MC Forward and Backwards TD($\lambda$) Consider an episode where $s$ is visited once at time-step $k$ TD($\lambda$) eligibility trace discounts time since visit$$ E_t(s) = \gamma\lambda E_{t-1} (s) + 1(S_t = s) = \begin{cases} 0 &amp; \text{if}~ t&lt;k \\ (\gamma\lambda)^{t-k} &amp; \text{if} ~ t\ge k \end{cases}$$ Backward TD($\lambda$) updates accumulate error online$$ \sum_{t=1}^{T} \alpha \delta_t E_t(s) = \alpha \sum_{t=k}^T (\gamma\lambda)^{t-k} \delta_t = \alpha(G_k^\lambda - V(S_k)) $$ By end of episode it accumulates total error for $\lambda$-return For multiple visits to $s$, $E_t(s)$ accumulates many errors Offline Equivalence of Forward and Backward TDOffline updates Updates are accumulated within episode but applied in batch at the end of episode Online updates TD($\lambda$) updates are applied online at each step within episode Forward and backward-view TD($\lambda$) are slightly different New: Exact online TD($\lambda$) achieves perfect equivalence By using a slightly differently form of eligibility trace Summary of Forward and Backward TD($\lambda$)]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL3 Planning by Dynamic Programming]]></title>
    <url>%2F2017%2F04%2F05%2FRL3%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL lecture 3 Planning by Dynamic ProgrammingintroductionDefinition Dynamic: sequential or temporal component to the problemProgramming: optimising a “program”, i.e. a policy A method for solving complex problems By breaking them down into sub-problems Solve the sub-problems Combine solutions to sub-problems RequirementsDynamic Programming is a very general solution method for problems which have two properties: Optimal substructure Principle of optimality applies Optimal solution can be decomposed into sub-problems Overlapping sub-problems Sub-problems recur many times Solution can be cached and reused Markov decision processes satisfy both properties Bellman equation gives recursive decomposition Value function stores and reuses solutions Planning by DP DP assumes full knowledge of the MDP It is used for planning in an MDP For prediction Input: MDP (S,A,P,R,y) and policy $\pi$ or: MRP $(S,P^{\pi},R^{\pi},y)$ Output: value function $v_{\pi}$ Or for control: Input: MDP (S,A,P,R,y) Output: optimal value function $v_*$ and: optimal policy $\pi_*$ Policy EvaluationIterative Policy Evaluation Problem: evaluate a given policy $\pi$ Solution: iterative application of Bellman expectation backup Using synchronous backups At each iteration $k+1$ For all states $s\in S$ Update $v_{k+1} (s)$ from $v_k(s’)$ where $s’$ is a successor state of $s$$$ v^{k+1} = R^{\pi} + y P^{\pi} v^k $$ Policy IterationPolicy Improvement Given a policy $\pi$ Evaluate the policy $\pi$ Improve the policy by acting greedily with respect to $v_{\pi}$ In general, need more iterations of improvement/evaluation But this process of policy iteration always converges to $\pi^*$ STEP Consider a deterministic policy, $a=\pi (s) $ We can improve the policy by acting greedily This improves the value from any state $s$ over one step It therefore improves the value function, $v_{\pi’} (s) \ge v_{\pi} (s) $ If improvements stop, $q_{\pi} (s,\pi’(s)) = v_{\pi} (s)$ Then the Bellman optimality equation has been satisfied $v_{\pi} (s) = \max_{a\in A} q_{\pi} (s,a)$ Therefore $v_{\pi} (s) = v_* (s) $ for all $s\in S$, so $\pi$ is an optimal policy Extension to Policy Iteration Does policy evaluation need to converge to $v_{\pi}$ Or should we introduce a stopping condition Or simply stop after k iterations of iterative policy evaluation Value IterationPrinciple of OptimalityAny optimal policy can be subdivided into two components An optimal first action $A_*$ Followed by an optimal policy from successor state $S’$ Theorem A policy $\pi (a|s)$ achieves the optimal value from state $s$, $v_{\pi} (s) = v_* (s)$, if and only if For any state $s’$ reachable from s $\pi$ achieves the optimal value from state $s’$ Deterministic Value Iteration If we know the solution to sub-problems $v_*(s’)$ Then solution $v_*(s)$ can be found by one-step lookahead The idea of value iteration is to apply these updates iteratively Intuition: start with final rewards and work backwards Still works with loopy, stochastic MDPs Value Iteration Problem: find optimal policy $\pi$ Solution: iterative application of Bellman optimality backup Unlike policy iteration, there is no explicit policy Intermediate value functions may not correspond to any policy Summary of DP algorithms Problem Bellman Equation Algorithm Prediction Bellman Expectation Equation Iterative Policy Evaluation Control Bellman Expectation Equation + Greedy Policy Improvement Policy Iteration Control Bellman Optimality Equation Value Iteration Algorithms are based on state-value function $v_{\pi} (s) $ or $v_* (s) $ Complexity $O(mn^2)$ per iteration, for $m$ action and $n$ states Could also apply to action-value function $q_{\pi} (s,a)$ or $q_* (s,a)$ Complexity $O(m^2 n^2)$ per iteration Extensions to DPAsynchronous DP Asynchronous DP backs up states individually, in any order For each selected state, apply the appropriate backup Can significantly reduce computation Guaranteed to converge if all states continue to be selected Three simple ideas In-place dynamic programming Synchronous value iteration stores two copies of value function for all $s$ in $S$In-place value iteration only stores one copy of value function for all $s$ Prioritised sweeping Use magnitude of Bellman error to guide state selectionBackup the state with the largest remaining Bellman errorUpdate Bellman error of affected states after each backupRequire knowledge of reverse dynamicsCan be implemented efficiently by maintaining a priority queue Real-time dynamic programming Idea: only states that are relevant to agentUse agent’s experience to guide the selection of statesAfter each time-step $S_t,A_t,R_{t+1}$Backup the state $S_t$ Full-width and sample backupsFull-width Backups DP uses full-width backups For each backup (sync or async) Every successor state and action is considered Using knowledge of the MDP transitions and reward function DP is effective for medium-sized problems (millions of states) For large problems DP suffers Bellman’s curse of dimensionality Even one backup can be too expensive Sample Backups Instead of reward function R and transition dynamics P Advantages Model-free: no advance knowledge of MDP required Breaks the curse of dimensionality through sampling Cost of backup is constant, independent of $n=|S|$ Approximate Dynamic Programming Approximate the value function Using a function approximator $\hat{v} (s,w)$ Apply dynamic programming to $\hat{v} (\cdot,w)$ Contraction MappingContraction Mapping resolves that convergence problem such as converge or not, uniqueness, and converge speed Value function Space Consider the vector space $V$ over value functions There are |S| dimensions Each points in this space fully specifies a value function Bellman backup brings values functions closer And therefore the backups must converge on a unique solution Bellman Expectation Backup is a ContractionWhen use the $\infty$ norm as the distance metric, we have Define the Bellman expectation backup operator $T^{\pi}$$$ T^{\pi} (v) = R^{\pi} + yP^{\pi} v $$ This operator is a y-contraction, it makes value functions closer by at least y Theorem (Contraction Mapping Theorem)For any metric space $V$ that is complete under an operator $T(v)$, where $T$ is a y-contraction $T$ converges to a unique fixed point At a linear convergence rate of y Convergence of Iter. Policy Evaluation and Policy Iteration The Bellman expectation operator $T^{\pi}$ has a unique fixed point $v_{\pi}$ is a fixed point of $T^{\pi}$ (by Bellman expectation equation) By contraction mapping theorem Iterative policy evaluation converges on $v_{\pi}$ Policy iteration converges on $v_*$ Bellman Optimality Backup is a Contraction Define the Bellman optimality backup operator $T^*$$$ T^* (v) = \max_{a\in A} R^a + y P^a v$$ This operator is a y-contraction, it makes value function closer by at least $y$$$ ||T^* (u) - T^* (v) ||_{\infty} \le y||u-v||_{\infty} $$ Convergence of Value Iteration The bellman optimality operator $T^*$ has a unique fixed point $v_*$ is a fixed point of $T^*$ (by Bellman optimality equation) By contraction mapping theorem Value iteration converges on $v_*$]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL1 Introduction to RL]]></title>
    <url>%2F2017%2F03%2F30%2FRL1%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL lecture 1 Introduction to Reinforcement Learningreinforcement learning feature no supervisor, only a reward signal delayed feedback time matters agent’s actions affect the subsequent data Reward Hypothesisall goal can be described by the maximisation of expected cumulative reward. environment state and agent stateenvironment state : whatever data to environmentagent state: whatever data to agent information stateInformation state is Markov Fully Observable Environments and Partially Observable EnvironmentsMajor Components of an RL Agent Policy Value function Model A model predict what the environment will do next P predict the next state R predict the next reward Maze ExampleAgent may have an internal model of the environmentDynamics: how actions change the stateRewards: how much reward from each stateThe model may be imperfect Categorize RL agents value based policy based Actor Critic Model Free Model Based Learning and PlanningTwo fundamental problems in sequential decision making Reinforcement learning The environment is initially unknown The agent interacts with the environment The agent improves its policy Planning A model of the environment is known The agent performs computation with its model The agent improves its policy Exploration and Exploitation Reinforcement learning is like trail-and-error learning the agent should discover a good policy From its experience of the environment Without losing too much reward along the way Prediction and Control Prediction: evaluate the future, given a policy Control: optimize the future, find the best policy]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL2 Markov decision Processes]]></title>
    <url>%2F2017%2F03%2F30%2FRL2%2F</url>
    <content type="text"><![CDATA[reference: UCL Course on RL lecture 2 Markov decision ProcessesMDP formally describe an environment for RL, where the environment is fully observable Markov PropertyThe future is independent of the past given the present$$ P(S_{t+1} | S_{t} ) = P(S_{t+1} | S_1,S_2,\dots,S_t) $$The state is sufficient statistic of the future Markov ProcessA Markov Process is a memoryless random process DefinitionA Markov Process (Markov Chain) is a tuple (S,P) S is a (finite) set of states P is a state transition probability matrix. Markov Reward ProcessA Markov reward process is a Markov chain with variable DefinitionA Markov Reward Process is a tuple (S,P,R,y) S is a finite set of states P is a state transition probability matrix R is a reward function: $ R_s = E[R_{t+1}|S_t=s] $ y is a discount factor Return ReturnThe return $G_t$ is the total discounted reward from time-step t.$$ G_t = \sum_{k=0}^{\infty} y^k R_{t+k+1}$$ Why discount? Mathematically convenient to discount reward Avoids infinite returns in cyclic Markov processed Uncertainty about the future may not be fully represented If the reward is financial, immediate rewards may earn more interest than delayed rewards Animal/human behaviour shows preference for immediate reward It is sometimes possible to use undiscounted Markov reward processes Value FunctionThe value function gives the long-term value of state s DefinitionThe state value function $v(s)$ of an MRP is the expected return starting from state s$$ v(s) = E[G_t | S_t = s]$$ Bellman Equation for MRPsThe value function can be decomposed into two patrs immediate reward $R_{t+1}$ discounted value of successor state $y v(S_{t+1})$ The Bellman equation can be expressed concisely using matrices,$$ v = R + y P v $$where $v$ is a column vector with one entry per states Solving Bellman Equation linear equation and can be solved directly Computational complexity is $O(n^3)$ for $n$ states Direct solution only possible for small MRPs Many iterative methods for large MRPs Dynamic Programming Monte-Carlo evaluation Temporal-Difference learning Markov Decision ProcessPolicy DefinitionA policy $\pi$ is a distribution over actions given states,$$ \pi(a|s) = P[A_t=a|S_t=s]$$MDP policies depend on the current state (not the history) Given an MDP M =(S,A,P,R,y) and a Policy $\pi$ The state sequence $S_1,S_2,\dots$ is a Markov process $ (S,P^{\pi}) $ The state and reward sequence is a Markov reward process $ (S,P^{\pi},R^{\pi},y) $ Value function State Value function The state value function $v_{\pi}(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\pi$$$ v_{\pi} (s) = E_{\pi} [G_t |S_t=s] $$ Action value function The action value function $q_{\pi}(s,a)$ is expected return starting from state $s$, taking action $a$, and then following policy $\pi$$$ q_{s,a} = E_{\pi} [G_t |S_t=s,A_t=a]$$ Bellman Expectation EquationThe state-value function/action-value function can be decomposed into immediate reward plus discounted value of successor state. For $V^{\pi}$ $$ V_{\pi} (s) = \sum_{a \in A} \pi (a|s) q_{\pi} (s,a) $$ For $Q^{\pi}$ $$ q_{\pi} (s,a) = R_{s}^{a} + y \sum_{s’ \in S} P_{ss’}^{a} v_{\pi} (s’) $$ For $V^{\pi}$ (2) $$ V_{\pi} (s) = \sum_{a \in A} \pi (a|s) R_{s}^{a} + y \sum_{s’ \in S} P_{ss’}^{a} v_{\pi} (s’) $$ For $Q^{\pi}$ (2)$$ q_{\pi} (s,a) = R_{s}^{a} + y \sum_{a \in A} \pi (a’|s’) q_{\pi} (s’,a’) $$ Matrix form$$ v_{\pi} = R^{\pi} + yP^{\pi} v_{\pi}$$ Optimal value function DefinitionThe optimal state-value function is the maximum value function over all policiesThe optimal action-value function is the maximum action-value function over all policies The optimal value function specifies the best possible performance in the MDP. An MDP is “solved” when we know the optimal value fn Optimal PolicyDefine a partial ordering over policies$$ \pi \ge \pi’, v_{\pi} (s) \ge v_{\pi ‘} (s) $$ Theorem For any Markov Decision Process There exists an optimal policy that is better than or equal to all other policies All optimal policies achieve the optimal value function All optimal policies achieve the optimal action-value function An optimal policy can be found by maximising over optimal q function There is always a deterministic optimal policy for any MDP If we know optimal q function, we immediately have the optimal policy Bellman Optimality Equation For $v_*$ $$v_* (s) = \max_{a} q_* (s,a) $$ For $Q^*$ $$ q_* (s,a) = R_s^a + y \sum_{s’ \in S} P_{ss’}^{a} v_* (s’) $$ For $V^*$ $$ v_*(s) = \max_{a} R_s^a + y \sum_{s’ \in S} P_{ss’}^a v_* (s’) $$ For $Q^*$ $$ q_* (s,a) = R_s^a + y \sum_{s’ \in S} P_{ss’}^a \max_{a’} q_* (s’,a’) $$ Solving the Bellman Optimality Equation Bellman Optimality Equation is non-linear No closed form solution (in general) Many iterative solution methods value iteration policy iteration q-learning Sarsa Extensions to MDPsInfinite and continuous MDPs Countably infinite state and/or action spaces Continuous state and/or action spaces Continuous time Requires partial differential equations Hamilton-Jacob-Bellman (HJB) equation Limiting case of Bellman equation as time-step –&gt; 0 Partially observable MDPsPOMDPs (Partially Observable MDPs) A Partially Observable Markov Decision Process is an MDP with hidden states. It is a hidden Markov model with actions. DefinitionA POMDP is a tuple (S,A,O,P,R,Z,y) S is a finite set of states A is a finite set of actions O is a finite set of observations P is a state transition probability matrix R is a reward function Z is an observable function y is a discount factor Belief States A history $H_t$ is a sequence of actions, observations and rewards $$ H_t = A_0, O_1, R_1, \dots, A_{t-1}, O_t, R_t $$ A belief state $b(h)$ is a probability distribution over states conditioned on the history h $$ b(h) = (P[S_t = s^1 |H_t = h], \dots, P[S_t=s^n |H_t =h]) $$ Reductions of POMDPs The history $H_t$ satisfies the Markov property The belief state $b(H_t)$ satisfies the Markov propertyUndiscounted, average reward MDPs Ergodic Markov Process An ergodic Markov process is Recurrent: each state is visited an infinite number of times Aperiodic: each state is visited without any systematic period Theorem An ergodic Markov process has a limiting stationary distribution $d^{\pi} (s)$ with the property$$ d^{\pi} (s) = \sum_{s’ \in S} d^{\pi} (s’) P_{s’s} $$ Ergodic MDP An MDP is ergodic if the Markov chain induced by any policy is ergodic. For any policy $\pi$, an ergodic MDP has an average reward per time-step that is independent of start state Average Reward Value Function The value function of an undiscounted, ergodic MDP can be expressed in terms of average reward.]]></content>
      <categories>
        <category>reinforcement learning</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keyboard_shortcuts]]></title>
    <url>%2F2017%2F03%2F16%2Fkeyboard-shortcuts%2F</url>
    <content type="text"><![CDATA[这里记录一些快捷键Ubuntu的快捷键alt+F7 : 移动窗口alt+F9 : 最小化窗口alt+F10: 最大化窗口 vim快捷键help模式下Ctrl+] 跳转到链接位置Ctrl+t 返回上次的页面]]></content>
      <categories>
        <category>work tips</category>
      </categories>
      <tags>
        <tag>work tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[runR]]></title>
    <url>%2F2016%2F11%2F13%2FrunR%2F</url>
    <content type="text"><![CDATA[Linux下使用R运行linux下的R脚本编写R文件 新建后缀名为R的文件 写入R程序 在脚本首行加入1#!/usr/bin/Rscript 运行R文件这里有两种方式 进入R的环境运行文件1&gt; source('test.R') 注意到在R的环境里面运行脚本，可以保持变量仍然处于环境中 直接在终端中运行12sudo chmod +x test.R %加入执行权限./test.R %运行R]]></content>
      <categories>
        <category>software setting</category>
      </categories>
      <tags>
        <tag>software</tag>
        <tag>statistical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[texstudio-bugs]]></title>
    <url>%2F2016%2F09%2F26%2Ftexstudio-bugs%2F</url>
    <content type="text"><![CDATA[Latex使用技巧记录一些使用latex的过程中出现的问题。 使用的环境是linux英文系统下的texstudio。 texstudio中的中文问题首先要更新ubuntu的对中文的语言支持，确保存在着中文的字体，同时安装中文输入法。然后针对texstudio，设置分为三步。 1.改变build命令，将编译器改成XeLaTeX2.将编辑器的默认编码改成UTF-83.修改tex文件12\documentclass[UTF8,12pt]&#123;report&#125; %加入UTF-8设置\usepackage&#123;ctex&#125;%使用ctex宏包 因为我主要使用英文的系统，所以这里只提供了基本的中文支持。详细的中文使用得需要参考google。]]></content>
      <categories>
        <category>Latex</category>
      </categories>
      <tags>
        <tag>Latex</tag>
        <tag>bugs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu-bugs]]></title>
    <url>%2F2016%2F09%2F23%2Fubuntu-bugs%2F</url>
    <content type="text"><![CDATA[ubuntu中的error与solutions这里记录一些在ubuntu使用过程中碰到的errors，并记录相应的解决办法。 CPG ERROR在/etc/apt/source.list中添加一些源时，执行1sudo apt-get update 若没有这个源对应的秘钥时，会产生error，其中error信息如下： GPG error: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY 06F90DE5381BA480 这里我们可以添加对应的秘钥，即可解决问题。12gpg --keyserver keyserver.ubuntu.com --recv 06F90DE5381BA480gpg --export --armor 06F90DE5381BA480 | sudo apt-key add -]]></content>
      <categories>
        <category>work tips</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>bugs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[backup-restore]]></title>
    <url>%2F2016%2F09%2F10%2Fbackup-restore%2F</url>
    <content type="text"><![CDATA[ubuntu14.04备份与恢复系统 备份首先清空回收站的垃圾，然后使用终端清理一些缓存123sudo apt-get autocleansudo apt-get cleansudo apt-get autoremove 然后新开一个终端123sudo su #su模式cd / #进入根文件夹tar cvpzf backup.tgz --exclude=/proc --exclude=/lost+found --exclude=/backup.tgz --exclude=/mnt --exclude=/sys --exclude=/home/dagui/Downloads / 注意到，因为我的文件夹中Downloads包含大量的下载文件，而我不想将它包含在备份的文件夹中，因此我排除了它。 命令说明：c – 创建一个新的备份文件v – 详细模式，将执行过程全部输出到屏幕p – 保留文件的权限信息以便恢复z – 使用gzip压缩文件，以便减小体积f – 指定备份文件的名称 恢复在一个可用的系统中1sudo su 拷贝backup.tgz到根目录下,然后执行命令1tar xvpfz backup.tgz -C / 然后重新创建之前剔除的目录1mkdir /proc /lost+found /mnt /sys /home/dagui/Downloads 参考链接Ubuntu 14.04如何备份和恢复系统Ubuntu Server服务器备份与还原备份命令]]></content>
      <categories>
        <category>work tips</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>backup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep-Q-learning-2]]></title>
    <url>%2F2016%2F09%2F09%2FDeep-Q-learning-2%2F</url>
    <content type="text"><![CDATA[Deep Q Learning Code Analyze (2)分析的源码来自于deepmind在Natrue上发表的论文Human-level control through deep reinforcement learning所附的源码。源码下载 文件结构 续上 NeuralQLearner.lua该文件定义了一个dqn.NerualQLearner的类，该类主要制定了深度Q学习的学习规则。同样地，这里对该类的成员函数一一进行解读。1local nql = torch.class('dqn.NeuralQLearner') nql:__init(args)类对象的初始化。由于初始化的对象很多，这里就不一一介绍，主要介绍几个难以理解的成员变量，其他成员变量可参考源文件。123456789101112131415161718192021222324252627282930313233343536373839 self.verbose = args.verbose --verbose，数字越大，训练时所输出的信息越多 self.best = args.best --布尔型变量，if true，那么载入模型时载入best_model self.discount = args.discount or 0.99 --在求future discount reward时的衰减因子. self.update_freq = args.update_freq or 1 --更新频率，每执行多少次action才进行一次学习，即每两次更新中所执行的action次数 self.n_replay = args.n_replay or 1 --每次更新时重复学习的次数 self.learn_start = args.learn_start or 0 --学习开始时的步数 self.rescale_r = args.rescale_r --布尔型变量，if true，则将reward除以self.r_max。 self.clip_delta = args.clip_delta --如果定义之后，会将输出层的残差限定在[-self.clip_delta,self.clip_delta]之间 self.target_q = args.target_q --整数，每隔target_q步将update网络的参数copy到target网络 self.bestq = 0 --记录网络的最大的q值 self.ncols = args.ncols or 1 -- 颜色通道数量 self.preproc = args.preproc -- 预处理网络名 self.network = args.network or self:createNetwork() --如果参数给了网络，就载入网络，否则就调用self:createNetwork()函数新建一个网络--参考TransitionTable.lua文件，初始化transiitons类 local transition_args = &#123; stateDim = self.state_dim, numActions = self.n_actions, histLen = self.hist_len, gpu = self.gpu, maxSize = self.replay_memory, histType = self.histType, histSpacing = self.histSpacing, nonTermProb = self.nonTermProb, bufferSize = self.bufferSize &#125; self.transitions = dqn.TransitionTable(transition_args) self.numSteps = 0 -- 执行的步数. self.v_avg = 0 -- Validation上的平均q值. self.tderr_avg = 0 -- target和destination之间的平均误差. self.w, self.dw = self.network:getParameters() --得到网络的参数w和dw self.dw:zero() self.deltas = self.dw:clone():fill(0) --设立中间变量，用来求梯度 self.tmp= self.dw:clone():fill(0) self.g = self.dw:clone():fill(0) self.g2 = self.dw:clone():fill(0) 另外，在初始化的函数中，还调用了lua语言内置的pcall函数来载入网络和预处理网络。例如如下代码：12345678910111213141516local msg, err = pcall(require, self.network)if not msg then local err_msg, exp = pcall(torch.load, self.network) if not err_msg then error("Could not find network file ") end if self.best and exp.best_model then self.network = exp.best_model else self.network = exp.model endelse print('Creating Agent Network from ' .. self.network) self.network = err self.network = self:network()end pcall函数是lua的内置处理函数,一般的使用方法是msg,err=pcall(func,param)，通过调用func(param)函数，如果调用成功，则msg返回true，err返回func(param)的返回值，如果出现错误和异常，则msg返回nil，err返回错误的信息。该函数在lua相当于try，out的作用。 通过使用pcall调用载入函数，可以事先对self.network和self.preproc进行初始化。 nql:reset(state)重置类对象，主要是载入state.best_network和state.model。然后将self.dw归零，将执行步数self.numSteps置零。 nql:preprocess(rawstate)将原始状态进行预处理1234567function nql:preprocess(rawstate) if self.preproc then return self.preproc:forward(rawstate:float()) :clone():reshape(self.state_dim) end return rawstateend nql:getQUpdate(args)该函数主要以一个transition: &lt; s,a,r,s’,term&gt;作为输入，然后通过计算获得Q值，以及targets，残差等。首先该函数会载入args的参数，包括args.s,args.a,args.r,args.s2,args.term。然后按照下面的程序计算：123456789101112131415161718-- delta = r + (1-terminal) * gamma * max_a Q(s2, a) - Q(s, a)term = term:clone():float():mul(-1):add(1) -- (1-term)q2_max = target_q_net:forward(s2):float():max(2) --max_a Q(s_2,a)-- 计算 q2 = (1-terminal) * gamma * max_a Q(s2, a)q2 = q2_max:clone():mul(self.discount):cmul(term)delta = r:clone():float()if self.rescale_r then --如果self.rescale_r定义了之后，就将reward除以self.r_max delta:div(self.r_max)enddelta:add(q2) --r + (1-terminal) * gamma * max_a Q(s2, a)local q_all = self.network:forward(s):float() --q_all矩阵用来存储q值，q_all[i][j]表示batchsize中的第i个输入的第j个action对应的Q值q = torch.FloatTensor(q_all:size(1))for i=1,q_all:size(1) do --q向量，这里q=Q(s,a) q[i] = q_all[i][a[i]]enddelta:add(-1, q) --delta = r + (1-terminal) * gamma * max_a Q(s2, a) - Q(s, a) 这里得到了delta=r + (1-terminal) gamma max_a Q(s2, a) - Q(s, a),注意到，如果定义了self.clip_delta，那么将残差进行限幅操作，将幅度不在[-self.clip_delta,self.clip_delta]的delta值强行clip。同时，函数定义了targets矩阵，其中target是一个二维矩阵，第一维表示batch_size，第二维表示actions。这里，我们将delta的值赋给target对应的action位置，其他action处，target=0。1234local targets = torch.zeros(self.minibatch_size, self.n_actions):float()for i=1,math.min(self.minibatch_size,a:size(1)) do targets[i][a[i]] = delta[i]end 最后函数返回targets,delta以及q2_max的值。 nql:qLearnMinibatch()这个函数的主要目的是执行一个minibatch的Q-learning的update，其中采用的更新权重的方法是PMSProp，这里w += alpha (r + gamma max Q(s2,a2) - Q(s,a)) dQ(s,a)/dw123456789101112131415161718192021222324local s, a, r, s2, term = self.transitions:sample(self.minibatch_size) --利用transition类的sample函数，得到新的transitionlocal targets, delta, q2_max = self:getQUpdate&#123;s=s, a=a, r=r, s2=s2,term=term, update_qmax=true&#125; --更新Qself.dw:zero()self.network:backward(s, targets) --反向传播，更新dwself.dw:add(-self.wc, self.w) --加入一阶正则化约束-- 更新学习率local t = math.max(0, self.numSteps - self.learn_start)self.lr = (self.lr_start - self.lr_end) * (self.lr_endt - t)/self.lr_endt +self.lr_endself.lr = math.max(self.lr, self.lr_end)-- 利用PMSProp求得梯度，这里加入了一阶的梯度momentum和二阶的梯度momentumself.g:mul(0.95):add(0.05, self.dw)self.tmp:cmul(self.dw, self.dw)self.g2:mul(0.95):add(0.05, self.tmp)self.tmp:cmul(self.g, self.g)self.tmp:mul(-1)self.tmp:add(self.g2)self.tmp:add(0.01)self.tmp:sqrt()self.deltas:mul(0):addcdiv(self.lr, self.dw, self.tmp) --lr*tmp/dwself.w:add(self.deltas) a=addcdiv(b,c,d)表示a=a+b*d/c nql:sample_validation_data()利用transition类的sample函数，采样self.validsize个样本，并将数据存储到self.valid\(s,a,r,s2,term)中。 nql:compute_validation_statistics()计算得到validation上的平均Q_max值，和平均误差（误差指target和destination之间的差）1234567function nql:compute_validation_statistics() local targets, delta, q2_max = self:getQUpdate&#123;s=self.valid_s, a=self.valid_a, r=self.valid_r, s2=self.valid_s2, term=self.valid_term&#125; self.v_avg = self.q_max * q2_max:mean() self.tderr_avg = delta:clone():abs():mean()end nql:eGreedy()该函数主要的目的按照greed expolation的方式去选择一个action12345678function nql:eGreedy(state, testing_ep) self.ep = testing_ep or (self.ep_end + math.max(0, (self.ep_start - self.ep_end) * (self.ep_endt - math.max(0, self.numSteps - self.learn_start))/self.ep_endt)) --更新self.ep的值 if torch.uniform() &lt; self.ep then return torch.random(1, self.n_actions) --以ep的概率值随机选择action的值 else return self:greedy(state) --以1-ep的概率值选择最大Q值的action，具体实现参考nql:greedy(state)函数 endend nql:greedy(state)这个函数的目的就是用来根据最大Q值选择一个action的值，注意到，如果有几个action的Q值均为最大，那么随机选择一个action执行。123456789101112131415local q = self.network:forward(state):float():squeeze()local maxq = q[1] --max qlocal besta = &#123;1&#125; --best actionfor a = 2, self.n_actions do if q[a] &gt; maxq then besta = &#123; a &#125; maxq = q[a] elseif q[a] == maxq then besta[#besta+1] = a endendself.bestq = maxqlocal r = torch.random(1, #besta)self.lastAction = besta[r] --存储到self.lastActionreturn besta[r] nql:perceive(reward,rawstate,terminal,testing,testing_ep)这个函数会与transition类之间进行交互，然后更新Q值，选择action，并进行参数的优化。首先，将rawstate进行预处理，并定义当前状态12local state = self:preprocess(rawstate):float()local curState 然后根据self.max_reward,self.min_reward和self.rescale_r将reward进行限幅。123456789if self.max_reward then reward = math.min(reward, self.max_reward)endif self.min_reward then reward = math.max(reward, self.min_reward)endif self.rescale_r then self.r_max = math.max(self.r_max, reward)end 调用transition类，将state（这里的state只包含一帧图像）加入recent存储区，然后从transition中采样得到新的state，此时的state是由多帧构成的。接下来将新的transition存储到存储区内。123456self.transitions:add_recent_state(state, terminal)local currentFullState = self.transitions:get_recent()if self.lastState and not testing then --testing标志位表示是否进行测试模式 self.transitions:add(self.lastState, self.lastAction, reward, self.lastTerminal, priority)end 12345if self.numSteps == self.learn_start+1 and not testing then --如果训练才刚刚开始，那么先采样验证集的数据 self:sample_validation_data()endcurState= self.transitions:get_recent() --得到当前专题太curState = curState:resize(1, unpack(self.input_dims)) 利用eGreedy算法得到新的action12345local actionIndex = 1if not terminal then actionIndex = self:eGreedy(curState, testing_ep)endself.transitions:add_recent_action(actionIndex) 进行Q-learning更行权重，这里更新每隔self.update_freq步才进行一次权重的的更新，也就是说每两次更新之间执行self.update_freq次action，然后每次更新会重复连续学习self.n_replay次123456if self.numSteps &gt; self.learn_start and not testing and self.numSteps % self.update_freq == 0 then for i = 1, self.n_replay do self:qLearnMinibatch() endend 更新学习步数123if not testing then self.numSteps = self.numSteps + 1end 学习完之后，此时的状态和action都发生的改变，我们需要将last的状态和action进行一个更新。123self.lastState = state:clone()self.lastAction = actionIndexself.lastTerminal = terminal 每隔self.target_q个步骤，将参数copy到target网络。123if self.target_q and self.numSteps % self.target_q == 1 then self.target_network = self.network:clone()end 最后返回要执行的actionIndex值12345if not terminal then return actionIndexelse return 0end nql:createNetwork()创建一个三个线性层的网络，这是一个三层的多层感知器1234567891011function nql:createNetwork() local n_hid = 128 local mlp = nn.Sequential() mlp:add(nn.Reshape(self.hist_len*self.ncols*self.state_dim)) mlp:add(nn.Linear(self.hist_len*self.ncols*self.state_dim, n_hid)) mlp:add(nn.Rectifier()) mlp:add(nn.Linear(n_hid, n_hid)) mlp:add(nn.Rectifier()) mlp:add(nn.Linear(n_hid, self.n_actions)) return mlpend nql:_loadNet()载入网络，返回self.network nql:init(arg)手动初始化 nql:report()调用nnutil.lua中的get_weight_norms,get_grad_norms函数，输出network的信息。 train_agent.lua这是训练的主程序，这里对其进行解析。 初始化调用setup.lua进行初始化，得到game_env,game_actions,agent,opt。12local opt = cmd:parse(arg)local game_env, game_actions, agent, opt = setup(opt) 然后初始化参数列表123456789101112131415161718local learn_start = agent.learn_start --学习开始时的步数local start_time = sys.clock() --开始的时间local reward_counts = &#123;&#125; --记录每次测试时的不等于0的reward数量local episode_counts = &#123;&#125; --记录每次测试时的episode数量local time_history = &#123;&#125; --记录每次测试的时间local v_history = &#123;&#125; --记录每次测试时的平均Q值local qmax_history = &#123;&#125; --记录每次测试时的qmaxlocal td_history = &#123;&#125; --记录每次测试时的误差值local reward_history = &#123;&#125; --记录每次测试时的总的rewardlocal step = 0 --训练时的步数time_history[1] = 0--测试时使用的参数local total_reward --总的rewardlocal nrewards --不为0的reward数量local nepisodes --总的episodeslocal episode_reward --中间变量，用来存储一个episode中的reward总量local screen, reward, terminal = game_env:getState() --从environment中获得rawstate(screen)，reward，terminal。 训练调用nql:perceive()函数进行训练，得到执行的action_index。12step = step + 1 --步数更新local action_index = agent:perceive(reward, screen, terminal) 如果游戏已经结束，那么重新进入下一个游戏123456789if not terminal then screen, reward, terminal = game_env:step(game_actions[action_index], true) --游戏没有结束，那么玩游戏，并得到下一个screen,reward,terminalelse if opt.random_starts &gt; 0 then --opt.random_start表示是否重新开始 screen, reward, terminal = game_env:nextRandomGame() else screen, reward, terminal = game_env:newGame() endend 每隔opt.prog_freq步就输出网络的信息123456if step % opt.prog_freq == 0 then assert(step==agent.numSteps, 'trainer step: ' .. step ..' &amp; agent.numSteps: ' .. agent.numSteps) print("Steps: ", step) agent:report() --调用nql:report()函数 collectgarbage()end 在lua语言中，不会自动处理垃圾，需要调用collectgarbage()手动处理。 训练（在特定的步数上进行验证）每隔opt.eval_freq步就进行验证。首先进行初始化。12345screen, reward, terminal = game_env:newGame()total_reward = 0 --总的rewardnrewards = 0 --不为0的reward的数量nepisodes = 0 --一共执行的episodes的数量episode_reward = 0 --中间变量，保存每个episode的reward的总量 然后调用nql:perceive()进行验证，注意到这里的testing参数为true，ep固定为0.05。123456789101112131415161718local eval_time = sys.clock() --记录测试开始时的时间for estep=1,opt.eval_steps do --opt.eval_step表示测试时的步数 local action_index = agent:perceive(reward, screen, terminal, true, 0.05) --这里testing=true，testing_ep=0.05 -- 在测试模式下进行游戏 screen, reward, terminal = game_env:step(game_actions[action_index]) if estep%1000 == 0 then collectgarbage() end episode_reward = episode_reward + reward if reward ~= 0 then nrewards = nrewards + 1 end if terminal then total_reward = total_reward + episode_reward episode_reward = 0 nepisodes = nepisodes + 1 screen, reward, terminal = game_env:nextRandomGame() endend 计算时间12eval_time = sys.clock() - eval_time --计算得到刚才测试所消耗的时间start_time = start_time + eval_time --更新时间 获得统计数据，注意到由于每次测试都有可能执行了不同的eposide，我们这里计算每个eposide的平均值。1234567891011121314151617181920 agent:compute_validation_statistics() --调用nql:compute_validation_statistics函数来计算得到平均Q值self.v_avg，和平均误差self.tferr_avg local ind = #reward_history+1 --迭代器，指向reward_history的下标 total_reward = total_reward/math.max(1, nepisodes) --得到每个eposide的平均reward if #reward_history == 0 or total_reward &gt; torch.Tensor(reward_history):max() then --如果reward_history中没有数据，或者测试时产生的新的total_reward比之前产生的都要好，那么更新agent.best_network agent.best_network = agent.network:clone() end if agent.v_avg then --记录统计数据 v_history[ind] = agent.v_avg td_history[ind] = agent.tderr_avg qmax_history[ind] = agent.q_max end print("V", v_history[ind], "TD error", td_history[ind], "Qmax", qmax_history[ind])--记录total_reward，nrewards，nepisodes，以及运行时间 reward_history[ind] = total_reward reward_counts[ind] = nrewards episode_counts[ind] = nepisodes time_history[ind+1] = sys.clock() - start_time --记录本次测试结束的时间 local time_dif = time_history[ind+1] - time_history[ind] --计算两次测试之间的时间差 local training_rate = opt.actrep*opt.eval_freq/time_dif --计算训练速率，指单位时间内训练的次数，opt.actrep表示重复执行actions的次数，opt.eval_freq表示验证的频率 输出信息1print(string.format('\nSteps: %d (frames: %d), reward: %.2f, epsilon: %.2f, lr: %G, ' .. 'training time: %ds, training rate: %dfps, testing time: %ds, ' .. 'testing rate: %dfps, num. ep.: %d, num. rewards: %d', step, step*opt.actrep, total_reward, agent.ep, agent.lr, time_dif, training_rate, eval_time, opt.actrep*opt.eval_steps/eval_time, nepisodes, nrewards)) 训练（在特定的步数上进行保存）每隔opt.save_freq步或者训练完之后，将网络进行保存。对于保存的程序，这里就不进行分析了。 上一页]]></content>
      <categories>
        <category>code analyze</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>Q-learning</tag>
        <tag>deepmind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep_Q_learning]]></title>
    <url>%2F2016%2F09%2F06%2FDeep-Q-learning%2F</url>
    <content type="text"><![CDATA[Deep Q Learning Code Analyze (1)分析的源码来自于deepmind在Natrue上发表的论文Human-level control through deep reinforcement learning所附的源码。源码下载 文件结构 代码采用torch框架进行组织，编写的语言均为lua语言，其中包括convnet.lua, convnet_atari3.lua, initenv.lua, net_downsample_2x_full_y.lua, NeuralQLearner.lua, nnutils.lua, Rectifier.lua, Scale.lua, train_agent.lua, TransitionTable.lua。 训练的主程序是从train_agent.lua(具体的train_agent.lua的解析见这里)开始。训练时的参数表如下：1234567891011121314151617181920212223242526272829cmd:option('-framework', '', 'name of training framework')cmd:option('-env', '', 'name of environment to use')cmd:option('-game_path', '', 'path to environment file (ROM)')cmd:option('-env_params', '', 'string of environment parameters')cmd:option('-pool_frms', '', 'string of frame pooling parameters (e.g.: size=2,type="max")')cmd:option('-actrep', 1, 'how many times to repeat action')cmd:option('-random_starts', 0, 'play action 0 between 1 and random_starts ' .. 'number of times at the start of each training episode')cmd:option('-name', '', 'filename used for saving network and training history')cmd:option('-network', '', 'reload pretrained network')cmd:option('-agent', '', 'name of agent file to use')cmd:option('-agent_params', '', 'string of agent parameters')cmd:option('-seed', 1, 'fixed input seed for repeatable experiments')cmd:option('-saveNetworkParams', false, 'saves the agent network in a separate file')cmd:option('-prog_freq', 5*10^3, 'frequency of progress output')cmd:option('-save_freq', 5*10^4, 'the model is saved every save_freq steps')cmd:option('-eval_freq', 10^4, 'frequency of greedy evaluation')cmd:option('-save_versions', 0, '')cmd:option('-steps', 10^5, 'number of training steps to perform')cmd:option('-eval_steps', 10^5, 'number of evaluation steps')cmd:option('-verbose', 2, 'the higher the level, the more information is printed to screen')cmd:option('-threads', 1, 'number of BLAS threads')cmd:option('-gpu', -1, 'gpu flag') 训练的开始会调用initenv.lua初始化game_env, game_actions, agent, opt。 initenv.luainitenv文件是在训练的初始阶段，用来初始化gamEnv，gameActions，agent，以及opt参数。提供了torchSetup函数和Setup函数，在这里torchSetup函数用来初始化一些与torch相关的参数，包括gpu参数，计算线程，以及tensorType等。 而Setup参数用来调用torchSetup函数，并对gameEnv，gameActions，agent进行了初始化操作。 gameEnv表示游戏的环境，通过调用getState()方法可以得到screen, reward和terminal参数。screen表示屏幕状态，这是DQN中的输入，terminal是布尔型变量，表示是否游戏结束。1local screen, reward, terminal = game_env:getState() nnutils.luannutils文件主要提供了一些辅助函数。该文件首先提供了recursive_map的函数，该函数接受module, field, func作为输入，返回一个字符串，其中module表示训练的模型，field指模型中的某类参数名，比如field=’weight’时，module[field]表示模型中的权重。该函数会返回字符串，包含了模型的类型名，对module[field]的统计数据（统计的方法视func而定）。1function recursive_map(module, field, func) 由于模型中包含了子模型，因此recusive_map函数会递归调用子模型，因此会形成模型的树状表示。1234567891011if module.modules then str = str .. "[" for i, submodule in ipairs(module.modules) do local submodule_str = recursive_map(submodule, field, func) str = str .. submodule_str if i &lt; #module.modules and string.len(submodule_str) &gt; 0 then str = str .. " " end end str = str .. "]"end 在nnuils的文件中，定义了abs_mean()和abs_max()的函数，表示平均值和最大值。另外也定义了get_weight_norms()和get_grad_norms()的函数，这两个函数会调用recursive_map函数，分别对权重和梯度值求均值和最大值。12345678910function get_weight_norms(module) return "Weight norms:\n" .. recursive_map(module, "weight", abs_mean) .. "\nWeight max:\n" .. recursive_map(module, "weight", abs_max)endfunction get_grad_norms(module) return "Weight grad norms:\n" .. recursive_map(module, "gradWeight", abs_mean) .. "\nWeight grad max:\n" .. recursive_map(module, "gradWeight", abs_max)end Scale.luascale.lua文件定义了训练时的scale层（此时的torch并没有内置scale的层），并定义了forward和updateOutput方法，实际上这两个方法都是相同的功能。123function scale:updateOutput(input) return self:forward(input)end 在scale:forward(x)函数中，x表示输入的图像，该函数会调用image.rgb2y(x)将输入的图像变成灰度图，然后将它按照初始化的宽高进行放缩。 Rectifier.lua同样地，Rectifier.lua文件定义了训练时的ReLU函数层，这里对前向传播和反向传播都进行了定义。12345678function Rectifier:updateOutput(input) return self.output:resizeAs(input):copy(input):abs():add(input):div(2)endfunction Rectifier:updateGradInput(input, gradOutput) self.gradInput:resizeAs(self.output) return self.gradInput:sign(self.output):cmul(gradOutput) end 这里self.output.resizeAs(input)的意思就是将output，resize成和input同样的size。cmul()表示矩阵对应元素相乘。 convnet.luaconvnet.lua文件的目的是建立CNN结构，该文件仅仅包含一个函数：create_network。输入层的定义由初始化时的input_dims给出。注意到，在函数里对GPU和CPU的卷积层的实现方式有所区分。卷积层的数量由初始化时的arg.n_units的长度给出（arg.n_units的每个元素的数值表示每一层的输出的feature map个数），如下所示，这里arg.nl()表示非线性层的意思。123456for i=1,(#args.n_units-1) do ---第二个卷积层到最后一个卷积层 net:add(convLayer(args.n_units[i], args.n_units[i+1], args.filter_size[i+1], args.filter_size[i+1], args.filter_stride[i+1], args.filter_stride[i+1])) net:add(args.nl())end 在卷积的最后一层通过人为构造0的输入的方式，进行前向传播，并对输出层进行nElement()的方法可以求得卷积最后一层的神经元数量。12nel = net:forward(torch.zeros(1,unpack(args.input_dims))):nElement()net:add(nn.Reshape(nel)) 然后加入多个线性层，同样的，线性层的数量由arg.n_hid的长度给出（arg.n_hid的每个元素的数值表示每个线性层输出的神经元数量）12345for i=1,(#args.n_hid-1) do --第二哥线性层到最后一个线性层 last_layer_size = args.n_hid[i+1] net:add(nn.Linear(args.n_hid[i], last_layer_size)) net:add(args.nl())end 最后加入一个线性层，其输出神经元的额数量等于actions的数量1net:add(nn.Linear(last_layer_size, args.n_actions)) convnet_atari3.lua这个文件主要是调用convnet.lua文件，并设置了一些对应的参数。123456789return function(args) args.n_units = &#123;32, 64, 64&#125; --三个卷积层，输出的feature map的数量是32,64,64 args.filter_size = &#123;8, 4, 3&#125; --每个卷积层的卷积核大小 args.filter_stride = &#123;4, 2, 1&#125; --每个卷积层的步长 args.n_hid = &#123;512&#125; --线性层的输出神经元数量 args.nl = nn.Rectifier --非线性类型 return create_network(args)end net_downsample_2x_full_y.lua这个文件会在构建网络时，在输入层增加一个Scale层，此时设置的长和宽均为84，Scale层会将输入的图像先变成灰度图，然后放缩成84x84的大小。 TransitionTable.lua该文件主要创造了一个dqn.TransitionTable类，每个transition表示&lt;s,a,r,s’>，其中s表示state，a表示actions，r表示rewards，s’表示在s状态下执行a，得到的下一个状态s’。这个类用来存储一定数量的transitions，充当replay memory的角色。在CNN训练时，从这个replay memory中进行sample，sample出来的样本作为了网络的输入。1local trans = torch.class('dqn.TransitionTable') 对于dqn.TransitionTable类，该文件中设计了不少的方法，这里进行一一的解读。 trans:__init(args)首先通过读args直接进行对象的初始化，这里包含的参数如下,在这里hist表示history的意思，每一个history中存储的帧图像合并才构成一个状态（这样做的原因是因为单独的某一帧的图像无法得到运动物体的速度信息等）：1234567891011121314self.stateDim = args.stateDim --state的维度self.numActions = args.numActions --Actions的数量self.histLen = args.histLen --History的长度self.maxSize = args.maxSize or 1024^2 --最大存储空间大小self.bufferSize = args.bufferSize or 1024 --缓存区的大小self.histType = args.histType or "linear" --采样History时使用类型，包括'linear','exp2','exp1.25'self.histSpacing = args.histSpacing or 1 --History的间隔，如果histType的类型是’linear‘，表示每个histIndices之间相差histSpacingself.zeroFrames = args.zeroFrames or 1 --若该参数为0，则表示每一个history中可以包含不同episode的帧图像self.nonTermProb = args.nonTermProb or 1self.nonEventProb = args.nonEventProb or 1self.gpu = args.gpuself.numEntries = 0 --存储transition的数量self.insertIndex = 0self.histIndices = &#123;&#125; --表示采样时的history下标 然后函数会针对不同的self.histType来设定不同的self.histIndices，同时，self.recentMemSize表示存储时的history的跨度，也就是histIndices[histLen]的值。 在self.histLen=5的情况下，如果self.histType=”linear”，且self.histSpacing=2时，那么self.histIndices={2,4,6,8,10}，self.recentMemSize=10。如果self.histType=”exp2”，那么self.histIndices={1,2,4,8,16}，self.recentMemSize=16。 接下来对self.s，self.a，self.r，self.t进行初始化设置。12345self.s = torch.ByteTensor(self.maxSize, self.stateDim):fill(0) --state，这里的state仅仅指一帧图像self.a = torch.LongTensor(self.maxSize):fill(0) --actionsself.r = torch.zeros(self.maxSize) --rewardself.t = torch.ByteTensor(self.maxSize):fill(0) --terminalself.action_encodings = torch.eye(self.numActions) 然后初始化了recent存储区，用来存储最近recentMemSize个帧的图像，也就是说在采样时这里只能采样一个状态，这可以用来建立最新的状态。123self.recent_s = &#123;&#125;self.recent_a = &#123;&#125;self.recent_t = &#123;&#125; 另外初始化时也定义了buffer区，在训练时的transition即来自buffer区。123456local s_size = self.stateDim*histLen --s_size表示将histLen个帧图像连接在一起构成的新的状态的大小self.buf_a = torch.LongTensor(self.bufferSize):fill(0)self.buf_r = torch.zeros(self.bufferSize)self.buf_term = torch.ByteTensor(self.bufferSize):fill(0)self.buf_s = torch.ByteTensor(self.bufferSize, s_size):fill(0)self.buf_s2 = torch.ByteTensor(self.bufferSize, s_size):fill(0) --s2表示s'，即在s下执行a得到的新的s buffer区的state是由几个frame连接得到的，而self.s仅仅指一帧。 trans:reset()重置transition memory1234function trans:reset() self.numEntries = 0 self.insertIndex = 0end trans:size()返回self.numEntries trans.empty()将self.numEntries置0 trans.concatFrames(index,use_recent)该函数负责将histLen个Frames的图像连接在一起，组成一个状态。至于Frames的选取方法，由self.histIndices的值来决定。use_recent是一个bool型的变量，这个变量决定是否使用recent table12345if use_recent then s, t = self.recent_s, self.recent_telse s, t = self.s, self.tend 函数新建了一个局部变量fullstate，用来存储histLen个Frames的数据。函数的输入变量index表示在s中采样的Frames的初始下标。这个函数会在index与index+self.histIndice[histLen]-1之间的Frames，按照index+self.histIndice的方式进行采样，然而，如果在这些帧图像之间出现了terminal状态，也就是说游戏重新开始了一遍，这里会将出现terminal状态前的采样帧进行归零处理。也就是说最后得到的fullstate只包含最新的episode（每次从游戏开始到结束称为一个episode）。最终得到的一个fullstate称为一个状态。123456789101112131415161718192021222324252627282930313233--初始化fullstate，大小是histLen个s的大小 local fullstate = s[1].new() fullstate:resize(self.histLen, unpack(s[1]:size():totable()))--将除了最新的episode外的帧图像归零 local zero_out = false --归零标志位 local episode_start = self.histLen --最新的episode开始的帧在fullstate中的下标 for i=self.histLen-1,1,-1 do --反向搜索，一旦搜索到terminal，就对前面的采样进行操作 if not zero_out then for j=index+self.histIndices[i]-1,index+self.histIndices[i+1]-2 do if t[j] == 1 then --t表示terminal，如果在两个采样的帧之间出现了terminal，代表这两个采样属于不同的episode，因此将之前的采样全部归零。 zero_out = true break end end end if zero_out then --一旦zero_out变为true之后，会一直保持为ture的状态 fullstate[i]:zero() else episode_start = i end end if self.zeroFrames == 0 then --self.zeroFrames参数，一旦等于0，则阻止归零的操作。 episode_start = 1 end for i=episode_start,self.histLen do fullstate[i]:copy(s[index+self.histIndices[i]-1]) --将最新的episode中的帧copy到fullstate中 end return fullstate trans:concatActions(index,use_recent)该函数的作用类似于trans:concatFrames，唯一的区别是它作用的对象是actions。 trans:get(index)调用self:concatFrames(index)得到s和s2，我们取s中的最后一帧的action和reward作为整个state的action和reward，terminal取整个state后的第一帧的t值。1234567function trans:get(index) local s = self:concatFrames(index) local s2 = self:concatFrames(index+1) local ar_index = index+self.recentMemSize-1 --训练状态的最后一帧的下标 return s, self.a[ar_index], self.r[ar_index], s2, self.t[ar_index+1]end trans:sample_one()在（2，self.numEntries-self.recentMemSize）之间进行均匀采样得到一个index，从2开始的原因是保证有一个previous action，index的最大值是self.numEntries-self.rencentMemSize，这样设置是因为训练的状态的最后一帧的下标与第一帧的下标之间相差recentMemSize。同时如果self.nonTermProb和self.nonEventProb不等于1的情况下，采样的状态会被随机抛弃。123456789101112131415161718192021function trans:sample_one() assert(self.numEntries &gt; 1) local index local valid = false while not valid do index = torch.random(2, self.numEntries-self.recentMemSize) --均匀随机采样一个index if self.t[index+self.recentMemSize-1] == 0 then valid = true end if self.nonTermProb &lt; 1 and self.t[index+self.recentMemSize] == 0 and torch.uniform() &gt; self.nonTermProb then --以（1-self.nonTermProb）的概率抛弃所采样的非terminal状态 valid = false end if self.nonEventProb &lt; 1 and self.t[index+self.recentMemSize] == 0 and self.r[index+self.recentMemSize-1] == 0 and torch.uniform() &gt; self.nonEventProb then --以（1-nonEventProb）的概率随机抛弃所采样的非terminal和无reward状态 valid = false end end return self:get(index)end trans:fill_buffer()这个函数通过调用trans:sample_one()的函数来进行采样，然后将这些随机采样的样本加入到buffer区。执行这个函数会刷新buffer区的数据。注意到这里必须保证原存储区的样本个数大于buffer区。12assert(self.numEntries &gt;= self.bufferSize)self.buf_ind = 1 然后进行采样，注意到该函数调用后会初始化一个类成员变量self.buf_ind，这个变量表示在buffer中训练时的下标指示器。每次调用该函数就会使这个变量置为1，即表示现在的buffer区的数据还没有被训练。12345678for buf_ind=1,self.bufferSize do local s, a, r, s2, term = self:sample_one(1) self.buf_s[buf_ind]:copy(s) self.buf_a[buf_ind] = a self.buf_r[buf_ind] = r self.buf_s2[buf_ind]:copy(s2) self.buf_term[buf_ind] = termend trans:sample(batch_size)在buffer区得到batch_size个tansition，注意到如果buffer区中所剩下的数据少于batch_size时会重新更新buffer区。12345678910111213141516function trans:sample(batch_size) local batch_size = batch_size or 1 assert(batch_size &lt; self.bufferSize) if not self.buf_ind or self.buf_ind + batch_size - 1 &gt; self.bufferSize then self:fill_buffer() --如果buffer区未更新过，或者剩余的数据量少于batch_size时，重新装填buffer区 end local index = self.buf_ind self.buf_ind = self.buf_ind+batch_size -- 更新self.buf_ind的值 local range = &#123;&#123;index, index+batch_size-1&#125;&#125; local buf_s, buf_s2, buf_a, buf_r, buf_term = self.buf_s, self.buf_s2,self.buf_a, self.buf_r, self.buf_term return buf_s[range], buf_a[range], buf_r[range], buf_s2[range], buf_term[range]end trans:add(s,a,r,term)该文件会将一组新的s，a，r，term（terminal）写进存储区，每写进一个数据self.numEntries会加1，直到self.maxSize为止。123if self.numEntries &lt; self.maxSize then self.numEntries = self.numEntries + 1end 这里用self.inserIndex来控制写入的下标，当存储区写满后，又从头开始写入。12345self.insertIndex = self.insertIndex + 1-- 如果写满了，则重头开始if self.insertIndex &gt; self.maxSize then self.insertIndex = 1end 写入存储区12345678self.s[self.insertIndex] = s:clone():float():mul(255)self.a[self.insertIndex] = aself.r[self.insertIndex] = rif term then self.t[self.insertIndex] = 1else self.t[self.insertIndex] = 0end trans:add_recent_state(s,term),trans:add_recent_action(a)这两个函数分别将s，term和a加入recent存储区，注意到由于recent存储区只存储一个状态，因此函数里面有维持recent存储区的大小等于self.recentMemSize的操作。 trans:get_recent() 从recent存储区取一个状态123function trans:get_recent() return self:concatFrames(1, true):float():div(255)end trans:write(file)将trans类的参数序列化写入文件 trans:read(file)执行反序列化，从文件中读取参数 下一页]]></content>
      <categories>
        <category>code analyze</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>Q-learning</tag>
        <tag>deepmind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo_github]]></title>
    <url>%2F2016%2F09%2F05%2FHexo-github%2F</url>
    <content type="text"><![CDATA[Hexo and github pages SETTINGIntroduction 因为在调试的过程中，经常会发现一些bug，而这些bug过一段时间就会遗忘，为了记住这些bug，因此开始学会养成写个人技术blog的习惯。 这里采用的是hexo 来解析markdown文件，然后将其发布到github pages上。然而在设置的时候有出现了一些问题，这里记录这些问题。 Setting 安装hexo安装Node.js先安装nvm1$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh nvm 安装完之后，重启终端，然后安装Node.js1$ nvm install stable 也可以通过源码来安装 安装git1$ sudo apt-get install git-core 安装hexo1$ npm install -g hexo-cli 安装hexo必备的服务安装server和git服务123$ cd (blog workspace)$ npm install hexo-server --save$ npm install hexo-deployer-git --save 设置github pagesgithub账号下，需要建立对应的仓库，其名字必须为(username).github.io。否则将无法得到正确的github pages显示。 设置hexo初始化blog文件夹12$ cd (blog workspace)$ hexo init 编辑配置文件_config.yml按照描述更改配置，注意要更改deploy选项。 deploy: type: git repository: git@github.com:chendagui16/chendagui16.github.io.git branch: master 这里我使用了ssh协议 更换主题与数学公式我采用了jacman的主题，设置方法可进入github中观看。 这个主题中提供了较多的选项，包括如何解决数学公式的显示问题，然而，我只得到了本地页面的显示并没有得到github pages上的显示，问题仍未解决。谁知道了能否告诉我怎么解决。 在jacman的主题下，设置数学显示包括两步，第一步是更改主题内的_config.yml文件。1mathjax: true 第二步是在写front-matter内添加一行1mathjax: true 命令 123456$ hexo init 初始化$ hexo new [layout] &lt;title&gt; 新建文档，layout提供默认版式$ hexo g[enerate] [-d] 渲染成静态网页$ hexo s[erver] 打开本地服务器$ hexo d[eploy] 推送到github$ hexo clean 清除原静态文件]]></content>
      <categories>
        <category>software setting</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>githup pages</tag>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
